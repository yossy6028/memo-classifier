#!/usr/bin/env python3
"""
çµ±åˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½ä»˜ãå¼·åŒ–ãƒ¡ãƒ¢å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
AppleScriptç¢ºèªç”»é¢ç”¨ã®å®Œå…¨ãªäº‹å‰åˆ†æ
"""

import sys
import os
import re
import json
from pathlib import Path
from datetime import datetime
from collections import Counter, defaultdict

# Ultrathinkingçµ±åˆ
try:
    from ultrathinking_analyzer import UltrathinkingAnalyzer
    ULTRATHINKING_AVAILABLE = True
    print("âœ… Ultrathinkingçµ±åˆãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹")
except ImportError:
    ULTRATHINKING_AVAILABLE = False
    print("âš ï¸ é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œï¼ˆUltrathinkingç„¡åŠ¹ï¼‰")


class IntegratedMemoProcessor:
    """çµ±åˆãƒ¡ãƒ¢ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾å¿œï¼‰"""
    
    def __init__(self):
        # åŸºæœ¬è¨­å®š
        self.obsidian_path = "/Users/yoshiikatsuhiko/Library/Mobile Documents/iCloud~md~obsidian/Documents"
        self.inbox_path = "02_Inbox"
        
        # ç·¨é›†ã•ã‚ŒãŸåˆ†æçµæœã‚’ä¿æŒ
        self._last_edited_analysis = None
        
        # éŸ³å£°å…¥åŠ›ç”¨ã‚«ã‚¿ã‚«ãƒŠâ†’è‹±èªå¤‰æ›è¾æ›¸
        self.katakana_to_english = {
            # æŠ€è¡“ç³»
            'ãƒãƒ£ãƒƒãƒˆGPT': 'ChatGPT',
            'ãƒãƒ£ãƒƒãƒˆã‚¸ãƒ¼ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼': 'ChatGPT',
            'ã¡ã‚ƒã£ã¨GPT': 'ChatGPT',
            'ã‚ªãƒ–ã‚·ãƒ‡ã‚£ã‚¢ãƒ³': 'Obsidian',
            'ã‚«ãƒ¼ã‚½ãƒ«': 'Cursor',
            'ãƒ‘ã‚¤ã‚½ãƒ³': 'Python',
            'ã‚¸ãƒ£ãƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ': 'JavaScript',
            'ã‚®ãƒƒãƒˆãƒãƒ–': 'GitHub',
            'ã‚®ãƒƒãƒˆ': 'Git',
            'ã‚¨ãƒ¼ã‚¢ã‚¤': 'AI',
            'ã‚¨ãƒ¼ãƒ”ãƒ¼ã‚¢ã‚¤': 'API',
            'ãƒ‡ã‚£ãƒ¼ã‚¨ãƒƒã‚¯ã‚¹': 'DX',
            'ãƒ‡ã‚¸ã‚¿ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³': 'DX',
            # ãƒ“ã‚¸ãƒã‚¹ç³»
            'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ': 'Client',
            'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ': 'Project',
            'ã‚³ãƒ³ã‚µãƒ«': 'Consulting',
            'ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°': 'Consulting',
            'ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ': 'Approach',
            'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°': 'Marketing',
            'ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°': 'Meeting',
            'ã‚¢ãƒ‰ãƒãƒ³ã‚¹ãƒ‰': 'Advanced',
            'ãƒœã‚¤ã‚¹ãƒ¢ãƒ¼ãƒ‰': 'Voice Mode',
            # SNSç³»
            'ãƒ„ã‚¤ãƒƒã‚¿ãƒ¼': 'Twitter',
            'ã‚¨ãƒƒã‚¯ã‚¹': 'X',
            'ãƒ•ã‚§ã‚¤ã‚¹ãƒ–ãƒƒã‚¯': 'Facebook',
            'ã‚¤ãƒ³ã‚¹ã‚¿ã‚°ãƒ©ãƒ ': 'Instagram',
            'ãƒªãƒ³ã‚¯ãƒˆã‚¤ãƒ³': 'LinkedIn',
            # éŸ³æ¥½ç³»
            'ãƒ‡ã‚£ãƒŸãƒ‹ãƒƒã‚·ãƒ¥': 'Diminished',
            'ã‚»ãƒ–ãƒ³ã‚¹': '7th',
            'ã‚³ãƒ¼ãƒ‰': 'Chord',
            'ã‚¹ã‚±ãƒ¼ãƒ«': 'Scale',
            'ãƒ¡ã‚¸ãƒ£ãƒ¼': 'Major',
            'ãƒã‚¤ãƒŠãƒ¼': 'Minor'
        }
        
        # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®Ÿéš›ã®Obsidianãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã«åˆã‚ã›ã¦ä¿®æ­£ï¼‰
        self.category_folders = {
            'education': 'Education',
            'tech': 'Tech', 
            'business': 'Consulting',  # businessã‚«ãƒ†ã‚´ãƒªã¯Consultingãƒ•ã‚©ãƒ«ãƒ€ã«
            'ideas': 'Others',  # ideasã‚«ãƒ†ã‚´ãƒªã¯Othersãƒ•ã‚©ãƒ«ãƒ€ã«
            'music': 'Music',
            'media': 'Media',  # mediaã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ 
            'general': 'Others',
            'kindle': 'kindle',  # å°æ–‡å­—ã®ã¾ã¾
            'readwise': 'Others'  # readwiseã¯Othersã«çµ±åˆ
        }
        
        # å¼·åŒ–ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªåˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.category_keywords = {
            'music': [
                # åŸºæœ¬éŸ³æ¥½ç†è«–
                'ã‚³ãƒ¼ãƒ‰', 'ã‚¹ã‚±ãƒ¼ãƒ«', 'ãƒ‡ã‚£ãƒŸãƒ‹ãƒƒã‚·ãƒ¥', 'ãƒãƒ¼ãƒ¢ãƒ‹ãƒ¼', 'éŸ³éš', 'æ¥½ç†',
                # ã‚³ãƒ¼ãƒ‰ç¨®é¡
                'ãƒã‚¤ãƒŠãƒ¼ã‚³ãƒ¼ãƒ‰', 'ãƒ¡ã‚¸ãƒ£ãƒ¼ã‚³ãƒ¼ãƒ‰', 'ã‚»ãƒ–ãƒ³ã‚¹ã‚³ãƒ¼ãƒ‰', '7thã‚³ãƒ¼ãƒ‰', 'sus4', 'add9',
                # ã‚¹ã‚±ãƒ¼ãƒ«ç¨®é¡
                'ãƒ›ãƒ¼ãƒ«ãƒãƒ¼ãƒ•ãƒ‡ã‚£ãƒŸãƒ‹ãƒƒã‚·ãƒ¥', 'ãƒãƒ¼ãƒ•ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒŸãƒ‹ãƒƒã‚·ãƒ¥', 'ã‚¯ãƒ­ãƒãƒãƒƒã‚¯', 'ãƒšãƒ³ã‚¿ãƒˆãƒ‹ãƒƒã‚¯',
                # éŸ³æ¥½ç”¨èª
                'ãƒ«ãƒ¼ãƒˆ', 'ã‚µãƒ¼ãƒ‰', 'ãƒ•ã‚£ãƒ•ã‚¹', 'ã‚»ãƒ–ãƒ³ã‚¹', 'ç€åœ°éŸ³', 'ã‚³ãƒ¼ãƒ‰ãƒˆãƒ¼ãƒ³', 'é€²è¡Œ',
                # æ¥½å™¨ãƒ»æ¼”å¥
                'ãƒ”ã‚¢ãƒ', 'ã‚®ã‚¿ãƒ¼', 'ãƒ™ãƒ¼ã‚¹', 'ãƒ‰ãƒ©ãƒ ', 'æ¥½å™¨', 'æ¼”å¥', 'å¼¾ã', 'å¥ã§ã‚‹'
            ],
            'education': [
                # åŸºæœ¬æ•™è‚²ç”¨èª
                'æ•™è‚²', 'æŒ‡å°', 'æˆæ¥­', 'å­¦ç¿’', 'å›½èª', 'èª­è§£', 'è¡¨ç¾', 'ç”Ÿå¾’', 'å…ˆç”Ÿ', 'æ•™å¸«',
                # æ–‡å­¦ãƒ»å›½èªæŠ€æ³•
                'å¯¾å¥æ³•', 'ãƒªã‚ºãƒ ', 'éŸ³æ•°', 'éŸ»å¾‹', 'ä¿®è¾', 'æŠ€æ³•', 'æ–‡ä½“', 'è¡¨ç¾æ³•',
                # æˆæ¥­å ´é¢ã®èªå½™
                'ã‚·ãƒ¼ãƒ³', 'è€ƒãˆã¦', 'é¸ã³ãªã•ã„', 'æ€ã„å‡ºã™', 'æ€ã„æµ®ã‹ã¹', 'ã‚ã‹ã‚‹', 'ã²ã£ã‹ã‘',
                'å•é¡Œ', 'ç­”ãˆ', 'æ­£è§£', 'ä¸æ­£è§£', 'ä¾‹ãˆã°', 'ä»®ã«', 'å ´é¢', 'çŠ¶æ³', 'çŠ¶æ…‹',
                # è©•ä¾¡ãƒ»æŒ‡å°èª
                'ãã†ã§ã™ã­', 'ç´ æ™´ã‚‰ã—ã„', 'æ®‹å¿µ', 'æƒœã—ã„', 'ãã‚“', 'ã•ã‚“', 'ã¡ã‚ƒã‚“',
                # äº”æ„Ÿãƒ»æ„Ÿè¦šï¼ˆãƒ¡ãƒ­ãƒ‡ã‚£ãƒ¼ã¯éŸ³æ¥½ã‚«ãƒ†ã‚´ãƒªã«ç§»å‹•ï¼‰
                'è´è¦š', 'è¦–è¦š', 'è§¦è¦š', 'å—…è¦š', 'å‘³è¦š', 'äº”æ„Ÿ', 'æ„Ÿè¦š', 'ä½“ã®éƒ¨åˆ†'
            ],
            'tech': [
                'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'API', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚¢ãƒ—ãƒª', 'python', 'javascript', 
                'tech', 'æŠ€è¡“', 'é–‹ç™º', 'ã‚³ãƒ¼ãƒ‰', 'ãƒ‡ãƒ¼ã‚¿', 'AI', 'æ©Ÿæ¢°å­¦ç¿’', 'ChatGPT', 
                'ãƒãƒ£ãƒƒãƒˆGPT', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ©Ÿèƒ½', 'ãƒœã‚¤ã‚¹ãƒ¢ãƒ¼ãƒ‰', 'ã‚¢ãƒ‰ãƒãƒ³ã‚¹ãƒ‰', 'ãƒ„ãƒ¼ãƒ«'
            ],
            'business': [
                'ãƒ“ã‚¸ãƒã‚¹', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'æˆ¦ç•¥', 'å–¶æ¥­', 'é›†å®¢', 'SEO', 'SNS', 
                'åºƒå‘Š', 'å£²ä¸Š', 'åç›Š', 'é¡§å®¢', 'å¸‚å ´', 'ã‚³ãƒ³ã‚µãƒ«', 'ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°', 
                'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', 'ä¼šè­°', 'ã‚¢ãƒ‰ãƒã‚¤ã‚¹', 'ææ¡ˆ', 'è³‡æ–™'
            ],
            'media': [
                'SNS', 'X', 'Twitter', 'Instagram', 'Facebook', 'YouTube', 'TikTok',
                'ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼', 'ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'ãƒã‚¹ãƒˆ', 'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ',
                'ãƒ¡ãƒ‡ã‚£ã‚¢', 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„', 'å‹•ç”»', 'é…ä¿¡', 'ãƒ©ã‚¤ãƒ–'
            ],
            'ideas': [
                'ã‚¢ã‚¤ãƒ‡ã‚¢', 'ä¼ç”»', 'ææ¡ˆ', 'æ¡ˆ', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', 'å‰µä½œ', 'ç™ºæƒ³', 
                'ãƒ–ãƒ¬ã‚¹ãƒˆ', 'ã‚³ãƒ³ã‚»ãƒ—ãƒˆ', 'ãƒ—ãƒ©ãƒ³'
            ]
        }
    
    def _convert_katakana_to_english(self, text: str) -> str:
        """éŸ³å£°å…¥åŠ›ã®ã‚«ã‚¿ã‚«ãƒŠã‚’è‹±èªã«å¤‰æ›"""
        converted = text
        # é•·ã„èªå¥ã‹ã‚‰é †ã«å¤‰æ›ï¼ˆéƒ¨åˆ†ä¸€è‡´ã‚’é˜²ããŸã‚ï¼‰
        for katakana, english in sorted(self.katakana_to_english.items(), key=lambda x: len(x[0]), reverse=True):
            converted = converted.replace(katakana, english)
        return converted
    
    def _extract_person_names(self, text: str) -> list:
        """çµ±ä¸€ã•ã‚ŒãŸäººåæŠ½å‡ºãƒ¡ã‚½ãƒƒãƒ‰"""
        person_names = []
        
        # çµ±ä¸€ã•ã‚ŒãŸäººåæŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        person_patterns = [
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ–‡è„ˆè€ƒæ…®ã§ã®äººåæŠ½å‡º
            r'(?:^|[ã€ã€‚\s]|å±•é–‹ä¸­ã®|ã¨ã®|ã¸ã®|ã«ã‚ˆã‚‹)([ã-ã‚“ä¸€-é¾¥]{2,6})(?:ã•ã‚“|æ§˜|æ°)',
            r'(?:^|[ã€ã€‚\s]|å±•é–‹ä¸­ã®|ã¨ã®|ã¸ã®|ã«ã‚ˆã‚‹)([ã‚¡-ãƒ¶ãƒ¼]{2,6})(?:ã•ã‚“|æ§˜|æ°)',
            r'(?:^|[ã€ã€‚\s]|å±•é–‹ä¸­ã®|ã¨ã®|ã¸ã®|ã«ã‚ˆã‚‹)([A-Za-z]{3,10})(?:ã•ã‚“|æ§˜|æ°)',
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ä¸€èˆ¬çš„ãªåŠ©è©ã®å¾Œã®äººå
            r'(?:ã¨|ã«|ã¸|ã®)([ã-ã‚“ä¸€-é¾¥]{2,6})(?:ã•ã‚“|æ§˜|æ°)',
            r'(?:ã¨|ã«|ã¸|ã®)([ã‚¡-ãƒ¶ãƒ¼]{2,6})(?:ã•ã‚“|æ§˜|æ°)',
            r'(?:ã¨|ã«|ã¸|ã®)([A-Za-z]{3,10})(?:ã•ã‚“|æ§˜|æ°)'
        ]
        
        # é™¤å¤–ã™ã¹ãæ–‡å­—åˆ—
        exclude_patterns = ['ã‚’å±•é–‹', 'ä¸­ã®', 'ã‚’ä½¿', 'ã«ã¤ã„ã¦', 'ã§ã¯', 'ã¨ã®']
        
        for pattern in person_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                # é™¤å¤–æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
                if not any(exclude in match for exclude in exclude_patterns):
                    # é©åˆ‡ãªé•·ã•ã®äººåã®ã¿æ¡ç”¨
                    if 2 <= len(match) <= 6 and match not in person_names:
                        person_names.append(match)
        
        return person_names
    
    def _extract_concrete_headings(self, content: str) -> list:
        """å®Ÿéš›ã®è¦‹å‡ºã—èªã‹ã‚‰å…·ä½“çš„ãªè©±é¡Œã‚’æŠ½å‡º"""
        headings = []
        # ## è¦‹å‡ºã— ã‹ã‚‰å…·ä½“çš„ãªãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º
        heading_matches = re.findall(r'^##\s+(.+)$', content, re.MULTILINE)
        for heading in heading_matches:
            if len(heading) > 3 and heading not in ['è¦ç´„', 'ãƒã‚¤ãƒ³ãƒˆ', 'é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«', 'å†…å®¹']:
                # è¦‹å‡ºã—ã‚’ãã®ã¾ã¾å…·ä½“çš„ãªãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦ä½¿ç”¨
                headings.append(f"ã€Œ{heading}ã€ã®å…·ä½“çš„ãªè§£èª¬ã¨å®Ÿè·µæ–¹æ³•")
        return headings
    
    def _extract_key_paragraph_summaries(self, content: str) -> list:
        """é‡è¦ãªæ®µè½ã‹ã‚‰å…·ä½“çš„ãªè¦ç´„ã‚’æŠ½å‡º"""
        paragraphs = []
        # æ®µè½å˜ä½ã§åˆ†å‰²ï¼ˆæ”¹è¡Œ2ã¤ä»¥ä¸Šã§åŒºåˆ‡ã‚Šï¼‰
        para_blocks = re.split(r'\n\s*\n', content)
        
        for para in para_blocks:
            if len(para.strip()) >= 100:  # ååˆ†ãªé•·ã•ã®æ®µè½
                # æ®µè½ã®æœ€åˆã®1-2æ–‡ã‚’è¦ç´„ã¨ã—ã¦æŠ½å‡º
                sentences = re.split(r'[ã€‚ï¼]', para.strip())
                if sentences and len(sentences[0]) > 20:
                    summary = sentences[0]
                    if len(summary) > 50:
                        summary = summary[:47] + "..."
                    paragraphs.append(summary)
        
        return paragraphs[:3]
    
    def _extract_concrete_contextual_points(self, content: str) -> list:
        """æ–‡è„ˆã‹ã‚‰å…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨é–¢é€£æ€§ã‚’æŠ½å‡º"""
        points = []
        
        # é‡è¦ãªå›ºæœ‰åè© + å…·ä½“çš„ãªå‹•ä½œãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            r'([A-Z][a-zA-Z\s]+)(?:ã‚’ä½¿ã£ãŸ|ã«ã‚ˆã‚‹|ã§ã®)([^ã€‚]{10,50})',
            r'([ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,})(?:ã‚·ã‚¹ãƒ†ãƒ |æ‰‹æ³•|æ–¹æ³•|ã‚¬ã‚¤ãƒ‰)(?:ã®|ã‚’)([^ã€‚]{10,40})',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match) == 2:
                    points.append(f"{match[0]}ã‚’æ´»ç”¨ã—ãŸ{match[1].strip()}")
        
        # å…·ä½“çš„ãªæˆæœãƒ»åŠ¹æœã®è¨˜è¿°
        effect_patterns = [
            r'([^ã€‚]{15,50})(?:ã®åŠ¹æœ|ãŒç¢ºèª|ã‚’å®Ÿç¾|ãŒå‘ä¸Š)',
            r'([^ã€‚]{15,50})(?:ã™ã‚‹ã“ã¨ã§|ã«ã‚ˆã‚Š)[ã€ï¼Œ]?([^ã€‚]{10,30})',
        ]
        
        for pattern in effect_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match) >= 1:
                    effect_text = match[0].strip()
                    if len(effect_text) > 10:
                        points.append(f"{effect_text}ã«ã‚ˆã‚‹å…·ä½“çš„ãªæ”¹å–„åŠ¹æœ")
        
        return points[:3]
    
    def _filter_and_deduplicate_points(self, points: list) -> list:
        """ãƒã‚¤ãƒ³ãƒˆã®é‡è¤‡é™¤å»ã¨å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        filtered_points = []
        seen = set()
        
        for point in points:
            if not point or len(point.strip()) < 10:
                continue
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆé¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
            is_duplicate = False
            for seen_point in seen:
                # ç°¡æ˜“é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆå…±é€šèªæ•°ï¼‰
                point_words = set(point.split())
                seen_words = set(seen_point.split())
                overlap = len(point_words & seen_words)
                if overlap > len(point_words) * 0.7:  # 70%ä»¥ä¸Šé‡è¤‡
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                filtered_points.append(point.strip())
                seen.add(point.strip())
        
        return filtered_points[:6]  # æœ€å¤§6å€‹
    
    def preview_analysis(self, content: str) -> dict:
        """ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã®å®Œå…¨åˆ†æï¼ˆç·¨é›†çŠ¶æ…‹ä¿æŒå¯¾å¿œï¼‰"""
        try:
            # ğŸ”§ ç·¨é›†æ¸ˆã¿åˆ†æçµæœãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’è¿”ã™ï¼ˆä¸Šæ›¸ãé˜²æ­¢ï¼‰
            if self._last_edited_analysis:
                print("ğŸ“ ç·¨é›†æ¸ˆã¿åˆ†æçµæœã‚’ä½¿ç”¨ï¼ˆä¸Šæ›¸ãé˜²æ­¢ï¼‰")
                return self._last_edited_analysis
            
            print("ğŸ”„ çµ±åˆåˆ†æé–‹å§‹...")
            
            # éŸ³å£°å…¥åŠ›å¯¾å¿œï¼šã‚«ã‚¿ã‚«ãƒŠã‚’è‹±èªã«å¤‰æ›
            content = self._convert_katakana_to_english(content)
            
            # 1. ã‚«ãƒ†ã‚´ãƒªåˆ†æï¼ˆå¼·åŒ–ç‰ˆï¼‰
            category_result = self._enhanced_category_analysis(content)
            print(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ†æ: {category_result}")
            
            # 2. ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆä¸»é¡ŒæŠŠæ¡ï¼‰
            title_result = self._intelligent_title_generation(content, category_result)
            print(f"ğŸ“‹ ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ: {title_result}")
            
            # 3. ã‚¿ã‚°ç”Ÿæˆï¼ˆå¤šå±¤åˆ†æï¼‰
            tags_result = self._comprehensive_tag_generation(content, category_result)
            print(f"ğŸ·ï¸ ã‚¿ã‚°ç”Ÿæˆ: {tags_result}")
            
            # 4. é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
            relations_result = self._find_related_files(content, title_result['title'])
            print(f"ğŸ”— é–¢é€£åˆ†æ: {relations_result}")
            
            # 5. å†…å®¹è¦ç´„ç”Ÿæˆ
            summary_result = self._generate_content_summary(content)
            print(f"ğŸ“ è¦ç´„ç”Ÿæˆ: å®Œäº†")
            
            # 6. çµ±åˆçµæœæ§‹ç¯‰
            result = {
                'success': True,
                'category': category_result,
                'title': title_result,
                'tags': tags_result,
                'relations': relations_result,
                'summary': summary_result,
                'preview_info': self._build_preview_info(category_result, title_result, tags_result, relations_result),
                'timestamp': datetime.now().isoformat()
            }
            
            print("âœ… çµ±åˆåˆ†æå®Œäº†!")
            return result
            
        except Exception as e:
            print(f"âŒ çµ±åˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e),
                'category': {'name': 'general', 'confidence': 0.0},
                'title': {'title': 'ã‚¨ãƒ©ãƒ¼', 'method': 'error'},
                'tags': {'tags': ['#ã‚¨ãƒ©ãƒ¼'], 'count': 1},
                'relations': {'relations': [], 'count': 0}
            }
    
    def _enhanced_category_analysis(self, content: str) -> dict:
        """å¼·åŒ–ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªåˆ†æï¼ˆUltrathinkingçµ±åˆç‰ˆï¼‰"""
        
        # Ultrathinkingåˆ†æã‚’æœ€åˆã«è©¦è¡Œ
        if ULTRATHINKING_AVAILABLE:
            try:
                print("ğŸ§  Ultrathinking ã‚«ãƒ†ã‚´ãƒªåˆ†æä¸­...")
                analyzer = UltrathinkingAnalyzer()
                ultra_result = analyzer.analyze_content(content)
                
                if ultra_result and ultra_result.get('category'):
                    category = ultra_result.get('category', 'general')
                    confidence = 0.9  # Ultrathinking ã®é«˜ã„ä¿¡é ¼åº¦
                    
                    print(f"ğŸ¯ Ultrathinkingåˆ¤å®š: {category} (ä¿¡é ¼åº¦: {confidence})")
                    
                    return {
                        'name': category,
                        'confidence': confidence,
                        'scores': {category: 10.0},  # é«˜ã‚¹ã‚³ã‚¢
                        'pattern_matches': {},
                        'keyword_matches': {},
                        'method': 'ultrathinking'
                    }
                    
            except Exception as e:
                print(f"âš ï¸ Ultrathinkingåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                print("ğŸ“Š å¾“æ¥åˆ†æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        
        # å¾“æ¥ã®åˆ†ææ–¹å¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        print("ğŸ“Š å¾“æ¥æ–¹å¼ã§ã‚«ãƒ†ã‚´ãƒªåˆ†æä¸­...")
        
        content_lower = content.lower()
        
        # å€‹äººã¨ã®æ‰“ã¡åˆã‚ã›åˆ¤å®šï¼ˆæœ€å„ªå…ˆï¼‰
        person_meeting_patterns = [
            r'([ã-ã‚“ä¸€-é¾¥ã‚¡-ãƒ¶ãƒ¼A-Za-z]+)(?:ã•ã‚“|æ§˜|æ°)(?:ã¨ã®|ã¸|ã¨)(?:æ‰“ã¡åˆã‚ã›|ä¼šè­°|ç›¸è«‡|ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°)',
            r'([ã-ã‚“ä¸€-é¾¥ã‚¡-ãƒ¶ãƒ¼A-Za-z]+)(?:ã•ã‚“|æ§˜|æ°)(?:ã«|ã¸ã®)(?:ææ¡ˆ|å ±å‘Š|é€£çµ¡)',
            r'([ã-ã‚“ä¸€-é¾¥ã‚¡-ãƒ¶ãƒ¼A-Za-z]+)(?:ã•ã‚“|æ§˜|æ°)(?:é–¢é€£|ã«ã¤ã„ã¦)',
            r'([ã-ã‚“ä¸€-é¾¥ã‚¡-ãƒ¶ãƒ¼A-Za-z]+)(?:ã•ã‚“|æ§˜|æ°)(?:ã¨|ã¨ã®)(?:å”è­°|æ¤œè¨|ç›¸è«‡)'
        ]
        
        # å€‹äººã¨ã®æ‰“ã¡åˆã‚ã›ã‹ãƒã‚§ãƒƒã‚¯
        is_person_meeting = False
        for pattern in person_meeting_patterns:
            if re.search(pattern, content):
                is_person_meeting = True
                break
        
        # å€‹äººã¨ã®æ‰“ã¡åˆã‚ã›ã®å ´åˆã¯å¼·åˆ¶çš„ã«ãƒ“ã‚¸ãƒã‚¹ï¼ˆã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ï¼‰ã«åˆ†é¡
        if is_person_meeting:
            return {
                'name': 'business',
                'confidence': 1.0,
                'scores': {'business': 10, 'education': 0, 'tech': 0, 'media': 0, 'ideas': 0, 'music': 0},
                'pattern_matches': {'business': 10},
                'keyword_matches': {},
                'special_rule': 'person_meeting_detected'
            }
        
        # ãƒ“ã‚¸ãƒã‚¹å„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šï¼ˆæ•™è‚²ç³»ã§ã‚‚å¼·åˆ¶çš„ã«business/techã«ï¼‰
        business_priority_keywords = [
            'ã‚³ãƒ³ã‚µãƒ«', 'ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°', 'Consulting', 'consulting',
            'AIå°å…¥', 'DX', 'AIæ´»ç”¨', 'ã‚·ã‚¹ãƒ†ãƒ å°å…¥', 'ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–',
            'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ', 'Client', 'client',
            'ãƒ“ã‚¸ãƒã‚¹æˆ¦ç•¥', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'å£²ä¸Š', 'åç›Š',
            'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†', 'Projectç®¡ç†'
        ]
        
        tech_priority_keywords = [
            'ChatGPT', 'API', 'ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°',
            'GitHub', 'github', 'Obsidian', 'obsidian',
            'ã‚¢ãƒ—ãƒªé–‹ç™º', 'ãƒ„ãƒ¼ãƒ«é–‹ç™º', 'ãƒ‡ãƒ¼ã‚¿åˆ†æ'
        ]
        
        # AIé–¢é€£ã¯æ–‡è„ˆã§åˆ¤å®š
        ai_context_business = ['AIå°å…¥', 'AIæ´»ç”¨', 'AIã‚³ãƒ³ã‚µãƒ«', 'AIæˆ¦ç•¥']
        ai_context_tech = ['AIé–‹ç™º', 'AIæŠ€è¡“', 'AIã‚·ã‚¹ãƒ†ãƒ ', 'AIãƒ—ãƒ­ã‚°ãƒ©ãƒ ']
        
        # ãƒ“ã‚¸ãƒã‚¹å„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        has_business_priority = any(keyword in content for keyword in business_priority_keywords)
        has_tech_priority = any(keyword in content for keyword in tech_priority_keywords)
        
        # AIæ–‡è„ˆåˆ¤å®š
        has_ai_business_context = any(keyword in content for keyword in ai_context_business)
        has_ai_tech_context = any(keyword in content for keyword in ai_context_tech)
        
        # å˜ç´”ãªã€ŒAIã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®æ–‡è„ˆåˆ¤å®š
        if 'AI' in content and not has_ai_business_context and not has_ai_tech_context:
            # ä»–ã®ãƒ“ã‚¸ãƒã‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°ãƒ“ã‚¸ãƒã‚¹ã€æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°ãƒ†ãƒƒã‚¯
            if any(word in content for word in ['å°å…¥', 'ã‚³ãƒ³ã‚µãƒ«', 'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ', 'æˆ¦ç•¥']):
                has_business_priority = True
            elif any(word in content for word in ['é–‹ç™º', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒ ', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ãƒ„ãƒ¼ãƒ«']):
                has_tech_priority = True
        
        # AIæ–‡è„ˆå„ªå…ˆåˆ¤å®š
        if has_ai_business_context:
            return {
                'name': 'business',
                'confidence': 1.0,
                'scores': {'business': 9, 'education': 0, 'tech': 0, 'media': 0, 'ideas': 0, 'music': 0},
                'pattern_matches': {'business': 9},
                'keyword_matches': {},
                'special_rule': 'ai_business_context_detected'
            }
        
        if has_ai_tech_context:
            return {
                'name': 'tech',
                'confidence': 1.0,
                'scores': {'tech': 9, 'education': 0, 'business': 0, 'media': 0, 'ideas': 0, 'music': 0},
                'pattern_matches': {'tech': 9},
                'keyword_matches': {},
                'special_rule': 'ai_tech_context_detected'
            }
        
        if has_business_priority:
            return {
                'name': 'business',
                'confidence': 1.0,
                'scores': {'business': 8, 'education': 0, 'tech': 0, 'media': 0, 'ideas': 0, 'music': 0},
                'pattern_matches': {'business': 8},
                'keyword_matches': {},
                'special_rule': 'business_priority_keyword_detected'
            }
        
        if has_tech_priority:
            return {
                'name': 'tech',
                'confidence': 1.0,
                'scores': {'tech': 8, 'education': 0, 'business': 0, 'media': 0, 'ideas': 0, 'music': 0},
                'pattern_matches': {'tech': 8},
                'keyword_matches': {},
                'special_rule': 'tech_priority_keyword_detected'
            }
        
        # æ•™è‚²ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆå³å¯†åŒ–ï¼‰
        education_patterns = [
            r'[ã-ã‚“ãƒ¼]+ãã‚“[ã¯ãŒã‚’ã«ã§ã€ã€‚]|[ã-ã‚“ãƒ¼]+ã•ã‚“[ã¯ãŒã‚’ã«ã§ã€ã€‚]',  # ç”Ÿå¾’åï¼ˆæ–‡è„ˆä»˜ãï¼‰
            r'ã‚ã‹ã‚‹ï¼Ÿ|ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ|ç†è§£ã§ããŸï¼Ÿ',  # æ•™å¸«ã®ç¢ºèª
            r'æ­£è§£ã§ã™|ä¸æ­£è§£ã§ã™|ã‚ˆãã§ãã¾ã—ãŸ',  # æ˜ç¢ºãªè©•ä¾¡
            r'é¸ã³ãªã•ã„|ç­”ãˆãªã•ã„|æ›¸ããªã•ã„',  # æ˜ç¢ºãªæŒ‡ç¤º
            r'ãƒ†ã‚¹ãƒˆ|è©¦é¨“|æˆæ¥­|å®¿é¡Œ|èª²é¡Œ',  # æ•™è‚²æ–‡è„ˆ
            r'å›½èª|ç®—æ•°|ç†ç§‘|ç¤¾ä¼š|è‹±èª',  # æ•™ç§‘
        ]
        
        # ãƒ“ã‚¸ãƒã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        business_patterns = [
            r'Client|Consulting|Project|Meeting',  # ãƒ“ã‚¸ãƒã‚¹è‹±èª
            r'ä¼šè­°|æ‰“ã¡åˆã‚ã›|å•†è«‡|ææ¡ˆ',  # ãƒ“ã‚¸ãƒã‚¹æ—¥æœ¬èª
            r'è³‡æ–™|ãƒ¬ãƒãƒ¼ãƒˆ|ãƒ—ãƒ¬ã‚¼ãƒ³',  # ãƒ“ã‚¸ãƒã‚¹æ–‡æ›¸
            r'æˆ¦ç•¥|æ–½ç­–|æ–¹é‡|è¨ˆç”»',  # ãƒ“ã‚¸ãƒã‚¹è¨ˆç”»
        ]
        
        # ãƒ†ãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        tech_patterns = [
            r'ChatGPT|AI|API|GitHub|Obsidian',  # ãƒ†ãƒƒã‚¯å›ºæœ‰åè©
            r'æ©Ÿèƒ½|ãƒ„ãƒ¼ãƒ«|ã‚·ã‚¹ãƒ†ãƒ |ã‚¢ãƒ—ãƒª',  # ãƒ†ãƒƒã‚¯ä¸€èˆ¬ç”¨èª
            r'ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰|ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰|ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«',  # ãƒ†ãƒƒã‚¯å‹•ä½œ
        ]
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢
        pattern_scores = defaultdict(int)
        
        # æ•™è‚²ãƒ‘ã‚¿ãƒ¼ãƒ³
        for pattern in education_patterns:
            if re.search(pattern, content):
                pattern_scores['education'] += 2  # ã‚¹ã‚³ã‚¢ã‚’é©æ­£åŒ–
        
        # ãƒ“ã‚¸ãƒã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
        for pattern in business_patterns:
            if re.search(pattern, content):
                pattern_scores['business'] += 3  # ãƒ“ã‚¸ãƒã‚¹ã¯é«˜ã‚¹ã‚³ã‚¢
        
        # ãƒ†ãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³
        for pattern in tech_patterns:
            if re.search(pattern, content):
                pattern_scores['tech'] += 3  # ãƒ†ãƒƒã‚¯ã‚‚é«˜ã‚¹ã‚³ã‚¢
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢
        keyword_scores = defaultdict(int)
        
        for category, keywords in self.category_keywords.items():
            for keyword in keywords:
                if keyword in content_lower:
                    keyword_scores[category] += 1
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        total_scores = defaultdict(float)
        
        for category in self.category_keywords.keys():
            pattern_score = pattern_scores.get(category, 0)
            keyword_score = keyword_scores.get(category, 0)
            
            # éŸ³æ¥½ã‚«ãƒ†ã‚´ãƒªã¯éŸ³æ¥½ç†è«–ç”¨èªã§é«˜ã‚¹ã‚³ã‚¢
            if category == 'music':
                music_theory_terms = ['ãƒ‡ã‚£ãƒŸãƒ‹ãƒƒã‚·ãƒ¥', 'ã‚¹ã‚±ãƒ¼ãƒ«', 'ã‚³ãƒ¼ãƒ‰', 'ã‚»ãƒ–ãƒ³ã‚¹', 'ãƒ«ãƒ¼ãƒˆ', 'ã‚µãƒ¼ãƒ‰', 'ãƒ•ã‚£ãƒ•ã‚¹']
                music_bonus = sum(2 for term in music_theory_terms if term in content)
                total_scores[category] = pattern_score + keyword_score + music_bonus
            # æ•™è‚²ã‚«ãƒ†ã‚´ãƒªã®ç‰¹åˆ¥æ‰±ã„ã‚’å‰Šé™¤
            elif category == 'education':
                total_scores[category] = pattern_score + keyword_score
            else:
                total_scores[category] = pattern_score + keyword_score
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ
        if total_scores:
            best_category = max(total_scores, key=total_scores.get)
            confidence = total_scores[best_category] / max(1, len(content.split()) * 0.1)
            confidence = min(1.0, confidence)  # 1.0ã‚’ä¸Šé™ã¨ã™ã‚‹
        else:
            best_category = 'general'
            confidence = 0.1
        
        return {
            'name': best_category,
            'confidence': confidence,
            'scores': dict(total_scores),
            'pattern_matches': dict(pattern_scores),
            'keyword_matches': dict(keyword_scores)
        }
    
    def _intelligent_title_generation(self, content: str, category_result: dict) -> dict:
        """çŸ¥çš„ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆUltrathinkingçµ±åˆç‰ˆï¼‰"""
        
        # Ultrathinking ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã‚’æœ€åˆã«è©¦è¡Œ
        if ULTRATHINKING_AVAILABLE:
            try:
                print("ğŸ§  Ultrathinking ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆä¸­...")
                analyzer = UltrathinkingAnalyzer()
                ultra_result = analyzer.analyze_content(content)
                
                if ultra_result and ultra_result.get('title'):
                    title = ultra_result.get('title', '')
                    confidence = 0.95  # éå¸¸ã«é«˜ã„ä¿¡é ¼åº¦
                    
                    print(f"ğŸ¯ Ultrathinking ã‚¿ã‚¤ãƒˆãƒ«: {title}")
                    
                    return {
                        'title': title,
                        'method': 'ultrathinking',
                        'alternatives': [],
                        'confidence': confidence
                    }
                    
            except Exception as e:
                print(f"âš ï¸ Ultrathinking ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                print("ğŸ“Š å¾“æ¥æ–¹å¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        
        # å¾“æ¥ã®ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆæ–¹å¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        print("ğŸ“Š å¾“æ¥æ–¹å¼ã§ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆä¸­...")
        
        # è¤‡æ•°ã®æ‰‹æ³•ã‚’è©¦è¡Œ
        methods = []
        
        # 0. æœ€åˆã®æ–‡ã‹ã‚‰ä¸»é¡Œã‚’æŠ½å‡ºï¼ˆæœ€å„ªå…ˆï¼‰
        first_sentence_title = self._extract_first_sentence_theme(content)
        if first_sentence_title:
            methods.append({'method': 'first_sentence', 'title': first_sentence_title, 'score': 4.0})
        
        # 1. ä¸»é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        theme_title = self._extract_theme_title(content)
        if theme_title:
            methods.append({'method': 'theme_pattern', 'title': theme_title, 'score': 3.0})
        
        # 2. ã‚«ãƒ†ã‚´ãƒªç‰¹åŒ–ã‚¿ã‚¤ãƒˆãƒ«
        category_title = self._generate_category_specific_title(content, category_result['name'])
        if category_title:
            methods.append({'method': 'category_specific', 'title': category_title, 'score': 2.5})
        
        # 3. é‡è¦èªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        cluster_title = self._generate_cluster_title(content)
        if cluster_title:
            methods.append({'method': 'word_clustering', 'title': cluster_title, 'score': 2.0})
        
        # 4. æ ¸å¿ƒå†…å®¹æŠ½å‡º
        core_title = self._extract_core_content_title(content)
        if core_title:
            methods.append({'method': 'core_content', 'title': core_title, 'score': 1.5})
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®æ‰‹æ³•ã‚’é¸æŠ
        if methods:
            best_method = max(methods, key=lambda x: x['score'])
            return {
                'title': best_method['title'],
                'method': best_method['method'],
                'alternatives': [m for m in methods if m != best_method],
                'confidence': best_method['score'] / 3.0
            }
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            fallback_title = f"ãƒ¡ãƒ¢_{datetime.now().strftime('%m%d_%H%M')}"
            return {
                'title': fallback_title,
                'method': 'fallback',
                'alternatives': [],
                'confidence': 0.1
            }
    
    def _extract_first_sentence_theme(self, content: str) -> str:
        """æœ€åˆã®æ–‡ã‹ã‚‰ä¸»é¡Œã‚’æŠ½å‡ºã—ã¦è¨€ã„åˆ‡ã‚Šå½¢ã®ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆ20-50æ–‡å­—ï¼‰"""
        # æœ€åˆã®æ–‡ã‚’å–å¾—
        sentences = re.split(r'[ã€‚ï¼ï¼ï¼Ÿ\n]', content)
        if not sentences:
            return ""
        
        first_sentence = sentences[0].strip()
        if len(first_sentence) < 5:
            return ""
        
        # å†—é•·ãªè¡¨ç¾ã‚’å‰Šé™¤
        clean_sentence = re.sub(r'(ã®ã§|ãŸã‚|ã®ã‚ˆã†ã«|ã¨ã„ã†ã®ã¯|ã¨ã„ã†ã“ã¨ã§|ã¨ã„ã£ãŸ|ãªã©|ã¨æ€ã„ã¾ã™|ãªã®ã‹ãªã¨æ€ã£ã¦ã„ã‚‹ã¨ã“ã‚ã§ã™|ã«å‘ãåˆã£ã¦ã„ã“ã†ã¨æ€ã„ã¾ã™)', '', first_sentence)
        
        # çµ±ä¸€ã•ã‚ŒãŸäººåæŠ½å‡ºï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ»è¦ç´„å…±é€šï¼‰
        person_names = self._extract_person_names(clean_sentence)
        
        # é‡è¦ãªå›ºæœ‰åè©ã¨æ¦‚å¿µã‚’æŠ½å‡ºï¼ˆäººåä»¥å¤–ï¼‰
        entities = re.findall(r'(?:ChatGPT|Projectæ©Ÿèƒ½|Project|ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ©Ÿèƒ½|ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ|Consulting|ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°|Client|ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ|Voice Mode|ãƒœã‚¤ã‚¹ãƒ¢ãƒ¼ãƒ‰|ã‚¢ãƒ‰ãƒãƒ³ã‚¹ãƒˆãƒœã‚¤ã‚¹ãƒ¢ãƒ¼ãƒ‰)', clean_sentence)
        actions = re.findall(r'(?:æ´»ç”¨|åˆ©ç”¨|å°å…¥|å®Ÿè£…|æ¤œè¨|åˆ†æ|è©•ä¾¡|é‹ç”¨|æ”¹å–„|è“„ç©|ç«‹ã¡ä¸Šã’|æ‰“ã¡åˆã‚ã›|ä¼šè­°|ç›¸è«‡|å ±å‘Š|ç¢ºèª|ä¾é ¼)', clean_sentence)
        targets = re.findall(r'(?:èª²é¡Œè§£æ±º|ä¼šè­°å±¥æ­´|è­°äº‹éŒ²|è³‡æ–™|ã‚„ã‚Šã¨ã‚Š|ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°|ã‚µãƒ¼ãƒ“ã‚¹|æˆ¦ç•¥|æ–¹é‡|è¨ˆç”»|ææ¡ˆ)', clean_sentence)
        
        # è¨€ã„åˆ‡ã‚Šå½¢ã‚¿ã‚¤ãƒˆãƒ«ã®ç”Ÿæˆï¼ˆäººåã‚’æœ€å„ªå…ˆï¼‰
        if person_names:
            # äººåãŒã‚ã‚‹å ´åˆã¯å¿…ãšå«ã‚ã‚‹
            main_person = person_names[0]
            
            if actions and targets:
                # äººå + ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ + å¯¾è±¡
                main_action = actions[0]
                main_target = targets[0]
                title = f"{main_person}ã•ã‚“ã¨ã®{main_target}{main_action}éŒ²"
            elif actions:
                # äººå + ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
                main_action = actions[0]
                if main_action in ['æ‰“ã¡åˆã‚ã›', 'ä¼šè­°', 'ç›¸è«‡']:
                    title = f"{main_person}ã•ã‚“ã¨ã®{main_action}ãƒ¡ãƒ¢"
                else:
                    title = f"{main_person}ã•ã‚“ã¸ã®{main_action}å†…å®¹"
            elif targets:
                # äººå + å¯¾è±¡
                main_target = targets[0]
                title = f"{main_person}ã•ã‚“é–¢é€£{main_target}ã¾ã¨ã‚"
            else:
                # äººåã®ã¿
                title = f"{main_person}ã•ã‚“ã¨ã®å”è­°äº‹é …"
                
        elif entities and actions and targets:
            # 3è¦ç´ æƒã£ãŸå ´åˆï¼šã€ŒChatGPTã‚’æ´»ç”¨ã—ãŸèª²é¡Œè§£æ±ºæ‰‹æ³•ã€
            main_entity = entities[0].replace('ã®', '')
            main_action = actions[0]
            main_target = targets[0]
            title = f"{main_entity}ã‚’{main_action}ã—ãŸ{main_target}æ‰‹æ³•"
            
        elif entities and actions:
            # 2è¦ç´ ã®å ´åˆï¼šã€ŒChatGPTãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ©Ÿèƒ½ã®æ´»ç”¨æ–¹æ³•ã€
            main_entity = entities[0].replace('ã®', '')
            main_action = actions[0]
            title = f"{main_entity}ã®{main_action}æ–¹æ³•"
            
        elif entities:
            # å›ºæœ‰åè©ã®ã¿ã®å ´åˆ
            main_entity = entities[0].replace('ã®', '')
            if 'ã‚³ãƒ³ã‚µãƒ«' in clean_sentence or 'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ' in clean_sentence:
                title = f"{main_entity}ã‚’æ´»ç”¨ã—ãŸã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥"
            elif 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ' in clean_sentence or 'Project' in clean_sentence:
                title = f"{main_entity}ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã®å®Ÿè·µæ³•"
            else:
                title = f"{main_entity}ã®åŠ¹æœçš„æ´»ç”¨æ³•"
                
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šé‡è¦èªå¥ã‹ã‚‰æ§‹æˆ
            important_phrases = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{4,12}', clean_sentence)
            if len(important_phrases) >= 2:
                title = f"{important_phrases[0]}ã¨{important_phrases[1]}ã®é€£æºæ‰‹æ³•"
            elif important_phrases:
                title = f"{important_phrases[0]}ã®å®Ÿè·µçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"
            else:
                title = "æ–°ã—ã„æ¥­å‹™æ”¹å–„æ‰‹æ³•"
        
        # æ–‡å­—æ•°èª¿æ•´ï¼ˆ20-50æ–‡å­—ï¼‰
        if len(title) < 20:
            # çŸ­ã™ãã‚‹å ´åˆã¯è£œå®Œ
            if 'ChatGPT' in title:
                title = title.replace('ã®', 'æ©Ÿèƒ½ã®').replace('ã‚’', 'ãƒ„ãƒ¼ãƒ«ã‚’')
            # äººåãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯éåº¦ãªè£œå®Œã‚’é¿ã‘ã‚‹
            elif not person_names and len(title) < 20:
                title += "ã«ã‚ˆã‚‹æ¥­å‹™åŠ¹ç‡åŒ–"
                
        elif len(title) > 50:
            # é•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®
            title = title[:47] + "..."
        
        return self._clean_title_text(title)
    
    def _create_natural_method_summary(self, methods: list) -> str:
        """è¤‡æ•°ã®æ‰‹æ®µã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«çµ±åˆ"""
        if not methods:
            return ""
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡
        project_methods = [m for m in methods if 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ' in m or 'Project' in m]
        data_methods = [m for m in methods if 'è³‡æ–™' in m or 'è­°äº‹éŒ²' in m or 'è“„ç©' in m]
        comm_methods = [m for m in methods if 'ãƒãƒ£ãƒƒãƒˆ' in m or 'ãƒœã‚¤ã‚¹' in m]
        
        summary_parts = []
        
        if project_methods:
            summary_parts.append("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†")
        
        if data_methods:
            summary_parts.append("æƒ…å ±ã®ä¸€å…ƒç®¡ç†")
        
        if comm_methods:
            summary_parts.append("å¤šæ§˜ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not summary_parts:
            summary_parts = methods[:2]
        
        return "ãƒ»".join(summary_parts[:2])
    
    def _extract_theme_title(self, content: str) -> str:
        """ä¸»é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º"""
        
        patterns = [
            # ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æœ€å„ªå…ˆ
            r'(.{5,30})ã§ã¯ã€Œ(.{5,25})ã€ã¨ã€Œ(.{5,25})ã€',  # ã€Œã€œã§ã¯ã€ŒAã€ã¨ã€ŒBã€ã€
            r'(.{5,30})ã§ã¯ã€Œ(.{5,25})ã€',  # ã€Œã€œã§ã¯ã€ŒAã€ã€
            r'(.{5,30})ã§ã¯ã€(.{5,25})ã€',  # ã€Œã€œã§ã¯ã€Aã€ã€
            r'ã€Œ(.{5,30})ã®(.{5,25})ã€',  # ã€ŒAã®Bã€
            r'ã€(.{5,30})ã®(.{5,25})ã€',  # ã€Aã®Bã€
            # ä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
            r'(.{5,25})ã«ã¤ã„ã¦[ã¯è©±ã—èª¬æ˜è€ƒãˆè§£èª¬]',
            r'(.{5,25})ã¨ã¯[^ã-ã‚“]{0,10}',
            r'(.{5,25})ã‚’[è€ƒãˆèª¬æ˜è§£èª¬æ¤œè¨åˆ†æ]',
            r'(.{5,25})ã«é–¢ã—ã¦',
            r'(.{5,25})ã«ãŠã‘ã‚‹',
            r'é‡è¦ãªã®ã¯(.{5,25})',
            r'ãƒã‚¤ãƒ³ãƒˆã¯(.{5,25})',
            r'ã€Œ(.{5,30})ã€[ã¨ã¨ã„ã†]',
            r'ã€(.{5,30})ã€[ã¨ã¨ã„ã†]',
            # æœ€å¾Œã«ç·©ã„ãƒ‘ã‚¿ãƒ¼ãƒ³
            r'(.{5,25})ã®[å•é¡Œèª²é¡ŒåŠ¹æœæ–¹æ³•æ‰‹é †è§£èª¬èª¬æ˜]',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                # è¤‡æ•°ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒã‚ã‚‹å ´åˆã¯æœ€ã‚‚æ„å‘³ã®ã‚ã‚‹ã‚‚ã®ã‚’é¸æŠ
                groups = match.groups()
                theme = None
                
                # è¤‡æ•°ã‚°ãƒ«ãƒ¼ãƒ—ã®å‡¦ç†
                if len(groups) == 3 and all(g for g in groups):
                    # ã€ŒAã§ã¯Bã¨Cã€ãƒ‘ã‚¿ãƒ¼ãƒ³
                    first, second, third = groups[0].strip(), groups[1].strip(), groups[2].strip()
                    theme = f"{first}ã®{second}ã¨{third}"
                    if len(theme) > 30:
                        theme = f"{first}ã®{second}"
                elif len(groups) == 2 and all(g for g in groups):
                    # ã€ŒAã§ã¯Bã€ã¾ãŸã¯ã€ŒAã®Bã€ãƒ‘ã‚¿ãƒ¼ãƒ³
                    first, second = groups[0].strip(), groups[1].strip()
                    if len(first) > 3 and len(second) > 3:
                        theme = f"{first}ã®{second}"
                        if len(theme) > 30:
                            theme = second if len(second) < len(first) else first
                    else:
                        theme = first if len(first) > len(second) else second
                else:
                    # å˜ä¸€ã‚°ãƒ«ãƒ¼ãƒ—ã¾ãŸã¯ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                    for group in groups:
                        if group and len(group.strip()) > 2 and not self._is_meaningless_phrase(group.strip()):
                            theme = group.strip()
                            break
                
                if theme:
                    return self._clean_title_text(theme)
        
        return ""
    
    def _generate_category_specific_title(self, content: str, category: str) -> str:
        """ã‚«ãƒ†ã‚´ãƒªç‰¹åŒ–ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ"""
        
        if category == 'education':
            # æ•™è‚²ã‚«ãƒ†ã‚´ãƒªã®ç‰¹æ®Šå‡¦ç†
            if 'å¯¾å¥æ³•' in content and 'ãƒªã‚ºãƒ ' in content:
                return "å¯¾å¥æ³•ã¨ãƒªã‚ºãƒ åˆ†æ"
            elif 'è´è¦š' in content and 'äº”æ„Ÿ' in content:
                return "è´è¦šã¨äº”æ„Ÿã®å­¦ç¿’"
            elif 'ã²ã£ã‹ã‘' in content and 'å•é¡Œ' in content:
                return "ã²ã£ã‹ã‘å•é¡Œã®æŒ‡å°"
            elif re.search(r'[ã-ã‚“ãƒ¼]+ãã‚“', content):
                return "æˆæ¥­è¨˜éŒ²ã¨ç”Ÿå¾’æŒ‡å°"
            elif 'æŒ‡å°' in content:
                return "æ•™è‚²æŒ‡å°ãƒ¡ãƒ¢"
            else:
                return "æ•™è‚²é–¢é€£è¨˜éŒ²"
        
        elif category == 'tech':
            # å›ºæœ‰åè©ï¼ˆã‚«ã‚¿ã‚«ãƒŠãƒ»è‹±èªï¼‰ã‚’å„ªå…ˆçš„ã«æŠ½å‡º
            katakana_entities = re.findall(r'[ã‚¢-ãƒ¶ãƒ¼]{3,10}', content)
            english_entities = re.findall(r'[A-Z][a-z]+|ChatGPT|Python|API|JavaScript|React|Vue|Node|Git|GitHub|Docker|AWS|Azure|GCP|Obsidian', content)
            tech_terms = re.findall(r'API|ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°|ã‚·ã‚¹ãƒ†ãƒ |ã‚¢ãƒ—ãƒª|é–‹ç™º|æŠ€è¡“', content, re.IGNORECASE)
            
            # å›ºæœ‰åè©ã‚’å«ã‚€ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ
            entities = []
            if katakana_entities:
                # ä¸€èˆ¬èªã‚’é™¤å¤–
                common_katakana = {'ã‚¢ã‚¤ãƒ‡ã‚¢', 'ãƒ¡ãƒ¢', 'ãƒ•ã‚¡ã‚¤ãƒ«', 'ãƒ•ã‚©ãƒ«ãƒ€', 'ãƒšãƒ¼ã‚¸', 'ã‚µã‚¤ãƒˆ', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ãƒ‡ãƒ¼ã‚¿', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'ã‚µãƒ¼ãƒ“ã‚¹'}
                entities.extend([e for e in katakana_entities if e not in common_katakana])
            if english_entities:
                entities.extend(english_entities)
            
            if len(entities) >= 2:
                return f"{entities[0]}ã¨{entities[1]}ã®é–‹ç™º"
            elif len(entities) == 1:
                if tech_terms:
                    return f"{entities[0]}{tech_terms[0]}"
                else:
                    return f"{entities[0]}é–‹ç™ºãƒ¡ãƒ¢"
            elif tech_terms:
                return f"{tech_terms[0]}ã«é–¢ã™ã‚‹æŠ€è¡“ãƒ¡ãƒ¢"
            else:
                return "æŠ€è¡“é–¢é€£ãƒ¡ãƒ¢"
        
        elif category == 'business':
            # ãƒ“ã‚¸ãƒã‚¹å›ºæœ‰åè©ã‚’æŠ½å‡º
            katakana_entities = re.findall(r'[ã‚¢-ãƒ¶ãƒ¼]{3,10}', content)
            english_entities = re.findall(r'[A-Z][a-z]+|Instagram|Twitter|Facebook|LinkedIn|YouTube|TikTok|Google|Amazon|Apple', content)
            biz_terms = re.findall(r'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°|æˆ¦ç•¥|å–¶æ¥­|é›†å®¢|SEO|ãƒ“ã‚¸ãƒã‚¹', content, re.IGNORECASE)
            
            entities = []
            if katakana_entities:
                common_katakana = {'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'ãƒ“ã‚¸ãƒã‚¹', 'ã‚µãƒ¼ãƒ“ã‚¹', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'ãƒ‡ãƒ¼ã‚¿'}
                entities.extend([e for e in katakana_entities if e not in common_katakana])
            if english_entities:
                entities.extend(english_entities)
                
            if len(entities) >= 1:
                if biz_terms:
                    return f"{entities[0]}{biz_terms[0]}"
                else:
                    return f"{entities[0]}ãƒ“ã‚¸ãƒã‚¹æˆ¦ç•¥"
            elif biz_terms:
                return f"{biz_terms[0]}æˆ¦ç•¥ãƒ¡ãƒ¢"
            else:
                return "ãƒ“ã‚¸ãƒã‚¹é–¢é€£ãƒ¡ãƒ¢"
        
        elif category == 'ideas':
            # ã‚¢ã‚¤ãƒ‡ã‚¢ç³»ã‚‚å›ºæœ‰åè©ã‚’å«ã‚ã‚‹
            katakana_entities = re.findall(r'[ã‚¢-ãƒ¶ãƒ¼]{3,10}', content)
            english_entities = re.findall(r'[A-Z][a-z]+', content)
            
            entities = []
            if katakana_entities:
                entities.extend([e for e in katakana_entities[:2]])
            if english_entities:
                entities.extend([e for e in english_entities[:2]])
                
            if entities:
                return f"{entities[0]}ã‚¢ã‚¤ãƒ‡ã‚¢"
            else:
                return "ã‚¢ã‚¤ãƒ‡ã‚¢ãƒ»ä¼ç”»ãƒ¡ãƒ¢"
        
        return ""
    
    def _generate_cluster_title(self, content: str) -> str:
        """é‡è¦èªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆå›ºæœ‰åè©å„ªå…ˆï¼‰"""
        
        # å›ºæœ‰åè©ã‚’æœ€å„ªå…ˆã§æŠ½å‡º
        entities = []
        
        # ã‚«ã‚¿ã‚«ãƒŠå›ºæœ‰åè©
        katakana_entities = re.findall(r'[ã‚¢-ãƒ¶ãƒ¼]{3,10}', content)
        common_katakana = {'ã‚¢ã‚¤ãƒ‡ã‚¢', 'ãƒ¡ãƒ¢', 'ãƒ•ã‚¡ã‚¤ãƒ«', 'ãƒ•ã‚©ãƒ«ãƒ€', 'ãƒšãƒ¼ã‚¸', 'ã‚µã‚¤ãƒˆ', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ãƒ‡ãƒ¼ã‚¿', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'ã‚µãƒ¼ãƒ“ã‚¹', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'ãƒ“ã‚¸ãƒã‚¹'}
        entities.extend([e for e in katakana_entities if e not in common_katakana])
        
        # è‹±èªå›ºæœ‰åè©
        english_entities = re.findall(r'[A-Z][a-z]+|ChatGPT|Python|API|JavaScript|Obsidian|GitHub|Instagram|Twitter|Facebook|Google|Amazon|Apple', content)
        entities.extend(english_entities)
        
        # é‡è¦ãªæ—¥æœ¬èªèªå½™
        important_words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8}', content)
        word_freq = Counter(important_words)
        
        # ä»£è¡¨çš„ãªèªå½™ï¼ˆé »å‡ºãƒ»å°‚é–€ç”¨èªãƒ»å‹•ä½œèªï¼‰
        representative_words = []
        for word, freq in word_freq.items():
            if (freq >= 2 and len(word) > 2 and not self._is_common_word(word)) or \
               word in ['é–‹ç™º', 'è¨­è¨ˆ', 'å®Ÿè£…', 'åˆ†æ', 'æ¤œè¨', 'æ§‹ç¯‰', 'ç”Ÿæˆ', 'é€£æº', 'æ´»ç”¨', 'è§£é‡ˆ', 'ç†è§£', 'æŒ‡å°', 'å­¦ç¿’', 'æˆæ¥­', 'è¨˜éŒ²']:
                representative_words.append(word)
        
        # ã‚¿ã‚¤ãƒˆãƒ«æ§‹ç¯‰ã®å„ªå…ˆé †ä½
        if len(entities) >= 2:
            return f"{entities[0]}ã¨{entities[1]}"
        elif len(entities) == 1 and len(representative_words) >= 1:
            return f"{entities[0]}{representative_words[0]}"
        elif len(entities) == 1:
            return f"{entities[0]}ã«ã¤ã„ã¦"
        elif len(representative_words) >= 2:
            return f"{representative_words[0]}ã¨{representative_words[1]}"
        elif len(representative_words) == 1:
            return f"{representative_words[0]}ã«é–¢ã—ã¦"
        
        return ""
    
    def _extract_core_content_title(self, content: str) -> str:
        """æ ¸å¿ƒå†…å®¹ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º"""
        
        sentences = re.split(r'[ã€‚ï¼ï¼ï¼Ÿ\n]', content)
        
        # å°å…¥æ–‡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ ¸å¿ƒéƒ¨åˆ†ã‚’è¦‹ã¤ã‘ã‚‹
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            
            # è³ªå•æ–‡ã‚„æŒ¨æ‹¶ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if (len(sentence) > 15 and 
                not sentence.endswith('ã§ã™ã‹ï¼Ÿ') and
                not sentence.endswith('ã ã‚ã†ã‹ï¼Ÿ') and
                not sentence.startswith('ã“ã®ã‚·ãƒ¼ãƒ³') and
                not sentence.startswith('ãã†ã§ã™ã­')):
                
                # æ–‡ã®ä¸»è¦éƒ¨åˆ†ã‚’æŠ½å‡º
                core_part = self._extract_sentence_core(sentence)
                if core_part:
                    return core_part
        
        return ""
    
    def _extract_sentence_core(self, sentence: str) -> str:
        """æ–‡ã®æ ¸å¿ƒéƒ¨åˆ†ã‚’æŠ½å‡º"""
        
        # ã€Œã€œã¯ã€œã§ã™ã€ã€Œã€œãŒã€œã™ã‚‹ã€ç­‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ä¸»èªãƒ»è¿°èªã‚’æŠ½å‡º
        patterns = [
            r'([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8})[ã¯ãŒ]([^ã€‚]{5,20})',
            r'([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8})ã‚’([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8})',
            r'([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8})ã¨ã„ã†([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, sentence)
            if match:
                subject = match.group(1)
                predicate = match.group(2)
                if len(subject) > 2 and len(predicate) > 2:
                    return f"{subject}ã®{predicate[:6]}"
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ–‡ã®æœ€åˆã®é‡è¦èªå¥
        words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,8}', sentence)
        important_words = [w for w in words if not self._is_common_word(w)]
        
        if important_words:
            return important_words[0]
        
        return ""
    
    def _comprehensive_tag_generation(self, content: str, category_result: dict) -> dict:
        """file-organizerå¼6å±¤ã‚¿ã‚°ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆUltrathinkingçµ±åˆç‰ˆï¼‰"""
        
        # Ultrathinking ã‚¿ã‚°ç”Ÿæˆã‚’æœ€åˆã«è©¦è¡Œ
        if ULTRATHINKING_AVAILABLE:
            try:
                print("ğŸ§  Ultrathinking ã‚¿ã‚°ç”Ÿæˆä¸­...")
                analyzer = UltrathinkingAnalyzer()
                ultra_result = analyzer.analyze_content(content)
                
                if ultra_result and ultra_result.get('tags'):
                    ultra_tags = ultra_result.get('tags', [])
                    confidence = 0.9
                    
                    print(f"ğŸ·ï¸ Ultrathinking ã‚¿ã‚°: {ultra_tags[:5]}...")  # æœ€åˆã®5å€‹è¡¨ç¤º
                    
                    return {
                        'tags': ultra_tags,
                        'count': len(ultra_tags),
                        'layer_info': 'ultrathinking_enhanced',
                        'method': 'ultrathinking',
                        'confidence': confidence
                    }
                    
            except Exception as e:
                print(f"âš ï¸ Ultrathinking ã‚¿ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                print("ğŸ“Š å¾“æ¥æ–¹å¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        
        # å¾“æ¥ã®ã‚¿ã‚°ç”Ÿæˆæ–¹å¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        print("ğŸ“Š å¾“æ¥æ–¹å¼ã§ã‚¿ã‚°ç”Ÿæˆä¸­...")
        
        try:
            category = category_result['name']
            tags = set()
            
            # Layer 1: æœ€å„ªå…ˆ - å›ºæœ‰åè©ãƒ»å°‚é–€ç”¨èªï¼ˆé‡ã¿: 3å€ï¼‰
            try:
                priority_tags = self._extract_priority_entities(content, category)
                for tag in priority_tags:
                    tags.add(f"PRIORITY:{tag}")
            except:
                pass
            
            # Layer 2: ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã‚¿ã‚°ï¼ˆé‡ã¿: 2å€ï¼‰
            try:
                category_tags = self._get_category_base_tags(category)
                for tag in category_tags:
                    keywords = self._get_category_keywords(category, tag)
                    if any(keyword in content for keyword in keywords):
                        tags.add(f"CATEGORY:{tag}")
            except:
                pass
            
            # Layer 3: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ»å‹•ä½œã‚¿ã‚°
            try:
                action_tags = self._extract_action_tags_enhanced(content)
                tags.update(action_tags)
            except:
                pass
            
            # Layer 4: æ„Ÿæƒ…ãƒ»ãƒˆãƒ¼ãƒ³ã‚¿ã‚°
            try:
                emotion_tags = self._extract_emotion_tags_enhanced(content)
                tags.update(emotion_tags)
            except:
                pass
            
            # Layer 5: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚¿ã‚°
            try:
                content_type_tags = self._extract_content_type_tags_enhanced(content)
                tags.update(content_type_tags)
            except:
                pass
            
            # Layer 6: é »å‡ºèªã‚¿ã‚°ï¼ˆ2å›ä»¥ä¸Šå‡ºç¾ï¼‰
            try:
                frequent_tags = self._extract_frequent_terms_enhanced(content)
                tags.update(frequent_tags)
            except:
                pass
            
            # å„ªå…ˆåº¦ã«åŸºã¥ã„ã¦ã‚¿ã‚°ã‚’ã‚½ãƒ¼ãƒˆãƒ»é¸æŠ
            final_tags = self._prioritize_and_select_tags(tags, content)
            
            # ç©ºã®å ´åˆã¯å¾“æ¥æ–¹å¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not final_tags:
                fallback_tags = self._extract_priority_terms(content, category)
                final_tags = [f"#{tag}" for tag in fallback_tags]
            
            prioritized_tags = final_tags[:12]  # æœ€å¤§12å€‹
            
            return {
                'tags': prioritized_tags,
                'count': len(prioritized_tags),
                'layer_info': '6-layer hierarchical system',
                'method': 'file-organizer_enhanced'
            }
            
        except Exception as e:
            print(f"âš ï¸ ã‚¿ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼ã€å¾“æ¥æ–¹å¼ã‚’ä½¿ç”¨: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¾“æ¥ã®æ–¹å¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            fallback_tags = self._extract_priority_terms(content, category)
            return {
                'tags': [f"#{tag}" for tag in fallback_tags],
                'count': len(fallback_tags),
                'method': 'fallback'
            }
    
    def _extract_priority_entities(self, content: str, category: str) -> set:
        """Layer 1: æœ€å„ªå…ˆå›ºæœ‰åè©ãƒ»å°‚é–€ç”¨èªæŠ½å‡º"""
        entities = set()
        
        # å…¨ã‚«ãƒ†ã‚´ãƒªå…±é€šã®é‡è¦å›ºæœ‰åè©ã‚’å…ˆã«ãƒã‚§ãƒƒã‚¯
        universal_entities = {
            'ChatGPT': ['ChatGPT', 'chatgpt', 'Chat GPT'],
            'GitHub': ['GitHub', 'github', 'Github'],
            'Obsidian': ['Obsidian', 'obsidian'],
            'AI': ['AI', 'A.I.'],
            'API': ['API'],
            'Claude': ['Claude', 'claude'],
            'Python': ['Python', 'python'],
            'JavaScript': ['JavaScript', 'javascript', 'JS'],
            'Project': ['Project', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ'],
            'Client': ['Client', 'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ'],
            'Consulting': ['Consulting', 'ã‚³ãƒ³ã‚µãƒ«', 'ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°']
        }
        
        for entity, patterns in universal_entities.items():
            if any(pattern in content for pattern in patterns):
                entities.add(entity)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®è¿½åŠ æŠ½å‡º
        if category == 'education':
            # å­¦æ ¡åï¼ˆæœ€å„ªå…ˆï¼‰
            school_patterns = {
                'é–‹æˆ': ['é–‹æˆä¸­å­¦', 'é–‹æˆ'],
                'éº»å¸ƒ': ['éº»å¸ƒä¸­å­¦', 'éº»å¸ƒ'],
                'é§’æ±': ['é§’å ´æ±é‚¦', 'é§’æ±'],
                'æ¡œè”­': ['æ¡œè”­ä¸­å­¦', 'æ¡œè”­'],
                'å¥³å­å­¦é™¢': ['å¥³å­å­¦é™¢', 'JG'],
                'é›™è‘‰': ['é›™è‘‰ä¸­å­¦', 'é›™è‘‰'],
                'ç­‘é§’': ['ç­‘æ³¢å¤§é§’å ´', 'ç­‘é§’'],
                'æ¸‹å¹•': ['æ¸‹è°·å¹•å¼µ', 'æ¸‹å¹•'],
                'æ­¦è”µ': ['æ­¦è”µä¸­å­¦', 'æ­¦è”µ'],
                'SAPIX': ['ã‚µãƒ”ãƒƒã‚¯ã‚¹', 'SAPIX', 'ã‚µãƒ”']
            }
            for school, patterns in school_patterns.items():
                if any(pattern in content for pattern in patterns):
                    entities.add(school)
                    
        elif category == 'tech':
            # æŠ€è¡“å›ºæœ‰åè©ï¼ˆæœ€å„ªå…ˆï¼‰
            tech_entities = {
                'Claude': ['Claude', 'claude'],
                'ChatGPT': ['ChatGPT', 'chatgpt', 'Chat GPT'],
                'GitHub': ['GitHub', 'github', 'Github'],
                'Python': ['Python', 'python'],
                'JavaScript': ['JavaScript', 'javascript', 'JS'],
                'Cursor': ['Cursor', 'cursor'],
                'Obsidian': ['Obsidian', 'obsidian'],
                'MCP': ['MCP', 'mcp'],
                'Supabase': ['Supabase', 'supabase']
            }
            for entity, patterns in tech_entities.items():
                if any(pattern in content for pattern in patterns):
                    entities.add(entity)
                    
        elif category == 'media':
            # ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ»SNSå›ºæœ‰åè©ï¼ˆæœ€å„ªå…ˆï¼‰
            media_entities = {
                'è¥¿æ‘å‰µä¸€æœ—': ['è¥¿æ‘å‰µä¸€æœ—', 'è¥¿æ‘'],
                'è¥¿å·å°†å²': ['è¥¿å·å°†å²', 'è¥¿å·'],
                'æ¢¶è°·å¥äºº': ['æ¢¶è°·å¥äºº', 'æ¢¶è°·'],
                'Xåˆ†æ': ['Xåˆ†æ', 'ï¼¸åˆ†æ'],
                'SNSåˆ†æ': ['SNSåˆ†æ', 'ãƒã‚¹ãƒˆåˆ†æ', 'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†æ'],
                'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ': ['ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'ã„ã„ã­', 'ãƒªãƒã‚¹ãƒˆ']
            }
            for entity, patterns in media_entities.items():
                if any(pattern in content for pattern in patterns):
                    entities.add(entity)
        
        # ä¸€èˆ¬çš„ãªé‡è¦å›ºæœ‰åè©
        general_entities = re.findall(r'\\b[A-Z][a-zA-Z]{3,15}\\b', content)
        for entity in general_entities:
            if len(entity) >= 4 and not re.match(r'^[A-Z]{3,4}$', entity):
                entities.add(entity)
        
        return entities
    
    def _get_category_base_tags(self, category: str) -> list:
        """Layer 2: ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã‚¿ã‚°å®šç¾©"""
        category_base_tags = {
            'education': ['ä¸­å­¦å—é¨“', 'å›½èªæŒ‡å°', 'éå»å•åˆ†æ', 'å…¥è©¦å¯¾ç­–', 'èª­è§£æŒ‡å°', 'è¡¨ç¾æŒ‡å°'],
            'tech': ['ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'AIé–‹ç™º', 'ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰', 'APIé€£æº', 'ãƒ‡ãƒ¼ã‚¿åˆ†æ', 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°'],
            'media': ['SNSæˆ¦ç•¥', 'SNSé‹ç”¨', 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ', 'ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼åˆ†æ', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ†æ'],
            'business': ['ãƒ“ã‚¸ãƒã‚¹æˆ¦ç•¥', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥', 'å£²ä¸Šåˆ†æ', 'é¡§å®¢ç²å¾—', 'ãƒ–ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°'],
            'ideas': ['ã‚¢ã‚¤ãƒ‡ã‚¢å‰µå‡º', 'ä¼ç”»ç«‹æ¡ˆ', 'ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°'],
            'general': ['ãƒ¡ãƒ¢', 'è¨˜éŒ²', 'æ•´ç†']
        }
        return category_base_tags.get(category, [])
    
    def _get_category_keywords(self, category: str, tag: str) -> list:
        """ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚¿ã‚°åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—"""
        keyword_map = {
            'education': {
                'ä¸­å­¦å—é¨“': ['ä¸­å­¦å—é¨“', 'å—é¨“', 'å…¥è©¦', 'åˆæ ¼'],
                'å›½èªæŒ‡å°': ['å›½èª', 'èª­è§£', 'è¡¨ç¾', 'æ–‡ç« '],
                'éå»å•åˆ†æ': ['éå»å•', 'å‡ºé¡Œå‚¾å‘', 'åˆ†æ'],
            },
            'tech': {
                'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°': ['ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚³ãƒ¼ãƒ‰', 'é–‹ç™º'],
                'AIé–‹ç™º': ['AI', 'æ©Ÿæ¢°å­¦ç¿’', 'Claude', 'ChatGPT'],
                'APIé€£æº': ['API', 'é€£æº', 'æ¥ç¶š'],
            },
            'media': {
                'SNSæˆ¦ç•¥': ['SNS', 'æˆ¦ç•¥', 'X', 'Twitter'],
                'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ†æ': ['ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'ã„ã„ã­', 'ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼'],
            }
        }
        return keyword_map.get(category, {}).get(tag, [])
    
    def _extract_action_tags_enhanced(self, content: str) -> set:
        """Layer 3: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ»å‹•ä½œã‚¿ã‚°æŠ½å‡º"""
        actions = set()
        action_patterns = {
            'å­¦ç¿’': ['å­¦ç¿’', 'å‹‰å¼·', 'ç¿’å¾—', 'ç†è§£'],
            'åˆ†æ': ['åˆ†æ', 'è§£æ', 'èª¿æŸ»', 'æ¤œè¨¼'],
            'è¨˜éŒ²': ['è¨˜éŒ²', 'ãƒ¡ãƒ¢', 'ä¿å­˜', 'æ•´ç†'],
            'è¨ˆç”»': ['è¨ˆç”»', 'æˆ¦ç•¥', 'è¨­è¨ˆ', 'ä¼ç”»'],
            'å®Ÿè¡Œ': ['å®Ÿè¡Œ', 'å®Ÿæ–½', 'å®Ÿè£…', 'é–‹ç™º'],
            'è©•ä¾¡': ['è©•ä¾¡', 'æ¤œè¨', 'åˆ¤æ–­', 'ç¢ºèª']
        }
        
        for action, keywords in action_patterns.items():
            if any(keyword in content for keyword in keywords):
                actions.add(action)
        
        return actions
    
    def _extract_emotion_tags_enhanced(self, content: str) -> set:
        """Layer 4: æ„Ÿæƒ…ãƒ»ãƒˆãƒ¼ãƒ³ã‚¿ã‚°æŠ½å‡º"""
        emotions = set()
        emotion_patterns = {
            'é‡è¦': ['é‡è¦', 'å¤§åˆ‡', 'å¿…é ˆ', '!', 'ï¼'],
            'ç–‘å•': ['ï¼Ÿ', '?', 'ã©ã†', 'ãªãœ', 'ã©ã®ã‚ˆã†ã«'],
            'ãƒã‚¸ãƒ†ã‚£ãƒ–': ['ç´ æ™´ã‚‰ã—ã„', 'è‰¯ã„', 'æˆåŠŸ', 'æ”¹å–„'],
            'èª²é¡Œ': ['èª²é¡Œ', 'å•é¡Œ', 'æ”¹å–„', 'å¯¾ç­–'],
            'ç™ºè¦‹': ['ç™ºè¦‹', 'æ°—ã¥ã', 'å­¦ã³', 'ã²ã‚‰ã‚ã']
        }
        
        for emotion, keywords in emotion_patterns.items():
            if any(keyword in content for keyword in keywords):
                emotions.add(emotion)
        
        return emotions
    
    def _extract_content_type_tags_enhanced(self, content: str) -> set:
        """Layer 5: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚¿ã‚°æŠ½å‡º"""
        content_types = set()
        type_patterns = {
            'ã‚¢ã‚¤ãƒ‡ã‚¢': ['ã‚¢ã‚¤ãƒ‡ã‚¢', 'æ¡ˆ', 'ææ¡ˆ', 'æ€ã„ã¤ã'],
            'ãƒ¬ãƒãƒ¼ãƒˆ': ['çµæœ', 'å ±å‘Š', 'ãƒ¬ãƒãƒ¼ãƒˆ', 'ã¾ã¨ã‚'],
            'ãƒ¡ãƒ¢': ['ãƒ¡ãƒ¢', 'è¦šæ›¸', 'å‚™å¿˜éŒ²'],
            'ãƒ„ãƒ¼ãƒ«': ['ãƒ„ãƒ¼ãƒ«', 'é“å…·', 'ã‚¢ãƒ—ãƒª', 'ã‚µãƒ¼ãƒ“ã‚¹'],
            'ãƒ—ãƒ­ã‚»ã‚¹': ['æ‰‹é †', 'ã‚¹ãƒ†ãƒƒãƒ—', 'ãƒ—ãƒ­ã‚»ã‚¹', 'æ–¹æ³•']
        }
        
        for content_type, keywords in type_patterns.items():
            if any(keyword in content for keyword in keywords):
                content_types.add(content_type)
        
        return content_types
    
    def _extract_frequent_terms_enhanced(self, content: str) -> set:
        """Layer 6: é »å‡ºèªã‚¿ã‚°æŠ½å‡ºï¼ˆ2å›ä»¥ä¸Šå‡ºç¾ï¼‰"""
        frequent_terms = set()
        
        # æ—¥æœ¬èªã®æ„å‘³ã®ã‚ã‚‹èªï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰ã‚’æŠ½å‡ºã€ãŸã ã—éŸ³æ¥½ç”¨èªã¯é©åˆ‡ã«å‡¦ç†
        japanese_words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,8}', content)
        
        # éŸ³æ¥½ç†è«–ç”¨èªã®ç‰¹åˆ¥å‡¦ç†
        music_terms = ['ãƒ‡ã‚£ãƒŸãƒ‹ãƒƒã‚·ãƒ¥ã‚¹ã‚±ãƒ¼ãƒ«', 'ãƒ›ãƒ¼ãƒ«ãƒãƒ¼ãƒ•ãƒ‡ã‚£ãƒŸãƒ‹ãƒƒã‚·ãƒ¥', 'ãƒãƒ¼ãƒ•ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒŸãƒ‹ãƒƒã‚·ãƒ¥']
        for term in music_terms:
            if term in content:
                frequent_terms.add(term)
        
        word_counts = Counter(japanese_words)
        
        for word, count in word_counts.items():
            if (count >= 2 and len(word) >= 3 and 
                not re.match(r'^[ã‚ã„ã†ãˆãŠã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ãªã«ã¬ã­ã®ã¯ã²ãµã¸ã»ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“]+$', word) and
                word not in ['ãƒ‡ã‚£ãƒŸãƒ‹', 'ãƒƒã‚·ãƒ¥ã‚¹', 'ãƒ¼ãƒ«ãƒãƒ¼']):  # ä¸å®Œå…¨ãªåˆ‡æ–­èªã¯é™¤å¤–
                frequent_terms.add(word)
        
        return frequent_terms
    
    def _prioritize_and_select_tags(self, tags: set, content: str) -> list:
        """å„ªå…ˆåº¦ã«åŸºã¥ã„ã¦ã‚¿ã‚°ã‚’ã‚½ãƒ¼ãƒˆãƒ»é¸æŠ"""
        prioritized_tags = []
        
        # å„ªå…ˆåº¦é †ã§ã‚¿ã‚°ã‚’å‡¦ç†
        priority_order = ['PRIORITY:', 'CATEGORY:', '']
        
        for prefix in priority_order:
            matching_tags = [tag for tag in tags if tag.startswith(prefix)]
            
            # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ãªã‚¿ã‚°ã«
            clean_tags = []
            for tag in matching_tags:
                clean_tag = tag.replace('PRIORITY:', '').replace('CATEGORY:', '')
                if len(clean_tag) >= 2:
                    clean_tags.append(f"#{clean_tag}")
            
            prioritized_tags.extend(clean_tags)
        
        # é‡è¤‡é™¤å»ã—ã¦é †åºä¿æŒ
        seen = set()
        final_tags = []
        for tag in prioritized_tags:
            if tag not in seen:
                seen.add(tag)
                final_tags.append(tag)
        
        return final_tags
    
    def _extract_priority_terms(self, content: str, category: str) -> set:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æœ€å„ªå…ˆå›ºæœ‰åè©æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        tags = set()
        
        if category == 'education':
            # å­¦æ ¡åã‚’æœ€å„ªå…ˆã§æŠ½å‡º
            school_names = ['é–‹æˆ', 'éº»å¸ƒ', 'é§’æ±', 'æ¡œè”­', 'å¥³å­å­¦é™¢', 'é›™è‘‰', 'ç­‘é§’', 'æ¸‹å¹•', 'æ¸‹æ¸‹', 'æ­¦è”µ', 'æµ·åŸ']
            for school in school_names:
                if school in content:
                    tags.add(school)
            
            # é‡è¦ãªæ•™è‚²ç”¨èª
            key_terms = ['ä¸­å­¦å—é¨“', 'å›½èª', 'éå»å•', 'å…¥è©¦', 'åˆ†æ', 'å‚¾å‘', 'å¯¾ç­–', 'SAPIX', 'ã‚µãƒ”ãƒƒã‚¯ã‚¹', 'å››è°·å¤§å¡š', 'æ—¥èƒ½ç ”']
            for term in key_terms:
                if term in content:
                    tags.add(term)
                    
        elif category == 'tech':
            # é‡è¦ãªæŠ€è¡“ç”¨èª
            key_tech = ['GitHub', 'Git', 'Python', 'JavaScript', 'API', 'ChatGPT', 'Claude', 'AI', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚¢ãƒ—ãƒª', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'é–‹ç™º', 'ãƒˆãƒ¼ã‚¯ãƒ³', 'èªè¨¼']
            for term in key_tech:
                if term in content or term.lower() in content.lower():
                    tags.add(term)
                    
        elif category == 'media':
            # é‡è¦ãªãƒ¡ãƒ‡ã‚£ã‚¢ãƒ»SNSç”¨èª
            key_media = ['X', 'Twitter', 'SNS', 'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ', 'ãƒã‚¹ãƒˆ', 'ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼', 'ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼', 'åˆ†æ', 'è¥¿æ‘å‰µä¸€æœ—', 'è¥¿å·å°†å²', 'æ¢¶è°·å¥äºº', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ']
            for term in key_media:
                if term in content:
                    tags.add(term)
        
        return tags
    
    def _extract_important_terms(self, content: str) -> list:
        """é‡è¦ãªç”¨èªã‚’æŠ½å‡ºï¼ˆã‚«ãƒ†ã‚´ãƒªä¸å•ï¼‰"""
        important_terms = []
        
        # å…¨ã‚«ãƒ†ã‚´ãƒªå…±é€šã®é‡è¦å›ºæœ‰åè©
        universal_entities = {
            'ChatGPT': ['ChatGPT', 'chatgpt', 'Chat GPT', 'ãƒãƒ£ãƒƒãƒˆGPT'],
            'GitHub': ['GitHub', 'github', 'Github'],
            'Obsidian': ['Obsidian', 'obsidian', 'ã‚ªãƒ–ã‚·ãƒ‡ã‚£ã‚¢ãƒ³'],
            'AI': ['AI', 'A.I.', 'ã‚¨ãƒ¼ã‚¢ã‚¤'],
            'API': ['API', 'ã‚¨ãƒ¼ãƒ”ãƒ¼ã‚¢ã‚¤'],
            'Claude': ['Claude', 'claude'],
            'Python': ['Python', 'python', 'ãƒ‘ã‚¤ã‚½ãƒ³'],
            'JavaScript': ['JavaScript', 'javascript', 'JS', 'ã‚¸ãƒ£ãƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ'],
            'Project': ['Project', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ'],
            'Client': ['Client', 'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ'],
            'Consulting': ['Consulting', 'ã‚³ãƒ³ã‚µãƒ«', 'ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°']
        }
        
        # å›ºæœ‰åè©ã®ãƒãƒƒãƒãƒ³ã‚°
        for entity, patterns in universal_entities.items():
            if any(pattern in content for pattern in patterns):
                important_terms.append(entity)
        
        # æŠ€è¡“é–¢é€£ã®é‡è¦ç”¨èª
        tech_terms = ['ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚¢ãƒ—ãƒª', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'é–‹ç™º', 'ãƒ‡ãƒ¼ã‚¿', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ„ãƒ¼ãƒ«']
        for term in tech_terms:
            if term in content:
                important_terms.append(term)
        
        # æ•™è‚²é–¢é€£ã®é‡è¦ç”¨èª
        education_terms = ['æ•™è‚²', 'å­¦ç¿’', 'æŒ‡å°', 'æˆæ¥­', 'å›½èª', 'åˆ†æ', 'å¯¾ç­–', 'ä¸­å­¦å—é¨“']
        for term in education_terms:
            if term in content:
                important_terms.append(term)
        
        # ãƒ“ã‚¸ãƒã‚¹é–¢é€£ã®é‡è¦ç”¨èª
        business_terms = ['ãƒ“ã‚¸ãƒã‚¹', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'æˆ¦ç•¥', 'å–¶æ¥­', 'é›†å®¢', 'SEO', 'SNS']
        for term in business_terms:
            if term in content:
                important_terms.append(term)
        
        # ãƒ¡ãƒ‡ã‚£ã‚¢é–¢é€£ã®é‡è¦ç”¨èª
        media_terms = ['X', 'Twitter', 'Instagram', 'ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'ãƒã‚¹ãƒˆ', 'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ']
        for term in media_terms:
            if term in content:
                important_terms.append(term)
        
        # é »å‡ºã™ã‚‹æ—¥æœ¬èªã®é‡è¦èªã‚’è¿½åŠ 
        japanese_words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,8}', content)
        word_counts = Counter(japanese_words)
        
        # 2å›ä»¥ä¸Šå‡ºç¾ã—ã€ä¸€èˆ¬çš„ã§ãªã„èªã‚’è¿½åŠ 
        for word, count in word_counts.items():
            if (count >= 2 and len(word) >= 3 and 
                not self._is_common_word(word) and
                word not in important_terms):
                important_terms.append(word)
        
        # é‡è¤‡ã‚’é™¤å»ã—ã¦è¿”ã™
        return list(dict.fromkeys(important_terms))

    def _find_related_files(self, content: str, title: str) -> dict:
        """file-organizerå¼å¼·åŒ–é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢"""
        
        try:
            vault_path = Path(self.obsidian_path)
            relations = []
            
            # æ—¢å­˜ã®markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆãƒ—ãƒ­ã‚°ãƒ©ãƒ é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–ï¼‰
            all_md_files = list(vault_path.rglob('*.md'))
            md_files = self._filter_non_program_files(all_md_files)
            
            # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šChatGPTé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡º
            chatgpt_files_found = 0
            files_processed = 0
            
            for md_file in md_files:
                try:
                    files_processed += 1
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                    with open(md_file, 'r', encoding='utf-8') as f:
                        file_content = f.read()
                    
                    # ChatGPTé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡º
                    if any(keyword in file_content for keyword in ['ChatGPT', 'ãƒãƒ£ãƒƒãƒˆGPT', 'chatgpt']):
                        chatgpt_files_found += 1
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
                    file_title = md_file.stem
                    
                    # é–¢é€£åº¦ã‚’è¨ˆç®—ï¼ˆéšå±¤çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
                    relation_score = self._calculate_hierarchical_relation_score(
                        content, file_content, title, file_title
                    )
                    
                    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å³æ ¼ãªé–¾å€¤è¨­å®šï¼ˆé–¢é€£åº¦å‘ä¸Šï¼‰
                    threshold = self._get_relation_threshold(title, file_title)
                    
                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆé–‹ç™ºæ™‚ã®ã¿æœ‰åŠ¹ï¼‰
                    # if any(keyword in file_content for keyword in ['ChatGPT', 'ãƒãƒ£ãƒƒãƒˆGPT', 'chatgpt']):
                    #     print(f"ğŸ” ChatGPTé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«: {file_title}")
                    #     print(f"   ã‚¹ã‚³ã‚¢: {relation_score:.3f}, é–¾å€¤: {threshold:.3f}")
                    
                    if relation_score > threshold:
                        # æ˜Ÿè©•ä¾¡ã‚’è¨ˆç®—
                        star_rating = self._calculate_star_rating(relation_score)
                        
                        relations.append({
                            'file_path': str(md_file),
                            'file_name': file_title,
                            'score': relation_score,
                            'star_rating': star_rating,
                            'relation_type': self._determine_relation_type_enhanced(content, file_content),
                            'preview': file_content[:100] + "..." if len(file_content) > 100 else file_content
                        })
                
                except Exception as e:
                    print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({md_file}): {e}")
                    continue
            
            # é–‹ç™ºãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆæœ¬ç•ªã§ã¯ç„¡åŠ¹ï¼‰
            # print(f"ğŸ“Š ChatGPTé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {chatgpt_files_found}/{files_processed}")
            # print(f"ğŸ“Š é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºæ•°: {len(relations)}")
            
            # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            relations.sort(key=lambda x: x['score'], reverse=True)
            
            return {
                'relations': relations[:3],  # ä¸Šä½3ä»¶ï¼ˆç²¾åº¦å‘ä¸Šï¼‰
                'count': len(relations),
                'total_files_checked': len(md_files)
            }
            
        except Exception as e:
            return {
                'relations': [],
                'count': 0,
                'error': str(e)
            }
    
    def _filter_non_program_files(self, md_files: list) -> list:
        """ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–ã—ã¦é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«å€™è£œã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        filtered_files = []
        
        # é™¤å¤–ã™ã¹ããƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³
        program_file_patterns = [
            # READMEç³»
            r'^README.*',
            r'^readme.*',
            r'^Readme.*',
            
            # è¨­å®šãƒ»æ§‹æˆãƒ•ã‚¡ã‚¤ãƒ«
            r'^CHANGELOG.*',
            r'^LICENSE.*',
            r'^CONTRIBUTING.*',
            r'^INSTALL.*',
            r'^USAGE.*',
            
            # ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãƒ»ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–¢é€£
            r'.*\.py\.md$',
            r'.*\.js\.md$',
            r'.*\.ts\.md$',
            r'.*\.json\.md$',
            r'.*\.yaml\.md$',
            r'.*\.yml\.md$',
            
            # æŠ€è¡“æ–‡æ›¸ï¼ˆAPIä»•æ§˜ç­‰ï¼‰
            r'^API.*',
            r'^api.*',
            r'.*_api\.md$',
            r'.*-api\.md$',
            
            # é–‹ç™ºè€…å‘ã‘æ–‡æ›¸
            r'^DEVELOPER.*',
            r'^developer.*',
            r'^DEV.*',
            r'^dev.*',
        ]
        
        # é™¤å¤–ã™ã¹ããƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¿ãƒ¼ãƒ³
        program_dir_patterns = [
            r'.*[/\\]\.git[/\\].*',
            r'.*[/\\]node_modules[/\\].*',
            r'.*[/\\]__pycache__[/\\].*',
            r'.*[/\\]\.vscode[/\\].*',
            r'.*[/\\]\.idea[/\\].*',
            r'.*[/\\]docs[/\\]api[/\\].*',
            r'.*[/\\]documentation[/\\].*',
        ]
        
        for md_file in md_files:
            file_name = md_file.name
            file_path_str = str(md_file)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åãƒã‚§ãƒƒã‚¯
            is_program_file = any(re.match(pattern, file_name, re.IGNORECASE) 
                                for pattern in program_file_patterns)
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ãƒã‚§ãƒƒã‚¯
            is_in_program_dir = any(re.match(pattern, file_path_str, re.IGNORECASE) 
                                  for pattern in program_dir_patterns)
            
            # å†…å®¹ãƒã‚§ãƒƒã‚¯ï¼ˆREADMEç­‰ã®ç¢ºå®Ÿãªé™¤å¤–ï¼‰
            is_program_content = self._is_program_related_content(md_file)
            
            if not (is_program_file or is_in_program_dir or is_program_content):
                filtered_files.append(md_file)
        
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(md_files)} â†’ {len(filtered_files)} (ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–¢é€£é™¤å¤–)")
        return filtered_files
    
    def _is_program_related_content(self, file_path: Path) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‹ã‚‰ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–¢é€£æ–‡æ›¸ã‹ã‚’åˆ¤å®š"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()[:500]  # æœ€åˆã®500æ–‡å­—ã®ã¿ãƒã‚§ãƒƒã‚¯
            
            # ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            program_keywords = [
                '# Installation', '## Installation', '# Usage', '## Usage',
                '# API', '## API', '```bash', '```shell', '```sh',
                'npm install', 'pip install', 'yarn add', 'composer install',
                '## Quick Start', '## Getting Started', '# Getting Started',
                'git clone', 'docker run', 'docker-compose',
                '# Requirements', '## Requirements', '# Dependencies',
                '# Configuration', '## Configuration'
            ]
            
            # 3å€‹ä»¥ä¸Šã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã¯é™¤å¤–
            keyword_count = sum(1 for keyword in program_keywords if keyword in content)
            return keyword_count >= 3
            
        except Exception:
            return False
    
    def _calculate_hierarchical_relation_score(self, content1: str, content2: str, title1: str, title2: str) -> float:
        """file-organizerå¼éšå±¤çš„é–¢é€£åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        max_score = 0.0
        
        # 0. é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç›´æ¥ãƒãƒƒãƒãƒ³ã‚°ï¼ˆæœ€å„ªå…ˆï¼‰
        important_keywords = [
            'ChatGPT', 'ãƒãƒ£ãƒƒãƒˆGPT', 'chatgpt', 'API', 'GitHub', 'Obsidian', 'AI', 'Claude',
            'Project', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', 'Consulting', 'ã‚³ãƒ³ã‚µãƒ«', 'Client', 'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ',
            'Python', 'JavaScript', 'MCP', 'Zapier', 'Notion'
        ]
        
        keyword_matches = 0
        for keyword in important_keywords:
            if keyword in content1 and keyword in content2:
                keyword_matches += 1
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãŒã‚ã‚‹å ´åˆã¯é«˜ã‚¹ã‚³ã‚¢ä¿è¨¼
        if keyword_matches > 0:
            keyword_score = min(0.3 + keyword_matches * 0.2, 0.9)
            max_score = max(max_score, keyword_score)
        
        # 1. ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ï¼ˆæœ€é‡è¦ï¼‰
        title_similarity = self._calculate_title_similarity(title1, title2)
        if title_similarity > 0.3:  # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦é–¾å€¤
            max_score = max(max_score, title_similarity * 1.5)  # é‡ã¿ä»˜ã‘
        
        # 2. ã‚¿ã‚°é¡ä¼¼åº¦
        tags1 = self._extract_simple_tags(content1)
        tags2 = self._extract_simple_tags(content2)
        tag_similarity = self._calculate_jaccard_similarity(tags1, tags2)
        if tag_similarity > 0.2:
            max_score = max(max_score, tag_similarity * 1.2)
        
        # 3. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é¡ä¼¼åº¦ï¼ˆæ”¹è‰¯æ¸ˆã¿ï¼‰
        jaccard_similarity = self._calculate_content_jaccard_similarity(content1, content2)
        max_score = max(max_score, jaccard_similarity)
        
        return max_score
    
    def _calculate_title_similarity(self, title1: str, title2: str) -> float:
        """ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦è¨ˆç®—"""
        words1 = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯A-Za-z]{2,}', title1.lower()))
        words2 = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯A-Za-z]{2,}', title2.lower()))
        
        # ä¸€èˆ¬èªé™¤å¤–
        common_words = {'ã«ã¤ã„ã¦', 'ã«é–¢ã—ã¦', 'ã®æ–¹æ³•', 'ã«ã¤ã„ã¦', 'ã¾ã¨ã‚', 'ãƒ¡ãƒ¢'}
        words1 = words1 - common_words
        words2 = words2 - common_words
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        return intersection / union if union > 0 else 0.0
    
    def _extract_simple_tags(self, content: str) -> set:
        """ç°¡æ˜“ã‚¿ã‚°æŠ½å‡º"""
        words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,8}', content)
        word_counts = Counter(words)
        return {word for word, count in word_counts.items() if count >= 2}
    
    def _calculate_jaccard_similarity(self, set1: set, set2: set) -> float:
        """Jaccardä¿‚æ•°è¨ˆç®—"""
        if not set1 or not set2:
            return 0.0
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        return intersection / union if union > 0 else 0.0
    
    def _calculate_content_jaccard_similarity(self, content1: str, content2: str) -> float:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®Jaccardé¡ä¼¼åº¦è¨ˆç®—ï¼ˆè‹±èªå¯¾å¿œå¼·åŒ–ï¼‰"""
        # æ—¥æœ¬èªã®å˜èªï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰
        jp_words1 = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,}', content1.lower()))
        jp_words2 = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,}', content2.lower()))
        
        # è‹±èªã®å˜èªï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰+ é‡è¦å›ºæœ‰åè©
        en_words1 = set(re.findall(r'[A-Za-z]{2,}', content1))
        en_words2 = set(re.findall(r'[A-Za-z]{2,}', content2))
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç›´æ¥ãƒãƒƒãƒãƒ³ã‚°ï¼ˆå¤§å¹…åŠ ç‚¹ï¼‰
        important_keywords = {
            'ChatGPT', 'chatgpt', 'ãƒãƒ£ãƒƒãƒˆGPT', 'API', 'GitHub', 'Obsidian', 'AI', 'Claude',
            'Project', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', 'Consulting', 'ã‚³ãƒ³ã‚µãƒ«', 'Client', 'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ',
            'Python', 'JavaScript', 'Tech', 'ãƒ“ã‚¸ãƒã‚¹', 'ã‚¢ã‚¤ãƒ‡ã‚¢'
        }
        
        keyword_matches = 0
        for keyword in important_keywords:
            if keyword in content1 and keyword in content2:
                keyword_matches += 1
        
        # ä¸€èˆ¬çš„ã™ãã‚‹èªã‚’é™¤å¤–
        common_words = {'ã«ã¤ã„ã¦', 'ã«é–¢ã—ã¦', 'ãŒã§ãã‚‹', 'ã§ã‚ã‚‹', 'ã¦ã„ã‚‹', 'ã¾ã—ãŸ', 'ã—ã¾ã™', 'ã•ã‚ŒãŸ', 'the', 'and', 'of', 'to', 'in', 'is', 'it'}
        jp_words1 = jp_words1 - common_words
        jp_words2 = jp_words2 - common_words
        en_words1 = en_words1 - common_words
        en_words2 = en_words2 - common_words
        
        # å…¨å˜èªã‚»ãƒƒãƒˆã®çµ„ã¿åˆã‚ã›
        all_words1 = jp_words1 | en_words1
        all_words2 = jp_words2 | en_words2
        
        jaccard_sim = self._calculate_jaccard_similarity(all_words1, all_words2)
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã«å¤§å¹…ãƒœãƒ¼ãƒŠã‚¹
        if keyword_matches > 0:
            bonus = min(keyword_matches * 0.3, 0.8)  # æœ€å¤§0.8ã®ãƒœãƒ¼ãƒŠã‚¹
            jaccard_sim = min(1.0, jaccard_sim + bonus)
        
        return jaccard_sim
    
    def _is_sns_analysis_file(self, title: str) -> bool:
        """SNSåˆ†æãƒ•ã‚¡ã‚¤ãƒ«åˆ¤å®š"""
        sns_keywords = ['XæŠ•ç¨¿', 'SNS', 'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†æ', 'ãƒã‚¹ãƒˆåˆ†æ', 'ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼', 'ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼']
        return any(keyword in title for keyword in sns_keywords)
    
    def _is_tech_file(self, title: str) -> bool:
        """æŠ€è¡“ãƒ•ã‚¡ã‚¤ãƒ«åˆ¤å®š"""
        tech_keywords = ['API', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚·ã‚¹ãƒ†ãƒ ', 'GitHub', 'Python', 'AI', 'Claude', 'ã‚³ãƒ¼ãƒ‰', 'ChatGPT', 'ãƒãƒ£ãƒƒãƒˆGPT', 'MCP', 'Zapier', 'Obsidian', 'Tech', 'æŠ€è¡“', 'é–‹ç™º']
        return any(keyword in title for keyword in tech_keywords)
    
    def _get_relation_threshold(self, title1: str, title2: str) -> float:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥é–¢é€£é–¾å€¤å–å¾—"""
        if self._is_sns_analysis_file(title1) and self._is_sns_analysis_file(title2):
            return 0.08  # SNSåˆ†æåŒå£«ï¼šç·©å’Œ
        elif self._is_tech_file(title1) and self._is_tech_file(title2):
            return 0.06  # Techç³»åŒå£«ï¼šç·©å’Œ
        else:
            return 0.05  # ä¸€èˆ¬ï¼šå¤§å¹…ç·©å’Œï¼ˆå®Ÿç”¨çš„ãªãƒ¬ãƒ™ãƒ«ã«ï¼‰
    
    def _calculate_star_rating(self, score: float) -> str:
        """ã‚¹ã‚³ã‚¢ã‹ã‚‰æ˜Ÿè©•ä¾¡ã‚’è¨ˆç®—"""
        if score >= 0.7:
            return "â˜…â˜…â˜…â˜…â˜…"
        elif score >= 0.5:
            return "â˜…â˜…â˜…â˜…"
        elif score >= 0.3:
            return "â˜…â˜…â˜…"
        elif score >= 0.2:
            return "â˜…â˜…"
        else:
            return "â˜…"
    
    def _determine_relation_type_enhanced(self, content1: str, content2: str) -> str:
        """é–¢é€£ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        
        # ç°¡æ˜“çš„ãªé–¢é€£ã‚¿ã‚¤ãƒ—åˆ¤å®š
        if any(word in content1 and word in content2 for word in ['æ•™è‚²', 'æŒ‡å°', 'æˆæ¥­']):
            return 'educational'
        elif any(word in content1 and word in content2 for word in ['æŠ€è¡“', 'API', 'ã‚·ã‚¹ãƒ†ãƒ ']):
            return 'technical'
        elif any(word in content1 and word in content2 for word in ['ãƒ“ã‚¸ãƒã‚¹', 'æˆ¦ç•¥', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°']):
            return 'business'
        else:
            return 'general'
    
    def _build_preview_info(self, category_result: dict, title_result: dict, 
                           tags_result: dict, relations_result: dict) -> dict:
        """ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºç”¨æƒ…å ±æ§‹ç¯‰"""
        
        # å®Ÿéš›ã®ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—
        actual_folder = self.category_folders.get(category_result['name'], 'Others')
        
        return {
            'category_display': f"{category_result['name']} (ä¿¡é ¼åº¦: {category_result['confidence']:.1%})",
            'title_display': title_result['title'],
            'tags_display': ' '.join(tags_result['tags'][:5]),  # æœ€åˆã®5å€‹
            'relations_display': f"{relations_result['count']}ä»¶ã®é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«",
            'folder_path': actual_folder,  # å®Ÿéš›ã®ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’è¿½åŠ 
            'save_path_display': f"{actual_folder}/{title_result['title']} {datetime.now().strftime('%Y-%m-%d')}.md",
            'full_analysis': {
                'category_scores': category_result.get('scores', {}),
                'title_alternatives': title_result.get('alternatives', []),
                'tag_sources': tags_result.get('sources', {}),
                'relation_details': relations_result.get('relations', [])
            }
        }
    
    # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _is_meaningless_phrase(self, phrase: str) -> bool:
        """ç„¡æ„å‘³ãªãƒ•ãƒ¬ãƒ¼ã‚ºã‹ãƒã‚§ãƒƒã‚¯"""
        meaningless = ['ã“ã®ã‚·ãƒ¼ãƒ³', 'ãã†ã§ã™ã­', 'ã‚ã‹ã‚‹', 'ã§ã™ã­', 'ã¾ã™', 'ã§ã™']
        return any(m in phrase for m in meaningless)
    
    def _clean_title_text(self, text: str) -> str:
        """ã‚¿ã‚¤ãƒˆãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        text = re.sub(r'[ã¯ãŒã‚’ã«ã§ã¨]$', '', text)
        text = re.sub(r'ã«ã¤ã„ã¦$|ã«é–¢ã—ã¦$|ã§ã¯$', '', text)
        # å¼•ç”¨ç¬¦ã®å‡¦ç†ï¼ˆé€”ä¸­ã§åˆ‡ã‚ŒãŸã‚‚ã®ã‚‚å«ã‚€ï¼‰
        text = re.sub(r'^[ã€Œã€]', '', text)
        text = re.sub(r'[ã€ã€]$', '', text)
        text = re.sub(r'ã€ã¨ã€Œ', 'ã¨', text)  # ã€ŒAã€ã¨ã€ŒBã€â†’Aã¨B
        text = re.sub(r'ã€ã¨ã€', 'ã¨', text)  # ã€Aã€ã¨ã€Bã€â†’Aã¨B
        # é©åˆ‡ãªé•·ã•ã«èª¿æ•´ï¼ˆ35æ–‡å­—ã¾ã§æ‹¡å¼µï¼‰
        cleaned = text.strip()
        if len(cleaned) > 35:
            # å˜èªã®é€”ä¸­ã§åˆ‡ã‚‰ãªã„ã‚ˆã†ã«èª¿æ•´
            cut_point = 32
            while cut_point > 20 and cleaned[cut_point] not in ['ã®', 'ã‚’', 'ã«', 'ã¨', 'ã§', ' ', 'ã€']:
                cut_point -= 1
            cleaned = cleaned[:cut_point] + "..."
        return cleaned
    
    def _is_common_word(self, word: str) -> bool:
        """ä¸€èˆ¬çš„ãªèªã‹ãƒã‚§ãƒƒã‚¯"""
        common = {'ã‚ã‚‹', 'ã„ã‚‹', 'ã™ã‚‹', 'ãªã‚‹', 'ã§ã™', 'ã¾ã™', 'ã“ã®', 'ãã®', 'ã‚ã®', 'ãã‚Œ', 'ã“ã‚Œ', 'ã‚ã‚Œ'}
        return word in common
    
    def _get_category_base_tags(self, category: str) -> list:
        """ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã‚¿ã‚°"""
        base_tags = {
            'education': ['#æ•™è‚²', '#å­¦ç¿’', '#æŒ‡å°'],
            'tech': ['#Tech', '#æŠ€è¡“', '#é–‹ç™º'],
            'business': ['#ãƒ“ã‚¸ãƒã‚¹', '#æˆ¦ç•¥', '#ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°'],
            'ideas': ['#ã‚¢ã‚¤ãƒ‡ã‚¢', '#ä¼ç”»', '#å‰µä½œ'],
            'general': ['#ãƒ¡ãƒ¢', '#è¨˜éŒ²']
        }
        return base_tags.get(category, ['#ãƒ¡ãƒ¢'])
    
    def _detect_content_type(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—æ¤œå‡º"""
        if re.search(r'https?://', content):
            return 'web_reference'
        elif re.search(r'ã‚¢ã‚¤ãƒ‡ã‚¢|ä¼ç”»|ææ¡ˆ', content):
            return 'idea'
        elif re.search(r'æˆæ¥­|æŒ‡å°|å­¦ç¿’', content):
            return 'learning'
        elif re.search(r'TODO|ã‚¿ã‚¹ã‚¯|ã‚„ã‚‹ã“ã¨', content):
            return 'todo'
        else:
            return 'general'
    
    def _get_content_type_tags(self, content_type: str) -> list:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—åˆ¥ã‚¿ã‚°"""
        type_tags = {
            'web_reference': ['#å‚è€ƒè³‡æ–™', '#Webè¨˜äº‹'],
            'idea': ['#ç™ºæƒ³', '#æ€è€ƒ'],
            'learning': ['#å­¦ç¿’è¨˜éŒ²', '#æˆæ¥­ãƒãƒ¼ãƒˆ'],
            'todo': ['#TODO', '#ã‚¿ã‚¹ã‚¯'],
            'general': []
        }
        return type_tags.get(content_type, [])
    
    def _extract_frequent_word_tags(self, content: str) -> list:
        """é »å‡ºèªã‚¿ã‚°æŠ½å‡º"""
        words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8}', content)
        word_freq = Counter(words)
        
        frequent_tags = []
        for word, freq in word_freq.items():
            if freq >= 2 and len(word) > 2 and not self._is_common_word(word):
                frequent_tags.append(f"#{word}")
        
        return frequent_tags[:3]
    
    def _extract_emotion_tags(self, content: str) -> list:
        """æ„Ÿæƒ…ã‚¿ã‚°æŠ½å‡º"""
        emotion_patterns = {
            '#ãƒã‚¸ãƒ†ã‚£ãƒ–': r'ç´ æ™´ã‚‰ã—ã„|ã„ã„|è‰¯ã„|æ¥½ã—|å¬‰ã—',
            '#ãƒã‚¬ãƒ†ã‚£ãƒ–': r'å›°ã£ãŸ|æ‚ªã„|æ®‹å¿µ|æƒœã—ã„',
            '#ç–‘å•': r'ãªãœ|ã©ã†ã—ã¦|ã‚ã‹ã‚‰ãªã„|\?|ï¼Ÿ',
            '#é‡è¦': r'é‡è¦|å¤§åˆ‡|ãƒã‚¤ãƒ³ãƒˆ|æ ¸å¿ƒ'
        }
        
        tags = []
        for tag, pattern in emotion_patterns.items():
            if re.search(pattern, content):
                tags.append(tag)
        return tags
    
    def _extract_action_tags(self, content: str) -> list:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚°æŠ½å‡º"""
        action_patterns = {
            '#å­¦ç¿’': r'å­¦ã¶|è¦šãˆã‚‹|ç†è§£|ç¿’å¾—',
            '#åˆ†æ': r'åˆ†æ|æ¤œè¨|èª¿æŸ»|è€ƒå¯Ÿ',
            '#è¨˜éŒ²': r'è¨˜éŒ²|ä¿å­˜|ãƒ¡ãƒ¢|ãƒãƒ¼ãƒˆ'
        }
        
        tags = []
        for tag, pattern in action_patterns.items():
            if re.search(pattern, content):
                tags.append(tag)
        return tags
    
    def _generate_content_summary(self, content: str) -> dict:
        """ãƒ¡ãƒ¢å†…å®¹ã®è¦ç´„ã¨ç®‡æ¡æ›¸ãã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼š3-6å€‹ã®è©³ç´°ãªç®‡æ¡æ›¸ãï¼‰"""
        
        # æ–‡å…¨ä½“ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®å‰å‡¦ç†
        clean_content = re.sub(r'(ã®ã§|ã®ã‚ˆã†ã«|ã¨ã„ã†ã“ã¨ã§|ãªã®ã‹ãªã¨æ€ã£ã¦ã„ã‚‹ã¨ã“ã‚ã§ã™)', '', content)
        sentences = re.split(r'[ã€‚ï¼ï¼ï¼Ÿ\n]', content)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
        
        # é‡è¦ãªå›ºæœ‰åè©ã¨æ¦‚å¿µã®æŠ½å‡º
        important_terms = self._extract_important_terms(content)
        
        # === è¦ç´„ç”Ÿæˆï¼ˆè«–ç†çš„æ§‹é€ ã§æ•´ç†ï¼‰===
        
        # 1. ä¸»è¦ç›®çš„ã®ç‰¹å®š
        purpose = ""
        purpose_patterns = [
            r'(ChatGPT[ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ©Ÿèƒ½]*|Project[æ©Ÿèƒ½]*|ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ[æ©Ÿèƒ½]*)[ã‚’ã«](.{5,20})(ã—ãŸã„|ã™ã‚‹|æ´»ç”¨|åˆ©ç”¨)',
            r'(.{10,25})[ã‚’ã«](.{5,15})(ã«å‘ãåˆ|æ´»ç”¨|åˆ©ç”¨)(ã—ã¦ã„ã“ã†|ã—ãŸã„)'
        ]
        
        for pattern in purpose_patterns:
            match = re.search(pattern, content)
            if match:
                groups = match.groups()
                if len(groups) >= 3:
                    tool = groups[0]
                    action = groups[2] if len(groups) > 2 else groups[1]
                    purpose = f"{tool}ã‚’{action}"
                    break
        
        if not purpose:
            # ãƒ¡ãƒ¢ã®æœ€åˆã®éƒ¨åˆ†ã‹ã‚‰ä¸»è¦ãªç›®çš„ã‚’æŠ½å‡º
            if sentences:
                first_sentence = sentences[0]
                # çµ±ä¸€ã•ã‚ŒãŸäººåæŠ½å‡ºã‚’ä½¿ç”¨
                person_names_summary = self._extract_person_names(first_sentence)
                if person_names_summary:
                    person_name = person_names_summary[0]
                    if 'AI' in content or 'DX' in content:
                        purpose = f"{person_name}ã•ã‚“ã¸ã®AIãƒ»DXå°å…¥æ”¯æ´"
                    elif 'æ•™è‚²' in content or 'å­¦ç¿’' in content or 'å›½èª' in content or 'ç®—æ•°' in content:
                        purpose = f"{person_name}ã•ã‚“ã¨ã®æ•™è‚²äº‹æ¥­é€£æº"
                    elif 'ã‚³ãƒ³ã‚µãƒ«' in content or 'ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°' in content:
                        purpose = f"{person_name}ã•ã‚“ã¸ã®ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ææ¡ˆ"
                    else:
                        purpose = f"{person_name}ã•ã‚“ã¨ã®å”æ¥­æ¤œè¨"
                else:
                    # äººåãŒãªã„å ´åˆã¯å†…å®¹ã‹ã‚‰ä¸»è¦ãƒ†ãƒ¼ãƒã‚’æŠ½å‡º
                    if 'AI' in content and 'å°å…¥' in content:
                        purpose = "AIå°å…¥æ”¯æ´ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ"
                    elif 'ChatGPT' in content or 'Project' in content:
                        purpose = "ChatGPTãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ©Ÿèƒ½ã®æ´»ç”¨"
                    else:
                        purpose = "æ¥­å‹™æ”¹å–„ãƒ»åŠ¹ç‡åŒ–ã®æ¤œè¨"
            else:
                purpose = "æ¥­å‹™æ”¹å–„ãƒ»åŠ¹ç‡åŒ–ã®æ¤œè¨"
        
        # 2. å…·ä½“çš„æ‰‹æ®µã®æŠ½å‡ºï¼ˆæ–‡è„ˆã‚’ä¿æŒã—ã¦è‡ªç„¶ãªè¡¨ç¾ã«ï¼‰
        methods = []
        
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé–¢é€£ã®æ‰‹æ®µ
        client_patterns = [
            r'(ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ|Client)ã”ã¨ã«(.{5,30}?)(?:ã‚’ç«‹ã¡ä¸Šã’|ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ)',
            r'(ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ|Client)(?:ã”ã¨|åˆ¥)ã«(.{5,25}?)(?:ã™ã‚‹|ç®¡ç†|é‹ç”¨)'
        ]
        
        for pattern in client_patterns:
            match = re.search(pattern, content)
            if match:
                action = match.group(2).strip()
                # ä¸å®Œå…¨ãªæ–‡å­—åˆ—ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                action = re.sub(r'[ã€ã€‚].*$', '', action)  # å¥èª­ç‚¹ä»¥é™ã‚’å‰Šé™¤
                if len(action) > 3 and 'ã‚’' not in action[-2:]:  # åŠ©è©ã§çµ‚ã‚ã‚‰ãªã„ã‚ˆã†ã«
                    methods.append(f"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ¥{action}ç®¡ç†")
                break
        
        # è“„ç©ãƒ»ç®¡ç†é–¢é€£ã®æ‰‹æ®µï¼ˆã‚ˆã‚Šå…·ä½“çš„ã«ï¼‰
        if 'è³‡æ–™' in content and 'è“„ç©' in content:
            methods.append("è³‡æ–™ã®ä¸€å…ƒç®¡ç†")
        if 'è­°äº‹éŒ²' in content and 'è“„ç©' in content:
            methods.append("è­°äº‹éŒ²ã®è“„ç©")
        
        # ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹æ®µ
        if 'ãƒãƒ£ãƒƒãƒˆ' in content and ('ã‚„ã‚Šã¨ã‚Š' in content or 'èª²é¡Œ' in content):
            methods.append("ãƒãƒ£ãƒƒãƒˆã§ã®ã‚„ã‚Šã¨ã‚Š")
        
        # ãƒœã‚¤ã‚¹ãƒ¢ãƒ¼ãƒ‰é–¢é€£
        if 'ãƒœã‚¤ã‚¹ãƒ¢ãƒ¼ãƒ‰' in content or 'Voice Mode' in content:
            methods.append("ãƒœã‚¤ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§ã®ç›¸è«‡")
        
        # 3. æœŸå¾…åŠ¹æœã®æŠ½å‡ºï¼ˆè‡ªç„¶ãªè¡¨ç¾ã§ï¼‰
        effects = []
        
        # èª²é¡Œè§£æ±ºé–¢é€£
        if 'èª²é¡Œ' in content and ('è§£æ±º' in content or 'æŠ½å‡º' in content):
            effects.append("èª²é¡Œè§£æ±ºã®å®Ÿç¾")
        
        # è­°é¡Œæ¤œè¨é–¢é€£
        if 'ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°' in content and 'è­°é¡Œ' in content:
            effects.append("åŠ¹ç‡çš„ãªä¼šè­°é‹å–¶")
        
        # ãã®ä»–ã®åŠ¹æœ
        if 'æ–¹æ³•' in content and 'è¦‹å‡ºã™' in content:
            effects.append("æ–°æ‰‹æ³•ã®ç™ºè¦‹")
        
        # è¦ç´„ã®çµ„ã¿ç«‹ã¦ï¼ˆè‡ªç„¶ãªæ—¥æœ¬èªã«ãªã‚‹ã‚ˆã†èª¿æ•´ï¼‰
        summary_parts = []
        
        # ä¸»ç›®çš„
        summary_parts.append(purpose)
        
        # æ‰‹æ®µï¼ˆæœ€å¤§2å€‹ã€è‡ªç„¶ã«ç¹‹ãŒã‚‹ã‚ˆã†ã«ï¼‰
        if methods:
            if len(methods) == 1:
                summary_parts.append(methods[0])
            else:
                # è¤‡æ•°ã®æ‰‹æ®µã‚’è‡ªç„¶ã«çµ±åˆ
                method_summary = self._create_natural_method_summary(methods[:3])
                summary_parts.append(method_summary)
        
        # åŠ¹æœ
        if effects:
            summary_parts.append(effects[0])
        
        # æ–‡å­—æ•°åˆ¶é™ã¨è‡ªç„¶æ€§ãƒã‚§ãƒƒã‚¯
        summary = " / ".join(summary_parts)
        
        # ä¸å®Œå…¨ãªæ–‡å­—åˆ—ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        summary = self._clean_summary_text(summary)
            
        # === æ®µè½ãƒ»è¦‹å‡ºã—èªãƒ™ãƒ¼ã‚¹ã®å…·ä½“çš„ãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ ===
        bullet_points = []
        
        # 1. è¦‹å‡ºã—æ§‹é€ ã®æŠ½å‡ºï¼ˆå…·ä½“çš„ãªè©±é¡Œãƒ»ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
        headings = self._extract_concrete_headings(content)
        
        # 2. é‡è¦ãªæ®µè½ã®è¦ç´„æŠ½å‡ºï¼ˆ50æ–‡å­—ä»¥ä¸Šã®æ„å‘³ã®ã‚ã‚‹æ®µè½ï¼‰
        key_paragraphs = self._extract_key_paragraph_summaries(content)
        
        # 3. å…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æ–‡è„ˆã®æŠ½å‡º
        concrete_points = self._extract_concrete_contextual_points(content)
        
        # å„ªå…ˆé †ä½: è¦‹å‡ºã— > æ®µè½è¦ç´„ > å…·ä½“çš„ãƒã‚¤ãƒ³ãƒˆ
        bullet_points.extend(headings[:3])  # æœ€å¤§3å€‹ã®è¦‹å‡ºã—
        bullet_points.extend(key_paragraphs[:3])  # æœ€å¤§3å€‹ã®æ®µè½è¦ç´„
        bullet_points.extend(concrete_points[:3])  # æœ€å¤§3å€‹ã®å…·ä½“ãƒã‚¤ãƒ³ãƒˆ
        
        # é‡è¤‡é™¤å»ã¨å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        bullet_points = self._filter_and_deduplicate_points(bullet_points)
        
        # æœ€ä½3å€‹ã‚’ä¿è¨¼ã™ã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if len(bullet_points) < 3:
            fallback_points = [
                "é‡è¦ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è©³ç´°ãªåˆ†æ",
                "ä¸»è¦ãªè­°é¡Œã¨æ¤œè¨äº‹é …ã®æ•´ç†", 
                "å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã®ç­–å®š"
            ]
            for point in fallback_points:
                if len(bullet_points) < 3:
                    bullet_points.append(point)
        
        # æœ€å¤§6å€‹ã«åˆ¶é™
        bullet_points = bullet_points[:6]
        
        return {
            'bullet_points': bullet_points,
            'key_terms': important_terms[:5]
        }
    
    def _clean_summary_text(self, text: str) -> str:
        """è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆä¸å®Œå…¨æ–‡å­—åˆ—ã®ä¿®æ­£å«ã‚€ï¼‰"""
        # ä¸è¦ãªæ¥ç¶šè©ã‚„å†—é•·ãªè¡¨ç¾ã‚’å‰Šé™¤
        text = re.sub(r'^(ã¾ãŸ|ãã—ã¦|ãã‚Œã‹ã‚‰|ã¤ã¾ã‚Š|è¦ã™ã‚‹ã«)', '', text)
        text = re.sub(r'(ã¨ã„ã†|ã¨ã‹|ãªã©|ã¿ãŸã„|ã‚ˆã†ãª)$', '', text)
        text = re.sub(r'ã€‚$', '', text)
        
        # ä¸å®Œå…¨ãªæ–‡å­—åˆ—ã‚’ä¿®æ­£
        # "æä¾›ã—ãŸè³‡æ–™ã‚’P" ã®ã‚ˆã†ãªé€”åˆ‡ã‚ŒãŸéƒ¨åˆ†ã‚’ä¿®æ­£
        text = re.sub(r'[ã‚’ã«][A-Za-z](?:[ã€/]|$)', 'ã®ç®¡ç†', text)  # "ã‚’P" â†’ "ã®ç®¡ç†"
        text = re.sub(r'[ã‚’ã«][ã€]', 'ã¨', text)  # "ã‚’ã€" â†’ "ã¨"
        
        # é€£ç¶šã™ã‚‹å¥èª­ç‚¹ã‚’æ•´ç†
        text = re.sub(r'[ã€]+', 'ã€', text)
        text = re.sub(r'[ã€‚]+', 'ã€‚', text)
        
        # æœ«å°¾ã®ä¸å®Œå…¨ãªåŠ©è©ã‚’å‰Šé™¤
        text = re.sub(r'[ã‚’ã«ãŒã€]$', '', text)
        
        return text.strip()
    
    def _clean_bullet_point(self, text: str) -> str:
        """ç®‡æ¡æ›¸ããƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        # æ‹¬å¼§ã‚„å¼•ç”¨ç¬¦ã‚’å‰Šé™¤
        text = re.sub(r'[ã€Œã€ã€ã€ï¼ˆï¼‰()]', '', text)
        # å†—é•·ãªåŠ©è©ã‚’å‰Šé™¤
        text = re.sub(r'(ã¨ã‹|ãªã©|ã¿ãŸã„ãª|ã‚ˆã†ãª)$', '', text)
        # é€£ç¶šã™ã‚‹åŠ©è©ã‚’æ•´ç†
        text = re.sub(r'(ã‚’|ã«|ã§|ã¨|ã®){2,}', r'\1', text)
        return text.strip()
    
    def _extract_entity_tags(self, content: str) -> list:
        """å›ºæœ‰åè©ã‚¿ã‚°æŠ½å‡º"""
        entities = []
        
        # è‹±èªå›ºæœ‰åè©
        english_entities = re.findall(r'[A-Z][a-z]+|ChatGPT|Python|API|JavaScript|React|Vue|Node|Git|GitHub|Docker|AWS|Azure|GCP', content)
        entities.extend(english_entities)
        
        # ã‚«ã‚¿ã‚«ãƒŠå›ºæœ‰åè©ï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰
        katakana_entities = re.findall(r'[ã‚¢-ãƒ¶ãƒ¼]{3,10}', content)
        # ã‚ˆãã‚ã‚‹ä¸€èˆ¬èªã‚’é™¤å¤–
        common_katakana = {'ã‚¢ã‚¤ãƒ‡ã‚¢', 'ãƒ¡ãƒ¢', 'ãƒ•ã‚¡ã‚¤ãƒ«', 'ãƒ•ã‚©ãƒ«ãƒ€', 'ãƒšãƒ¼ã‚¸', 'ã‚µã‚¤ãƒˆ', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ãƒ‡ãƒ¼ã‚¿', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'ã‚µãƒ¼ãƒ“ã‚¹'}
        katakana_entities = [e for e in katakana_entities if e not in common_katakana]
        entities.extend(katakana_entities)
        
        # é‡è¤‡é™¤å»ã—ã¦æœ€å¤§5å€‹
        unique_entities = list(dict.fromkeys(entities))[:5]
        return [f"#{entity}" for entity in unique_entities]
    
    def _prioritize_tags(self, tags: list, content: str, sources: dict) -> list:
        """ã‚¿ã‚°ã®å„ªå…ˆé †ä½ä»˜ã‘"""
        tag_scores = {}
        
        for tag in tags:
            score = 1.0
            
            # ã‚½ãƒ¼ã‚¹ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
            source = sources.get(tag, 'unknown')
            if source == 'category':
                score += 2.0
            elif source == 'frequent_words':
                score += 1.0
            elif source == 'emotion':
                score += 0.5
            
            # å‡ºç¾é »åº¦ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
            tag_word = tag.replace('#', '')
            frequency = content.lower().count(tag_word.lower())
            score += frequency * 0.3
            
            tag_scores[tag] = score
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        return sorted(tag_scores.keys(), key=lambda x: tag_scores[x], reverse=True)


    def save_memo(self, content: str) -> dict:
        """ãƒ¡ãƒ¢ã‚’å®Ÿéš›ã«ä¿å­˜ï¼ˆç·¨é›†å†…å®¹ä¿æŒå¯¾å¿œï¼‰"""
        try:
            import tempfile
            from pathlib import Path
            import json
            
            # ã¾ãšä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç·¨é›†æ¸ˆã¿åˆ†æçµæœã‚’ç¢ºèª
            temp_edit_file = Path(tempfile.gettempdir()) / "memo_classifier_edited_analysis.json"
            if temp_edit_file.exists():
                try:
                    with open(temp_edit_file, 'r', encoding='utf-8') as f:
                        analysis = json.load(f)
                    print(f"ğŸ’¾ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç·¨é›†æ¸ˆã¿å†…å®¹ã‚’èª­ã¿è¾¼ã¿: {temp_edit_file}")
                    # ä½¿ç”¨å¾Œã¯å‰Šé™¤
                    temp_edit_file.unlink()
                except Exception as e:
                    print(f"âš ï¸ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                    analysis = None
            else:
                analysis = None
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯é€šå¸¸ã®å‡¦ç†
            if not analysis:
                # ç·¨é›†ã•ã‚ŒãŸåˆ†æçµæœãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°æ–°è¦åˆ†æ
                if self._last_edited_analysis:
                    print("ğŸ’¾ ç·¨é›†æ¸ˆã¿å†…å®¹ã§ä¿å­˜å®Ÿè¡Œ...")
                    analysis = self._last_edited_analysis
                    # ä¿å­˜æˆåŠŸå¾Œã«ã‚¯ãƒªã‚¢ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã‚¯ãƒªã‚¢ã—ãªã„
                else:
                    print("ğŸ’¾ æ–°è¦åˆ†æã§ä¿å­˜å®Ÿè¡Œ...")
                    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æã‚’å†å®Ÿè¡Œ
                    analysis = self.preview_analysis(content)
            
            if not analysis['success']:
                return analysis
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            file_path = self._save_memo_file(
                analysis['title']['title'], 
                content, 
                analysis['category']['name'],
                analysis['tags']['tags'],
                analysis['relations']['relations'],
                analysis.get('summary', {})  # è¦ç´„ãƒ»ç®‡æ¡æ›¸ããƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            )
            
            # Obsidian [[]] ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
            if analysis['relations']['relations']:
                self._add_obsidian_links(str(file_path), analysis['relations']['relations'])
            
            # ä¿å­˜æˆåŠŸå¾Œã«ç·¨é›†çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
            if self._last_edited_analysis:
                print("ğŸ—‘ï¸ ç·¨é›†çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢ï¼ˆä¿å­˜å®Œäº†ï¼‰")
                self._last_edited_analysis = None
            
            return {
                'success': True,
                'file_path': str(file_path),
                'title': analysis['title']['title'],
                'category': analysis['category']['name'],
                'tags_count': len(analysis['tags']['tags']),
                'relations_count': len(analysis['relations']['relations'])
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _save_memo_file(self, title: str, content: str, category: str, tags: list, relations: list, summary_data: dict = None) -> Path:
        """ãƒ¡ãƒ¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®šï¼ˆå®Ÿéš›ã®Obsidianãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã«åˆã‚ã›ã¦ä¿®æ­£ï¼‰
        folder_name = self.category_folders.get(category, 'Others')
        save_dir = Path(self.obsidian_path) / self.inbox_path / folder_name
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼ˆæ—¥ä»˜ã®ã¿ã€æ™‚åˆ†ç§’ãªã—ï¼‰
        timestamp = datetime.now().strftime('%Y%m%d')
        safe_title = re.sub(r'[<>:"/\\|?*]', '', title)
        safe_title = re.sub(r'\s+', '_', safe_title)
        filename = f"{timestamp}_{safe_title}.md"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        file_path = save_dir / filename
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹æ§‹ç¯‰
        file_content = self._build_markdown_content(title, content, category, tags, relations, summary_data)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(file_content)
        
        return file_path
    
    def _build_markdown_content(self, title: str, content: str, category: str, tags: list, relations: list, summary_data: dict = None) -> str:
        """Markdownãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’æ§‹ç¯‰"""
        
        lines = []
        
        # YAMLãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼
        lines.append('---')
        lines.append(f'title: "{title}"')
        lines.append(f'category: {category}')
        lines.append(f'tags: {json.dumps(tags, ensure_ascii=False)}')
        lines.append(f'created: {datetime.now().isoformat()}')
        lines.append('---')
        lines.append('')
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        lines.append(f'# {title}')
        lines.append('')
        
        # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå†’é ­ã«ç§»å‹•ï¼‰
        if relations:
            lines.append('## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«')
            lines.append('')
            for relation in relations:
                file_name = relation["file_name"]
                star_rating = relation.get('star_rating', 'â˜…â˜…â˜…')
                relation_type = relation.get("relation_type", "ç›¸äº’ãƒªãƒ³ã‚¯")
                lines.append(f'- [[{file_name}]] {star_rating} ({relation_type})')
            lines.append('')
        
        # ã‚¿ã‚°è¡¨ç¤º
        if tags:
            lines.append(f'**ã‚¿ã‚°**: {" ".join(tags)}')
            lines.append('')
        
        # å…·ä½“çš„ãªãƒã‚¤ãƒ³ãƒˆï¼ˆæ®µè½ãƒ»è¦‹å‡ºã—èªãƒ™ãƒ¼ã‚¹ï¼‰
        if summary_data and summary_data.get('bullet_points'):
            lines.append('## ãƒã‚¤ãƒ³ãƒˆ')
            lines.append('')
            for point in summary_data['bullet_points']:
                lines.append(f'- {point}')
            lines.append('')
        
        # ãƒ¡ã‚¤ãƒ³å†…å®¹
        lines.append('## å†…å®¹')
        lines.append('')
        lines.append(content)
        
        return '\n'.join(lines)
    
    def _add_obsidian_links(self, target_file_path: str, related_files: list):
        """Obsidianãƒ•ã‚¡ã‚¤ãƒ«ã«ç›¸äº’ãƒªãƒ³ã‚¯ã‚’è¿½åŠ """
        try:
            target_path = Path(target_file_path)
            
            if not target_path.exists():
                return
            
            # æ—¢å­˜ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿
            with open(target_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ /æ›´æ–°
            updated_content = self._add_new_links_section(content, related_files)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãæˆ»ã—
            with open(target_path, 'w', encoding='utf-8') as f:
                f.write(updated_content)
            
            print(f"ğŸ“ {target_path.name}ã«é–¢é€£ãƒªãƒ³ã‚¯ã‚’è¿½åŠ ")
            
            # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«å´ã«ã‚‚é€†ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
            self._add_reverse_links(target_path, related_files)
            
        except Exception as e:
            print(f"âš ï¸ ãƒªãƒ³ã‚¯è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _add_new_links_section(self, content: str, related_files: list) -> str:
        """æ–°ã—ã„é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ """
        
        # æ—¢å­˜ã®é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®‰å…¨ã«å‰Šé™¤
        # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®é–‹å§‹ã‹ã‚‰æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«çµ‚ç«¯ã¾ã§
        content = re.sub(r'\n## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«\n.*?(?=\n## |\n---\n|$)', '', content, flags=re.DOTALL)
        # æœ«å°¾ã«ã‚ã‚‹é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚‚å‰Šé™¤
        content = re.sub(r'\n## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«\n.*$', '', content, flags=re.DOTALL)
        
        if not related_files:
            return content
        
        # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å†’é ­ã«é…ç½®ã™ã‚‹ãŸã‚ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ†æ
        lines = content.split('\n')
        
        # ã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆ# ã§å§‹ã¾ã‚‹è¡Œï¼‰
        title_line_index = -1
        for i, line in enumerate(lines):
            if line.strip().startswith('# ') and not line.strip().startswith('##'):
                title_line_index = i
                break
        
        # æ–°ã—ã„é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰
        links_section_lines = ["", "## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«", ""]
        
        for rel_file in related_files:
            file_name = rel_file['file_name']
            star_rating = rel_file.get('star_rating', 'â˜…â˜…â˜…')
            
            # é–¢é€£ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
            relation_type = rel_file.get('relation_type', 'general')
            if relation_type == 'educational':
                comment = '(æ•™è‚²é–¢é€£)'
            elif relation_type == 'technical':
                comment = '(æŠ€è¡“é–¢é€£)'
            elif relation_type == 'business':
                comment = '(ãƒ“ã‚¸ãƒã‚¹é–¢é€£)'
            elif relation_type == 'media':
                comment = '(ãƒ¡ãƒ‡ã‚£ã‚¢é–¢é€£)'
            else:
                comment = '(ç›¸äº’ãƒªãƒ³ã‚¯)'
            
            links_section_lines.append(f"- [[{file_name}]] {star_rating} {comment}")
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®ç›´å¾Œã«é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŒ¿å…¥
        if title_line_index >= 0:
            # ã‚¿ã‚¤ãƒˆãƒ«è¡Œã®å¾Œã«é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŒ¿å…¥
            new_lines = lines[:title_line_index + 1] + links_section_lines + lines[title_line_index + 1:]
            return '\n'.join(new_lines)
        else:
            # ã‚¿ã‚¤ãƒˆãƒ«è¡ŒãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ«å°¾ã«è¿½åŠ ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            return content + '\n\n' + '\n'.join(links_section_lines)
    
    def _add_reverse_links(self, source_file: Path, related_files: list):
        """é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«å´ã«é€†ãƒªãƒ³ã‚¯ã‚’è¿½åŠ """
        source_name = source_file.stem
        
        for rel_file in related_files:
            try:
                rel_file_path = Path(rel_file['file_path'])
                
                if not rel_file_path.exists():
                    continue
                
                # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                with open(rel_file_path, 'r', encoding='utf-8') as f:
                    rel_content = f.read()
                
                # æ—¢ã«ç›¸äº’ãƒªãƒ³ã‚¯ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if f"[[{source_name}]]" in rel_content:
                    continue
                
                # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if "## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«" in rel_content:
                    # æ—¢å­˜ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
                    star_rating = rel_file.get('star_rating', 'â˜…â˜…â˜…')
                    new_link = f"- [[{source_name}]] {star_rating} (ç›¸äº’ãƒªãƒ³ã‚¯)"
                    
                    # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æœ€å¾Œã«è¿½åŠ 
                    rel_content = re.sub(
                        r'(## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«\n\n(?:[^\n]*\n)*)',
                        r'\1' + new_link + '\n',
                        rel_content
                    )
                else:
                    # æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚¿ã‚¤ãƒˆãƒ«ç›´å¾Œã«ä½œæˆ
                    star_rating = rel_file.get('star_rating', 'â˜…â˜…â˜…')
                    rel_lines = rel_content.split('\n')
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚’è¦‹ã¤ã‘ã‚‹
                    title_line_index = -1
                    for i, line in enumerate(rel_lines):
                        if line.strip().startswith('# ') and not line.strip().startswith('##'):
                            title_line_index = i
                            break
                    
                    if title_line_index >= 0:
                        # ã‚¿ã‚¤ãƒˆãƒ«ç›´å¾Œã«é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŒ¿å…¥
                        links_section = ["", "## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«", "", f"- [[{source_name}]] {star_rating} (ç›¸äº’ãƒªãƒ³ã‚¯)"]
                        new_lines = rel_lines[:title_line_index + 1] + links_section + rel_lines[title_line_index + 1:]
                        rel_content = '\n'.join(new_lines)
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ«å°¾ã«è¿½åŠ 
                        new_section = f"\n\n## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«\n\n- [[{source_name}]] {star_rating} (ç›¸äº’ãƒªãƒ³ã‚¯)\n"
                        rel_content += new_section
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãæˆ»ã—
                with open(rel_file_path, 'w', encoding='utf-8') as f:
                    f.write(rel_content)
                
                print(f"ğŸ“ {rel_file_path.name}ã«é€†ãƒªãƒ³ã‚¯ã‚’è¿½åŠ ")
                
            except Exception as e:
                print(f"âš ï¸ é€†ãƒªãƒ³ã‚¯è¿½åŠ ã‚¨ãƒ©ãƒ¼ ({rel_file['file_name']}): {e}")
                continue


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹æ³•: python3 preview_enhanced_memo.py [preview|save] <content>")
        return
    
    command = sys.argv[1]
    memo_content = " ".join(sys.argv[2:])
    
    if not memo_content.strip():
        print("âŒ ãƒ¡ãƒ¢å†…å®¹ãŒç©ºã§ã™")
        return
    
    processor = IntegratedMemoProcessor()
    
    if command == "preview":
        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æå®Ÿè¡Œ
        result = processor.preview_analysis(memo_content)
        
        # AppleScriptç”¨ã®å˜ç´”ãªå½¢å¼ã§å‡ºåŠ›
        print("RESULT_START")
        print(f"TITLE:{result['title']['title']}")
        print(f"CATEGORY:{result['category']['name']}")
        
        # å®Ÿéš›ã®ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’è¿½åŠ 
        actual_folder = processor.category_folders.get(result['category']['name'], 'Others')
        print(f"FOLDER:{actual_folder}")
        
        # ã‚¿ã‚°ã‚’å˜ç´”ãªå½¢å¼ã§å‡ºåŠ›
        tags_list = result['tags']['tags']
        tags_str = ",".join(tags_list) if tags_list else "ãªã—"
        print(f"TAGS:{tags_str}")
        
        # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å˜ç´”ãªå½¢å¼ã§å‡ºåŠ›
        relations_list = result['relations']['relations']
        if relations_list:
            relation_names = [rel['file_name'] for rel in relations_list[:3]]
            relations_str = f"{len(relations_list)}ä»¶:" + ",".join(relation_names)
        else:
            relations_str = "ãªã—"
        print(f"RELATIONS:{relations_str}")
        
        # è¦ç´„æƒ…å ±ã‚’å‡ºåŠ›
        summary_info = result.get('summary', {})
        summary_text = summary_info.get('summary', '')
        bullet_points = summary_info.get('bullet_points', [])
        
        print(f"SUMMARY:{summary_text}")
        if bullet_points:
            print(f"BULLET_POINTS:{' | '.join(bullet_points)}")
        else:
            print("BULLET_POINTS:ãªã—")
            
        print("RESULT_END")
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨ã«JSONå½¢å¼ã‚‚å‡ºåŠ›
        print("JSON_START")
        print(json.dumps(result, ensure_ascii=False, indent=2))
        print("JSON_END")
        
    elif command == "save":
        # å®Ÿéš›ã®ä¿å­˜å‡¦ç†
        result = processor.save_memo(memo_content)
        
        if result['success']:
            print(f"âœ… ãƒ¡ãƒ¢ä¿å­˜å®Œäº†!")
            print(f"ğŸ“‹ ã‚¿ã‚¤ãƒˆãƒ«: {result['title']}")
            print(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒª: {result['category']}")
            print(f"ğŸ·ï¸ ã‚¿ã‚°æ•°: {result['tags_count']}")
            print(f"ğŸ”— é–¢é€£æ•°: {result['relations_count']}")
            print(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«: {Path(result['file_path']).name}")
        else:
            print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {result['error']}")
    
    else:
        print("âŒ ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰ã€‚'preview' ã¾ãŸã¯ 'save' ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()