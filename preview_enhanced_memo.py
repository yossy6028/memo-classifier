#!/usr/bin/env python3
"""
çµ±åˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½ä»˜ãå¼·åŒ–ãƒ¡ãƒ¢å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
AppleScriptç¢ºèªç”»é¢ç”¨ã®å®Œå…¨ãªäº‹å‰åˆ†æ
"""

import sys
import os
import re
import json
from pathlib import Path
from datetime import datetime
from collections import Counter, defaultdict


class IntegratedMemoProcessor:
    """çµ±åˆãƒ¡ãƒ¢ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾å¿œï¼‰"""
    
    def __init__(self):
        # åŸºæœ¬è¨­å®š
        self.obsidian_path = "/Users/yoshiikatsuhiko/Library/Mobile Documents/iCloud~md~obsidian/Documents"
        self.inbox_path = "02_Inbox"
        
        # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°
        self.category_folders = {
            'education': '0_Education_å›½èªæ•™è‚²_AI',
            'tech': '1_Tech_MCP_API', 
            'business': '2_Business_é›†å®¢_ã‚¢ã‚¤ãƒ‡ã‚¢',
            'ideas': '3_Ideas_ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ',
            'general': '4_General',
            'kindle': '5_Kindle',
            'readwise': '6_Readwise'
        }
        
        # å¼·åŒ–ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªåˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.category_keywords = {
            'education': [
                # åŸºæœ¬æ•™è‚²ç”¨èª
                'æ•™è‚²', 'æŒ‡å°', 'æˆæ¥­', 'å­¦ç¿’', 'å›½èª', 'èª­è§£', 'è¡¨ç¾', 'ç”Ÿå¾’', 'å…ˆç”Ÿ', 'æ•™å¸«',
                # æ–‡å­¦ãƒ»å›½èªæŠ€æ³•
                'å¯¾å¥æ³•', 'ãƒªã‚ºãƒ ', 'éŸ³æ•°', 'éŸ»å¾‹', 'ä¿®è¾', 'æŠ€æ³•', 'æ–‡ä½“', 'è¡¨ç¾æ³•',
                # æˆæ¥­å ´é¢ã®èªå½™
                'ã‚·ãƒ¼ãƒ³', 'è€ƒãˆã¦', 'é¸ã³ãªã•ã„', 'æ€ã„å‡ºã™', 'æ€ã„æµ®ã‹ã¹', 'ã‚ã‹ã‚‹', 'ã²ã£ã‹ã‘',
                'å•é¡Œ', 'ç­”ãˆ', 'æ­£è§£', 'ä¸æ­£è§£', 'ä¾‹ãˆã°', 'ä»®ã«', 'å ´é¢', 'çŠ¶æ³', 'çŠ¶æ…‹',
                # è©•ä¾¡ãƒ»æŒ‡å°èª
                'ãã†ã§ã™ã­', 'ç´ æ™´ã‚‰ã—ã„', 'æ®‹å¿µ', 'æƒœã—ã„', 'ãã‚“', 'ã•ã‚“', 'ã¡ã‚ƒã‚“',
                # äº”æ„Ÿãƒ»æ„Ÿè¦š
                'è´è¦š', 'è¦–è¦š', 'è§¦è¦š', 'å—…è¦š', 'å‘³è¦š', 'äº”æ„Ÿ', 'æ„Ÿè¦š', 'ä½“ã®éƒ¨åˆ†', 'ãƒ¡ãƒ­ãƒ‡ã‚£ãƒ¼'
            ],
            'tech': [
                'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'API', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚¢ãƒ—ãƒª', 'python', 'javascript', 
                'tech', 'æŠ€è¡“', 'é–‹ç™º', 'ã‚³ãƒ¼ãƒ‰', 'ãƒ‡ãƒ¼ã‚¿', 'AI', 'æ©Ÿæ¢°å­¦ç¿’'
            ],
            'business': [
                'ãƒ“ã‚¸ãƒã‚¹', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'æˆ¦ç•¥', 'å–¶æ¥­', 'é›†å®¢', 'SEO', 'SNS', 
                'åºƒå‘Š', 'å£²ä¸Š', 'åç›Š', 'é¡§å®¢', 'å¸‚å ´'
            ],
            'ideas': [
                'ã‚¢ã‚¤ãƒ‡ã‚¢', 'ä¼ç”»', 'ææ¡ˆ', 'æ¡ˆ', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', 'å‰µä½œ', 'ç™ºæƒ³', 
                'ãƒ–ãƒ¬ã‚¹ãƒˆ', 'ã‚³ãƒ³ã‚»ãƒ—ãƒˆ', 'ãƒ—ãƒ©ãƒ³'
            ]
        }
    
    def preview_analysis(self, content: str) -> dict:
        """ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã®å®Œå…¨åˆ†æ"""
        try:
            print("ğŸ”„ çµ±åˆåˆ†æé–‹å§‹...")
            
            # 1. ã‚«ãƒ†ã‚´ãƒªåˆ†æï¼ˆå¼·åŒ–ç‰ˆï¼‰
            category_result = self._enhanced_category_analysis(content)
            print(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ†æ: {category_result}")
            
            # 2. ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆä¸»é¡ŒæŠŠæ¡ï¼‰
            title_result = self._intelligent_title_generation(content, category_result)
            print(f"ğŸ“‹ ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ: {title_result}")
            
            # 3. ã‚¿ã‚°ç”Ÿæˆï¼ˆå¤šå±¤åˆ†æï¼‰
            tags_result = self._comprehensive_tag_generation(content, category_result)
            print(f"ğŸ·ï¸ ã‚¿ã‚°ç”Ÿæˆ: {tags_result}")
            
            # 4. é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
            relations_result = self._find_related_files(content, title_result['title'])
            print(f"ğŸ”— é–¢é€£åˆ†æ: {relations_result}")
            
            # 5. çµ±åˆçµæœæ§‹ç¯‰
            result = {
                'success': True,
                'category': category_result,
                'title': title_result,
                'tags': tags_result,
                'relations': relations_result,
                'preview_info': self._build_preview_info(category_result, title_result, tags_result, relations_result),
                'timestamp': datetime.now().isoformat()
            }
            
            print("âœ… çµ±åˆåˆ†æå®Œäº†!")
            return result
            
        except Exception as e:
            print(f"âŒ çµ±åˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e),
                'category': {'name': 'general', 'confidence': 0.0},
                'title': {'title': 'ã‚¨ãƒ©ãƒ¼', 'method': 'error'},
                'tags': {'tags': ['#ã‚¨ãƒ©ãƒ¼'], 'count': 1},
                'relations': {'relations': [], 'count': 0}
            }
    
    def _enhanced_category_analysis(self, content: str) -> dict:
        """å¼·åŒ–ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªåˆ†æ"""
        
        content_lower = content.lower()
        
        # æ•™è‚²ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆç‰¹åˆ¥å¼·åŒ–ï¼‰
        education_patterns = [
            r'[ã-ã‚“ãƒ¼]+ãã‚“|[ã-ã‚“ãƒ¼]+ã•ã‚“|[ã-ã‚“ãƒ¼]+ã¡ã‚ƒã‚“',  # ç”Ÿå¾’å
            r'ã‚ã‹ã‚‹ï¼Ÿ|ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ|ç†è§£ã§ããŸï¼Ÿ',  # æ•™å¸«ã®ç¢ºèª
            r'ãã†ã§ã™ã­|ç´ æ™´ã‚‰ã—ã„|æ®‹å¿µ|æƒœã—ã„|æ­£è§£|ä¸æ­£è§£',  # è©•ä¾¡
            r'è€ƒãˆã¦|æ€ã„å‡ºã™|æ€ã„æµ®ã‹ã¹|é¸ã³ãªã•ã„|ç­”ãˆãªã•ã„',  # æŒ‡ç¤º
            r'ä¾‹ãˆã°|ä»®ã«|å ´åˆ|ã‚·ãƒ¼ãƒ³|çŠ¶æ³|å ´é¢',  # è¨­å®š
            r'ã²ã£ã‹ã‘|å•é¡Œ|ãƒ†ã‚¹ãƒˆ|æˆæ¥­|æŒ‡å°',  # æ•™è‚²æ–‡è„ˆ
            r'è´è¦š|è¦–è¦š|äº”æ„Ÿ|ä½“ã®éƒ¨åˆ†|æ„Ÿè¦š',  # æ„Ÿè¦šæ•™è‚²
        ]
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢
        pattern_scores = defaultdict(int)
        
        for pattern in education_patterns:
            if re.search(pattern, content):
                pattern_scores['education'] += 3  # é«˜ã‚¹ã‚³ã‚¢
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢
        keyword_scores = defaultdict(int)
        
        for category, keywords in self.category_keywords.items():
            for keyword in keywords:
                if keyword in content_lower:
                    keyword_scores[category] += 1
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        total_scores = defaultdict(float)
        
        for category in self.category_keywords.keys():
            pattern_score = pattern_scores.get(category, 0)
            keyword_score = keyword_scores.get(category, 0)
            
            # æ•™è‚²ã‚«ãƒ†ã‚´ãƒªã¯ç‰¹åˆ¥æ‰±ã„
            if category == 'education':
                total_scores[category] = pattern_score * 2 + keyword_score
            else:
                total_scores[category] = pattern_score + keyword_score
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ
        if total_scores:
            best_category = max(total_scores, key=total_scores.get)
            confidence = total_scores[best_category] / max(1, len(content.split()) * 0.1)
            confidence = min(1.0, confidence)  # 1.0ã‚’ä¸Šé™ã¨ã™ã‚‹
        else:
            best_category = 'general'
            confidence = 0.1
        
        return {
            'name': best_category,
            'confidence': confidence,
            'scores': dict(total_scores),
            'pattern_matches': dict(pattern_scores),
            'keyword_matches': dict(keyword_scores)
        }
    
    def _intelligent_title_generation(self, content: str, category_result: dict) -> dict:
        """çŸ¥çš„ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ"""
        
        # è¤‡æ•°ã®æ‰‹æ³•ã‚’è©¦è¡Œ
        methods = []
        
        # 1. ä¸»é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        theme_title = self._extract_theme_title(content)
        if theme_title:
            methods.append({'method': 'theme_pattern', 'title': theme_title, 'score': 3.0})
        
        # 2. ã‚«ãƒ†ã‚´ãƒªç‰¹åŒ–ã‚¿ã‚¤ãƒˆãƒ«
        category_title = self._generate_category_specific_title(content, category_result['name'])
        if category_title:
            methods.append({'method': 'category_specific', 'title': category_title, 'score': 2.5})
        
        # 3. é‡è¦èªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        cluster_title = self._generate_cluster_title(content)
        if cluster_title:
            methods.append({'method': 'word_clustering', 'title': cluster_title, 'score': 2.0})
        
        # 4. æ ¸å¿ƒå†…å®¹æŠ½å‡º
        core_title = self._extract_core_content_title(content)
        if core_title:
            methods.append({'method': 'core_content', 'title': core_title, 'score': 1.5})
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®æ‰‹æ³•ã‚’é¸æŠ
        if methods:
            best_method = max(methods, key=lambda x: x['score'])
            return {
                'title': best_method['title'],
                'method': best_method['method'],
                'alternatives': [m for m in methods if m != best_method],
                'confidence': best_method['score'] / 3.0
            }
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            fallback_title = f"ãƒ¡ãƒ¢_{datetime.now().strftime('%m%d_%H%M')}"
            return {
                'title': fallback_title,
                'method': 'fallback',
                'alternatives': [],
                'confidence': 0.1
            }
    
    def _extract_theme_title(self, content: str) -> str:
        """ä¸»é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º"""
        
        patterns = [
            # ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æœ€å„ªå…ˆ
            r'(.{5,30})ã§ã¯ã€Œ(.{5,25})ã€ã¨ã€Œ(.{5,25})ã€',  # ã€Œã€œã§ã¯ã€ŒAã€ã¨ã€ŒBã€ã€
            r'(.{5,30})ã§ã¯ã€Œ(.{5,25})ã€',  # ã€Œã€œã§ã¯ã€ŒAã€ã€
            r'(.{5,30})ã§ã¯ã€(.{5,25})ã€',  # ã€Œã€œã§ã¯ã€Aã€ã€
            r'ã€Œ(.{5,30})ã®(.{5,25})ã€',  # ã€ŒAã®Bã€
            r'ã€(.{5,30})ã®(.{5,25})ã€',  # ã€Aã®Bã€
            # ä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
            r'(.{5,25})ã«ã¤ã„ã¦[ã¯è©±ã—èª¬æ˜è€ƒãˆè§£èª¬]',
            r'(.{5,25})ã¨ã¯[^ã-ã‚“]{0,10}',
            r'(.{5,25})ã‚’[è€ƒãˆèª¬æ˜è§£èª¬æ¤œè¨åˆ†æ]',
            r'(.{5,25})ã«é–¢ã—ã¦',
            r'(.{5,25})ã«ãŠã‘ã‚‹',
            r'é‡è¦ãªã®ã¯(.{5,25})',
            r'ãƒã‚¤ãƒ³ãƒˆã¯(.{5,25})',
            r'ã€Œ(.{5,30})ã€[ã¨ã¨ã„ã†]',
            r'ã€(.{5,30})ã€[ã¨ã¨ã„ã†]',
            # æœ€å¾Œã«ç·©ã„ãƒ‘ã‚¿ãƒ¼ãƒ³
            r'(.{5,25})ã®[å•é¡Œèª²é¡ŒåŠ¹æœæ–¹æ³•æ‰‹é †è§£èª¬èª¬æ˜]',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                # è¤‡æ•°ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒã‚ã‚‹å ´åˆã¯æœ€ã‚‚æ„å‘³ã®ã‚ã‚‹ã‚‚ã®ã‚’é¸æŠ
                groups = match.groups()
                theme = None
                
                # è¤‡æ•°ã‚°ãƒ«ãƒ¼ãƒ—ã®å‡¦ç†
                if len(groups) == 3 and all(g for g in groups):
                    # ã€ŒAã§ã¯Bã¨Cã€ãƒ‘ã‚¿ãƒ¼ãƒ³
                    first, second, third = groups[0].strip(), groups[1].strip(), groups[2].strip()
                    theme = f"{first}ã®{second}ã¨{third}"
                    if len(theme) > 30:
                        theme = f"{first}ã®{second}"
                elif len(groups) == 2 and all(g for g in groups):
                    # ã€ŒAã§ã¯Bã€ã¾ãŸã¯ã€ŒAã®Bã€ãƒ‘ã‚¿ãƒ¼ãƒ³
                    first, second = groups[0].strip(), groups[1].strip()
                    if len(first) > 3 and len(second) > 3:
                        theme = f"{first}ã®{second}"
                        if len(theme) > 30:
                            theme = second if len(second) < len(first) else first
                    else:
                        theme = first if len(first) > len(second) else second
                else:
                    # å˜ä¸€ã‚°ãƒ«ãƒ¼ãƒ—ã¾ãŸã¯ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                    for group in groups:
                        if group and len(group.strip()) > 2 and not self._is_meaningless_phrase(group.strip()):
                            theme = group.strip()
                            break
                
                if theme:
                    return self._clean_title_text(theme)
        
        return ""
    
    def _generate_category_specific_title(self, content: str, category: str) -> str:
        """ã‚«ãƒ†ã‚´ãƒªç‰¹åŒ–ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ"""
        
        if category == 'education':
            # æ•™è‚²ã‚«ãƒ†ã‚´ãƒªã®ç‰¹æ®Šå‡¦ç†
            if 'å¯¾å¥æ³•' in content and 'ãƒªã‚ºãƒ ' in content:
                return "å¯¾å¥æ³•ã¨ãƒªã‚ºãƒ åˆ†æ"
            elif 'è´è¦š' in content and 'äº”æ„Ÿ' in content:
                return "è´è¦šã¨äº”æ„Ÿã®å­¦ç¿’"
            elif 'ã²ã£ã‹ã‘' in content and 'å•é¡Œ' in content:
                return "ã²ã£ã‹ã‘å•é¡Œã®æŒ‡å°"
            elif re.search(r'[ã-ã‚“ãƒ¼]+ãã‚“', content):
                return "æˆæ¥­è¨˜éŒ²ã¨ç”Ÿå¾’æŒ‡å°"
            elif 'æŒ‡å°' in content:
                return "æ•™è‚²æŒ‡å°ãƒ¡ãƒ¢"
            else:
                return "æ•™è‚²é–¢é€£è¨˜éŒ²"
        
        elif category == 'tech':
            # å›ºæœ‰åè©ï¼ˆã‚«ã‚¿ã‚«ãƒŠãƒ»è‹±èªï¼‰ã‚’å„ªå…ˆçš„ã«æŠ½å‡º
            katakana_entities = re.findall(r'[ã‚¢-ãƒ¶ãƒ¼]{3,10}', content)
            english_entities = re.findall(r'[A-Z][a-z]+|ChatGPT|Python|API|JavaScript|React|Vue|Node|Git|GitHub|Docker|AWS|Azure|GCP|Obsidian', content)
            tech_terms = re.findall(r'API|ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°|ã‚·ã‚¹ãƒ†ãƒ |ã‚¢ãƒ—ãƒª|é–‹ç™º|æŠ€è¡“', content, re.IGNORECASE)
            
            # å›ºæœ‰åè©ã‚’å«ã‚€ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ
            entities = []
            if katakana_entities:
                # ä¸€èˆ¬èªã‚’é™¤å¤–
                common_katakana = {'ã‚¢ã‚¤ãƒ‡ã‚¢', 'ãƒ¡ãƒ¢', 'ãƒ•ã‚¡ã‚¤ãƒ«', 'ãƒ•ã‚©ãƒ«ãƒ€', 'ãƒšãƒ¼ã‚¸', 'ã‚µã‚¤ãƒˆ', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ãƒ‡ãƒ¼ã‚¿', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'ã‚µãƒ¼ãƒ“ã‚¹'}
                entities.extend([e for e in katakana_entities if e not in common_katakana])
            if english_entities:
                entities.extend(english_entities)
            
            if len(entities) >= 2:
                return f"{entities[0]}ã¨{entities[1]}ã®é–‹ç™º"
            elif len(entities) == 1:
                if tech_terms:
                    return f"{entities[0]}{tech_terms[0]}"
                else:
                    return f"{entities[0]}é–‹ç™ºãƒ¡ãƒ¢"
            elif tech_terms:
                return f"{tech_terms[0]}ã«é–¢ã™ã‚‹æŠ€è¡“ãƒ¡ãƒ¢"
            else:
                return "æŠ€è¡“é–¢é€£ãƒ¡ãƒ¢"
        
        elif category == 'business':
            # ãƒ“ã‚¸ãƒã‚¹å›ºæœ‰åè©ã‚’æŠ½å‡º
            katakana_entities = re.findall(r'[ã‚¢-ãƒ¶ãƒ¼]{3,10}', content)
            english_entities = re.findall(r'[A-Z][a-z]+|Instagram|Twitter|Facebook|LinkedIn|YouTube|TikTok|Google|Amazon|Apple', content)
            biz_terms = re.findall(r'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°|æˆ¦ç•¥|å–¶æ¥­|é›†å®¢|SEO|ãƒ“ã‚¸ãƒã‚¹', content, re.IGNORECASE)
            
            entities = []
            if katakana_entities:
                common_katakana = {'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'ãƒ“ã‚¸ãƒã‚¹', 'ã‚µãƒ¼ãƒ“ã‚¹', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'ãƒ‡ãƒ¼ã‚¿'}
                entities.extend([e for e in katakana_entities if e not in common_katakana])
            if english_entities:
                entities.extend(english_entities)
                
            if len(entities) >= 1:
                if biz_terms:
                    return f"{entities[0]}{biz_terms[0]}"
                else:
                    return f"{entities[0]}ãƒ“ã‚¸ãƒã‚¹æˆ¦ç•¥"
            elif biz_terms:
                return f"{biz_terms[0]}æˆ¦ç•¥ãƒ¡ãƒ¢"
            else:
                return "ãƒ“ã‚¸ãƒã‚¹é–¢é€£ãƒ¡ãƒ¢"
        
        elif category == 'ideas':
            # ã‚¢ã‚¤ãƒ‡ã‚¢ç³»ã‚‚å›ºæœ‰åè©ã‚’å«ã‚ã‚‹
            katakana_entities = re.findall(r'[ã‚¢-ãƒ¶ãƒ¼]{3,10}', content)
            english_entities = re.findall(r'[A-Z][a-z]+', content)
            
            entities = []
            if katakana_entities:
                entities.extend([e for e in katakana_entities[:2]])
            if english_entities:
                entities.extend([e for e in english_entities[:2]])
                
            if entities:
                return f"{entities[0]}ã‚¢ã‚¤ãƒ‡ã‚¢"
            else:
                return "ã‚¢ã‚¤ãƒ‡ã‚¢ãƒ»ä¼ç”»ãƒ¡ãƒ¢"
        
        return ""
    
    def _generate_cluster_title(self, content: str) -> str:
        """é‡è¦èªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆå›ºæœ‰åè©å„ªå…ˆï¼‰"""
        
        # å›ºæœ‰åè©ã‚’æœ€å„ªå…ˆã§æŠ½å‡º
        entities = []
        
        # ã‚«ã‚¿ã‚«ãƒŠå›ºæœ‰åè©
        katakana_entities = re.findall(r'[ã‚¢-ãƒ¶ãƒ¼]{3,10}', content)
        common_katakana = {'ã‚¢ã‚¤ãƒ‡ã‚¢', 'ãƒ¡ãƒ¢', 'ãƒ•ã‚¡ã‚¤ãƒ«', 'ãƒ•ã‚©ãƒ«ãƒ€', 'ãƒšãƒ¼ã‚¸', 'ã‚µã‚¤ãƒˆ', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ãƒ‡ãƒ¼ã‚¿', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'ã‚µãƒ¼ãƒ“ã‚¹', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'ãƒ“ã‚¸ãƒã‚¹'}
        entities.extend([e for e in katakana_entities if e not in common_katakana])
        
        # è‹±èªå›ºæœ‰åè©
        english_entities = re.findall(r'[A-Z][a-z]+|ChatGPT|Python|API|JavaScript|Obsidian|GitHub|Instagram|Twitter|Facebook|Google|Amazon|Apple', content)
        entities.extend(english_entities)
        
        # é‡è¦ãªæ—¥æœ¬èªèªå½™
        important_words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8}', content)
        word_freq = Counter(important_words)
        
        # ä»£è¡¨çš„ãªèªå½™ï¼ˆé »å‡ºãƒ»å°‚é–€ç”¨èªãƒ»å‹•ä½œèªï¼‰
        representative_words = []
        for word, freq in word_freq.items():
            if (freq >= 2 and len(word) > 2 and not self._is_common_word(word)) or \
               word in ['é–‹ç™º', 'è¨­è¨ˆ', 'å®Ÿè£…', 'åˆ†æ', 'æ¤œè¨', 'æ§‹ç¯‰', 'ä½œæˆ', 'ç”Ÿæˆ', 'é€£æº', 'æ´»ç”¨', 'è§£é‡ˆ', 'ç†è§£', 'æŒ‡å°', 'å­¦ç¿’', 'æˆæ¥­', 'è¨˜éŒ²']:
                representative_words.append(word)
        
        # ã‚¿ã‚¤ãƒˆãƒ«æ§‹ç¯‰ã®å„ªå…ˆé †ä½
        if len(entities) >= 2:
            return f"{entities[0]}ã¨{entities[1]}"
        elif len(entities) == 1 and len(representative_words) >= 1:
            return f"{entities[0]}{representative_words[0]}"
        elif len(entities) == 1:
            return f"{entities[0]}ã«ã¤ã„ã¦"
        elif len(representative_words) >= 2:
            return f"{representative_words[0]}ã¨{representative_words[1]}"
        elif len(representative_words) == 1:
            return f"{representative_words[0]}ã«é–¢ã—ã¦"
        
        return ""
    
    def _extract_core_content_title(self, content: str) -> str:
        """æ ¸å¿ƒå†…å®¹ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º"""
        
        sentences = re.split(r'[ã€‚ï¼ï¼ï¼Ÿ\n]', content)
        
        # å°å…¥æ–‡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ ¸å¿ƒéƒ¨åˆ†ã‚’è¦‹ã¤ã‘ã‚‹
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            
            # è³ªå•æ–‡ã‚„æŒ¨æ‹¶ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if (len(sentence) > 15 and 
                not sentence.endswith('ã§ã™ã‹ï¼Ÿ') and
                not sentence.endswith('ã ã‚ã†ã‹ï¼Ÿ') and
                not sentence.startswith('ã“ã®ã‚·ãƒ¼ãƒ³') and
                not sentence.startswith('ãã†ã§ã™ã­')):
                
                # æ–‡ã®ä¸»è¦éƒ¨åˆ†ã‚’æŠ½å‡º
                core_part = self._extract_sentence_core(sentence)
                if core_part:
                    return core_part
        
        return ""
    
    def _extract_sentence_core(self, sentence: str) -> str:
        """æ–‡ã®æ ¸å¿ƒéƒ¨åˆ†ã‚’æŠ½å‡º"""
        
        # ã€Œã€œã¯ã€œã§ã™ã€ã€Œã€œãŒã€œã™ã‚‹ã€ç­‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ä¸»èªãƒ»è¿°èªã‚’æŠ½å‡º
        patterns = [
            r'([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8})[ã¯ãŒ]([^ã€‚]{5,20})',
            r'([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8})ã‚’([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8})',
            r'([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8})ã¨ã„ã†([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, sentence)
            if match:
                subject = match.group(1)
                predicate = match.group(2)
                if len(subject) > 2 and len(predicate) > 2:
                    return f"{subject}ã®{predicate[:6]}"
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ–‡ã®æœ€åˆã®é‡è¦èªå¥
        words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,8}', sentence)
        important_words = [w for w in words if not self._is_common_word(w)]
        
        if important_words:
            return important_words[0]
        
        return ""
    
    def _comprehensive_tag_generation(self, content: str, category_result: dict) -> dict:
        """file-organizerå¼6å±¤ã‚¿ã‚°ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
        try:
            category = category_result['name']
            tags = set()
            
            # Layer 1: æœ€å„ªå…ˆ - å›ºæœ‰åè©ãƒ»å°‚é–€ç”¨èªï¼ˆé‡ã¿: 3å€ï¼‰
            try:
                priority_tags = self._extract_priority_entities(content, category)
                for tag in priority_tags:
                    tags.add(f"PRIORITY:{tag}")
            except:
                pass
            
            # Layer 2: ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã‚¿ã‚°ï¼ˆé‡ã¿: 2å€ï¼‰
            try:
                category_tags = self._get_category_base_tags(category)
                for tag in category_tags:
                    keywords = self._get_category_keywords(category, tag)
                    if any(keyword in content for keyword in keywords):
                        tags.add(f"CATEGORY:{tag}")
            except:
                pass
            
            # Layer 3: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ»å‹•ä½œã‚¿ã‚°
            try:
                action_tags = self._extract_action_tags_enhanced(content)
                tags.update(action_tags)
            except:
                pass
            
            # Layer 4: æ„Ÿæƒ…ãƒ»ãƒˆãƒ¼ãƒ³ã‚¿ã‚°
            try:
                emotion_tags = self._extract_emotion_tags_enhanced(content)
                tags.update(emotion_tags)
            except:
                pass
            
            # Layer 5: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚¿ã‚°
            try:
                content_type_tags = self._extract_content_type_tags_enhanced(content)
                tags.update(content_type_tags)
            except:
                pass
            
            # Layer 6: é »å‡ºèªã‚¿ã‚°ï¼ˆ2å›ä»¥ä¸Šå‡ºç¾ï¼‰
            try:
                frequent_tags = self._extract_frequent_terms_enhanced(content)
                tags.update(frequent_tags)
            except:
                pass
            
            # å„ªå…ˆåº¦ã«åŸºã¥ã„ã¦ã‚¿ã‚°ã‚’ã‚½ãƒ¼ãƒˆãƒ»é¸æŠ
            final_tags = self._prioritize_and_select_tags(tags, content)
            
            # ç©ºã®å ´åˆã¯å¾“æ¥æ–¹å¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not final_tags:
                fallback_tags = self._extract_priority_terms(content, category)
                final_tags = [f"#{tag}" for tag in fallback_tags]
            
            prioritized_tags = final_tags[:12]  # æœ€å¤§12å€‹
            
            return {
                'tags': prioritized_tags,
                'count': len(prioritized_tags),
                'layer_info': '6-layer hierarchical system',
                'method': 'file-organizer_enhanced'
            }
            
        except Exception as e:
            print(f"âš ï¸ ã‚¿ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼ã€å¾“æ¥æ–¹å¼ã‚’ä½¿ç”¨: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¾“æ¥ã®æ–¹å¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            fallback_tags = self._extract_priority_terms(content, category)
            return {
                'tags': [f"#{tag}" for tag in fallback_tags],
                'count': len(fallback_tags),
                'method': 'fallback'
            }
    
    def _extract_priority_entities(self, content: str, category: str) -> set:
        """Layer 1: æœ€å„ªå…ˆå›ºæœ‰åè©ãƒ»å°‚é–€ç”¨èªæŠ½å‡º"""
        entities = set()
        
        if category == 'education':
            # å­¦æ ¡åï¼ˆæœ€å„ªå…ˆï¼‰
            school_patterns = {
                'é–‹æˆ': ['é–‹æˆä¸­å­¦', 'é–‹æˆ'],
                'éº»å¸ƒ': ['éº»å¸ƒä¸­å­¦', 'éº»å¸ƒ'],
                'é§’æ±': ['é§’å ´æ±é‚¦', 'é§’æ±'],
                'æ¡œè”­': ['æ¡œè”­ä¸­å­¦', 'æ¡œè”­'],
                'å¥³å­å­¦é™¢': ['å¥³å­å­¦é™¢', 'JG'],
                'é›™è‘‰': ['é›™è‘‰ä¸­å­¦', 'é›™è‘‰'],
                'ç­‘é§’': ['ç­‘æ³¢å¤§é§’å ´', 'ç­‘é§’'],
                'æ¸‹å¹•': ['æ¸‹è°·å¹•å¼µ', 'æ¸‹å¹•'],
                'æ­¦è”µ': ['æ­¦è”µä¸­å­¦', 'æ­¦è”µ'],
                'SAPIX': ['ã‚µãƒ”ãƒƒã‚¯ã‚¹', 'SAPIX', 'ã‚µãƒ”']
            }
            for school, patterns in school_patterns.items():
                if any(pattern in content for pattern in patterns):
                    entities.add(school)
                    
        elif category == 'tech':
            # æŠ€è¡“å›ºæœ‰åè©ï¼ˆæœ€å„ªå…ˆï¼‰
            tech_entities = {
                'Claude': ['Claude', 'claude'],
                'ChatGPT': ['ChatGPT', 'chatgpt', 'Chat GPT'],
                'GitHub': ['GitHub', 'github', 'Github'],
                'Python': ['Python', 'python'],
                'JavaScript': ['JavaScript', 'javascript', 'JS'],
                'Cursor': ['Cursor', 'cursor'],
                'Obsidian': ['Obsidian', 'obsidian'],
                'MCP': ['MCP', 'mcp'],
                'Supabase': ['Supabase', 'supabase']
            }
            for entity, patterns in tech_entities.items():
                if any(pattern in content for pattern in patterns):
                    entities.add(entity)
                    
        elif category == 'media':
            # ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ»SNSå›ºæœ‰åè©ï¼ˆæœ€å„ªå…ˆï¼‰
            media_entities = {
                'è¥¿æ‘å‰µä¸€æœ—': ['è¥¿æ‘å‰µä¸€æœ—', 'è¥¿æ‘'],
                'è¥¿å·å°†å²': ['è¥¿å·å°†å²', 'è¥¿å·'],
                'æ¢¶è°·å¥äºº': ['æ¢¶è°·å¥äºº', 'æ¢¶è°·'],
                'Xåˆ†æ': ['Xåˆ†æ', 'ï¼¸åˆ†æ'],
                'SNSåˆ†æ': ['SNSåˆ†æ', 'ãƒã‚¹ãƒˆåˆ†æ', 'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†æ'],
                'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ': ['ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'ã„ã„ã­', 'ãƒªãƒã‚¹ãƒˆ']
            }
            for entity, patterns in media_entities.items():
                if any(pattern in content for pattern in patterns):
                    entities.add(entity)
        
        # ä¸€èˆ¬çš„ãªé‡è¦å›ºæœ‰åè©
        general_entities = re.findall(r'\\b[A-Z][a-zA-Z]{3,15}\\b', content)
        for entity in general_entities:
            if len(entity) >= 4 and not re.match(r'^[A-Z]{3,4}$', entity):
                entities.add(entity)
        
        return entities
    
    def _get_category_base_tags(self, category: str) -> list:
        """Layer 2: ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã‚¿ã‚°å®šç¾©"""
        category_base_tags = {
            'education': ['ä¸­å­¦å—é¨“', 'å›½èªæŒ‡å°', 'éå»å•åˆ†æ', 'å…¥è©¦å¯¾ç­–', 'èª­è§£æŒ‡å°', 'è¡¨ç¾æŒ‡å°'],
            'tech': ['ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'AIé–‹ç™º', 'ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰', 'APIé€£æº', 'ãƒ‡ãƒ¼ã‚¿åˆ†æ', 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°'],
            'media': ['SNSæˆ¦ç•¥', 'SNSé‹ç”¨', 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ', 'ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼åˆ†æ', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ†æ'],
            'business': ['ãƒ“ã‚¸ãƒã‚¹æˆ¦ç•¥', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥', 'å£²ä¸Šåˆ†æ', 'é¡§å®¢ç²å¾—', 'ãƒ–ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°'],
            'ideas': ['ã‚¢ã‚¤ãƒ‡ã‚¢å‰µå‡º', 'ä¼ç”»ç«‹æ¡ˆ', 'ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°'],
            'general': ['ãƒ¡ãƒ¢', 'è¨˜éŒ²', 'æ•´ç†']
        }
        return category_base_tags.get(category, [])
    
    def _get_category_keywords(self, category: str, tag: str) -> list:
        """ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚¿ã‚°åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—"""
        keyword_map = {
            'education': {
                'ä¸­å­¦å—é¨“': ['ä¸­å­¦å—é¨“', 'å—é¨“', 'å…¥è©¦', 'åˆæ ¼'],
                'å›½èªæŒ‡å°': ['å›½èª', 'èª­è§£', 'è¡¨ç¾', 'æ–‡ç« '],
                'éå»å•åˆ†æ': ['éå»å•', 'å‡ºé¡Œå‚¾å‘', 'åˆ†æ'],
            },
            'tech': {
                'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°': ['ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚³ãƒ¼ãƒ‰', 'é–‹ç™º'],
                'AIé–‹ç™º': ['AI', 'æ©Ÿæ¢°å­¦ç¿’', 'Claude', 'ChatGPT'],
                'APIé€£æº': ['API', 'é€£æº', 'æ¥ç¶š'],
            },
            'media': {
                'SNSæˆ¦ç•¥': ['SNS', 'æˆ¦ç•¥', 'X', 'Twitter'],
                'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ†æ': ['ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'ã„ã„ã­', 'ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼'],
            }
        }
        return keyword_map.get(category, {}).get(tag, [])
    
    def _extract_action_tags_enhanced(self, content: str) -> set:
        """Layer 3: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ»å‹•ä½œã‚¿ã‚°æŠ½å‡º"""
        actions = set()
        action_patterns = {
            'å­¦ç¿’': ['å­¦ç¿’', 'å‹‰å¼·', 'ç¿’å¾—', 'ç†è§£'],
            'åˆ†æ': ['åˆ†æ', 'è§£æ', 'èª¿æŸ»', 'æ¤œè¨¼'],
            'è¨˜éŒ²': ['è¨˜éŒ²', 'ãƒ¡ãƒ¢', 'ä¿å­˜', 'æ•´ç†'],
            'è¨ˆç”»': ['è¨ˆç”»', 'æˆ¦ç•¥', 'è¨­è¨ˆ', 'ä¼ç”»'],
            'å®Ÿè¡Œ': ['å®Ÿè¡Œ', 'å®Ÿæ–½', 'å®Ÿè£…', 'é–‹ç™º'],
            'è©•ä¾¡': ['è©•ä¾¡', 'æ¤œè¨', 'åˆ¤æ–­', 'ç¢ºèª']
        }
        
        for action, keywords in action_patterns.items():
            if any(keyword in content for keyword in keywords):
                actions.add(action)
        
        return actions
    
    def _extract_emotion_tags_enhanced(self, content: str) -> set:
        """Layer 4: æ„Ÿæƒ…ãƒ»ãƒˆãƒ¼ãƒ³ã‚¿ã‚°æŠ½å‡º"""
        emotions = set()
        emotion_patterns = {
            'é‡è¦': ['é‡è¦', 'å¤§åˆ‡', 'å¿…é ˆ', '!', 'ï¼'],
            'ç–‘å•': ['ï¼Ÿ', '?', 'ã©ã†', 'ãªãœ', 'ã©ã®ã‚ˆã†ã«'],
            'ãƒã‚¸ãƒ†ã‚£ãƒ–': ['ç´ æ™´ã‚‰ã—ã„', 'è‰¯ã„', 'æˆåŠŸ', 'æ”¹å–„'],
            'èª²é¡Œ': ['èª²é¡Œ', 'å•é¡Œ', 'æ”¹å–„', 'å¯¾ç­–'],
            'ç™ºè¦‹': ['ç™ºè¦‹', 'æ°—ã¥ã', 'å­¦ã³', 'ã²ã‚‰ã‚ã']
        }
        
        for emotion, keywords in emotion_patterns.items():
            if any(keyword in content for keyword in keywords):
                emotions.add(emotion)
        
        return emotions
    
    def _extract_content_type_tags_enhanced(self, content: str) -> set:
        """Layer 5: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚¿ã‚°æŠ½å‡º"""
        content_types = set()
        type_patterns = {
            'ã‚¢ã‚¤ãƒ‡ã‚¢': ['ã‚¢ã‚¤ãƒ‡ã‚¢', 'æ¡ˆ', 'ææ¡ˆ', 'æ€ã„ã¤ã'],
            'ãƒ¬ãƒãƒ¼ãƒˆ': ['çµæœ', 'å ±å‘Š', 'ãƒ¬ãƒãƒ¼ãƒˆ', 'ã¾ã¨ã‚'],
            'ãƒ¡ãƒ¢': ['ãƒ¡ãƒ¢', 'è¦šæ›¸', 'å‚™å¿˜éŒ²'],
            'ãƒ„ãƒ¼ãƒ«': ['ãƒ„ãƒ¼ãƒ«', 'é“å…·', 'ã‚¢ãƒ—ãƒª', 'ã‚µãƒ¼ãƒ“ã‚¹'],
            'ãƒ—ãƒ­ã‚»ã‚¹': ['æ‰‹é †', 'ã‚¹ãƒ†ãƒƒãƒ—', 'ãƒ—ãƒ­ã‚»ã‚¹', 'æ–¹æ³•']
        }
        
        for content_type, keywords in type_patterns.items():
            if any(keyword in content for keyword in keywords):
                content_types.add(content_type)
        
        return content_types
    
    def _extract_frequent_terms_enhanced(self, content: str) -> set:
        """Layer 6: é »å‡ºèªã‚¿ã‚°æŠ½å‡ºï¼ˆ2å›ä»¥ä¸Šå‡ºç¾ï¼‰"""
        frequent_terms = set()
        
        # æ—¥æœ¬èªã®æ„å‘³ã®ã‚ã‚‹èªï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰
        japanese_words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,8}', content)
        word_counts = Counter(japanese_words)
        
        for word, count in word_counts.items():
            if (count >= 2 and len(word) >= 3 and 
                not re.match(r'^[ã‚ã„ã†ãˆãŠã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ãªã«ã¬ã­ã®ã¯ã²ãµã¸ã»ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“]+$', word)):
                frequent_terms.add(word)
        
        return frequent_terms
    
    def _prioritize_and_select_tags(self, tags: set, content: str) -> list:
        """å„ªå…ˆåº¦ã«åŸºã¥ã„ã¦ã‚¿ã‚°ã‚’ã‚½ãƒ¼ãƒˆãƒ»é¸æŠ"""
        prioritized_tags = []
        
        # å„ªå…ˆåº¦é †ã§ã‚¿ã‚°ã‚’å‡¦ç†
        priority_order = ['PRIORITY:', 'CATEGORY:', '']
        
        for prefix in priority_order:
            matching_tags = [tag for tag in tags if tag.startswith(prefix)]
            
            # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ãªã‚¿ã‚°ã«
            clean_tags = []
            for tag in matching_tags:
                clean_tag = tag.replace('PRIORITY:', '').replace('CATEGORY:', '')
                if len(clean_tag) >= 2:
                    clean_tags.append(f"#{clean_tag}")
            
            prioritized_tags.extend(clean_tags)
        
        # é‡è¤‡é™¤å»ã—ã¦é †åºä¿æŒ
        seen = set()
        final_tags = []
        for tag in prioritized_tags:
            if tag not in seen:
                seen.add(tag)
                final_tags.append(tag)
        
        return final_tags
    
    def _extract_priority_terms(self, content: str, category: str) -> set:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æœ€å„ªå…ˆå›ºæœ‰åè©æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        tags = set()
        
        if category == 'education':
            # å­¦æ ¡åã‚’æœ€å„ªå…ˆã§æŠ½å‡º
            school_names = ['é–‹æˆ', 'éº»å¸ƒ', 'é§’æ±', 'æ¡œè”­', 'å¥³å­å­¦é™¢', 'é›™è‘‰', 'ç­‘é§’', 'æ¸‹å¹•', 'æ¸‹æ¸‹', 'æ­¦è”µ', 'æµ·åŸ']
            for school in school_names:
                if school in content:
                    tags.add(school)
            
            # é‡è¦ãªæ•™è‚²ç”¨èª
            key_terms = ['ä¸­å­¦å—é¨“', 'å›½èª', 'éå»å•', 'å…¥è©¦', 'åˆ†æ', 'å‚¾å‘', 'å¯¾ç­–', 'SAPIX', 'ã‚µãƒ”ãƒƒã‚¯ã‚¹', 'å››è°·å¤§å¡š', 'æ—¥èƒ½ç ”']
            for term in key_terms:
                if term in content:
                    tags.add(term)
                    
        elif category == 'tech':
            # é‡è¦ãªæŠ€è¡“ç”¨èª
            key_tech = ['GitHub', 'Git', 'Python', 'JavaScript', 'API', 'ChatGPT', 'Claude', 'AI', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚¢ãƒ—ãƒª', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'é–‹ç™º', 'ãƒˆãƒ¼ã‚¯ãƒ³', 'èªè¨¼']
            for term in key_tech:
                if term in content or term.lower() in content.lower():
                    tags.add(term)
                    
        elif category == 'media':
            # é‡è¦ãªãƒ¡ãƒ‡ã‚£ã‚¢ãƒ»SNSç”¨èª
            key_media = ['X', 'Twitter', 'SNS', 'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ', 'ãƒã‚¹ãƒˆ', 'ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼', 'ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼', 'åˆ†æ', 'è¥¿æ‘å‰µä¸€æœ—', 'è¥¿å·å°†å²', 'æ¢¶è°·å¥äºº', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ']
            for term in key_media:
                if term in content:
                    tags.add(term)
        
        return tags

    def _find_related_files(self, content: str, title: str) -> dict:
        """file-organizerå¼å¼·åŒ–é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢"""
        
        try:
            vault_path = Path(self.obsidian_path)
            relations = []
            
            # æ—¢å­˜ã®markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            md_files = list(vault_path.rglob('*.md'))
            
            for md_file in md_files:
                try:
                    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                    with open(md_file, 'r', encoding='utf-8') as f:
                        file_content = f.read()
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
                    file_title = md_file.stem
                    
                    # é–¢é€£åº¦ã‚’è¨ˆç®—ï¼ˆéšå±¤çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
                    relation_score = self._calculate_hierarchical_relation_score(
                        content, file_content, title, file_title
                    )
                    
                    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å³æ ¼ãªé–¾å€¤è¨­å®šï¼ˆé–¢é€£åº¦å‘ä¸Šï¼‰
                    threshold = self._get_relation_threshold(title, file_title)
                    
                    if relation_score > threshold:
                        # æ˜Ÿè©•ä¾¡ã‚’è¨ˆç®—
                        star_rating = self._calculate_star_rating(relation_score)
                        
                        relations.append({
                            'file_path': str(md_file),
                            'file_name': file_title,
                            'score': relation_score,
                            'star_rating': star_rating,
                            'relation_type': self._determine_relation_type_enhanced(content, file_content),
                            'preview': file_content[:100] + "..." if len(file_content) > 100 else file_content
                        })
                
                except Exception as e:
                    continue
            
            # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            relations.sort(key=lambda x: x['score'], reverse=True)
            
            return {
                'relations': relations[:3],  # ä¸Šä½3ä»¶ï¼ˆç²¾åº¦å‘ä¸Šï¼‰
                'count': len(relations),
                'total_files_checked': len(md_files)
            }
            
        except Exception as e:
            return {
                'relations': [],
                'count': 0,
                'error': str(e)
            }
    
    def _calculate_hierarchical_relation_score(self, content1: str, content2: str, title1: str, title2: str) -> float:
        """file-organizerå¼éšå±¤çš„é–¢é€£åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        max_score = 0.0
        
        # 1. ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ï¼ˆæœ€é‡è¦ï¼‰
        title_similarity = self._calculate_title_similarity(title1, title2)
        if title_similarity > 0.3:  # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦é–¾å€¤
            max_score = max(max_score, title_similarity * 1.5)  # é‡ã¿ä»˜ã‘
        
        # 2. ã‚¿ã‚°é¡ä¼¼åº¦
        tags1 = self._extract_simple_tags(content1)
        tags2 = self._extract_simple_tags(content2)
        tag_similarity = self._calculate_jaccard_similarity(tags1, tags2)
        if tag_similarity > 0.2:
            max_score = max(max_score, tag_similarity * 1.2)
        
        # 3. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é¡ä¼¼åº¦
        jaccard_similarity = self._calculate_content_jaccard_similarity(content1, content2)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å³æ ¼ãªé–¾å€¤è¨­å®šï¼ˆé–¢é€£åº¦å‘ä¸Šï¼‰
        if self._is_sns_analysis_file(title1) and self._is_sns_analysis_file(title2):
            if jaccard_similarity > 0.15:  # SNSåˆ†æåŒå£«ï¼šã‚ˆã‚Šå³æ ¼
                max_score = max(max_score, jaccard_similarity)
        elif self._is_tech_file(title1) and self._is_tech_file(title2):
            if jaccard_similarity > 0.12:  # Techç³»åŒå£«ï¼šã‚ˆã‚Šå³æ ¼  
                max_score = max(max_score, jaccard_similarity)
        else:
            if jaccard_similarity > 0.18:  # ä¸€èˆ¬ãƒ•ã‚¡ã‚¤ãƒ«ï¼šã‚ˆã‚Šå³æ ¼
                max_score = max(max_score, jaccard_similarity)
        
        return max_score
    
    def _calculate_title_similarity(self, title1: str, title2: str) -> float:
        """ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦è¨ˆç®—"""
        words1 = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯A-Za-z]{2,}', title1.lower()))
        words2 = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯A-Za-z]{2,}', title2.lower()))
        
        # ä¸€èˆ¬èªé™¤å¤–
        common_words = {'ã«ã¤ã„ã¦', 'ã«é–¢ã—ã¦', 'ã®æ–¹æ³•', 'ã«ã¤ã„ã¦', 'ã¾ã¨ã‚', 'ãƒ¡ãƒ¢'}
        words1 = words1 - common_words
        words2 = words2 - common_words
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        return intersection / union if union > 0 else 0.0
    
    def _extract_simple_tags(self, content: str) -> set:
        """ç°¡æ˜“ã‚¿ã‚°æŠ½å‡º"""
        words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,8}', content)
        word_counts = Counter(words)
        return {word for word, count in word_counts.items() if count >= 2}
    
    def _calculate_jaccard_similarity(self, set1: set, set2: set) -> float:
        """Jaccardä¿‚æ•°è¨ˆç®—"""
        if not set1 or not set2:
            return 0.0
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        return intersection / union if union > 0 else 0.0
    
    def _calculate_content_jaccard_similarity(self, content1: str, content2: str) -> float:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®Jaccardé¡ä¼¼åº¦è¨ˆç®—"""
        words1 = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,}', content1.lower()))
        words2 = set(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,}', content2.lower()))
        
        # ä¸€èˆ¬çš„ã™ãã‚‹èªã‚’é™¤å¤–
        common_words = {'ã«ã¤ã„ã¦', 'ã«é–¢ã—ã¦', 'ãŒã§ãã‚‹', 'ã§ã‚ã‚‹', 'ã¦ã„ã‚‹', 'ã¾ã—ãŸ', 'ã—ã¾ã™', 'ã•ã‚ŒãŸ'}
        words1 = words1 - common_words
        words2 = words2 - common_words
        
        return self._calculate_jaccard_similarity(words1, words2)
    
    def _is_sns_analysis_file(self, title: str) -> bool:
        """SNSåˆ†æãƒ•ã‚¡ã‚¤ãƒ«åˆ¤å®š"""
        sns_keywords = ['XæŠ•ç¨¿', 'SNS', 'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†æ', 'ãƒã‚¹ãƒˆåˆ†æ', 'ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼', 'ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼']
        return any(keyword in title for keyword in sns_keywords)
    
    def _is_tech_file(self, title: str) -> bool:
        """æŠ€è¡“ãƒ•ã‚¡ã‚¤ãƒ«åˆ¤å®š"""
        tech_keywords = ['API', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚·ã‚¹ãƒ†ãƒ ', 'GitHub', 'Python', 'AI', 'Claude', 'ã‚³ãƒ¼ãƒ‰']
        return any(keyword in title for keyword in tech_keywords)
    
    def _get_relation_threshold(self, title1: str, title2: str) -> float:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥é–¢é€£é–¾å€¤å–å¾—"""
        if self._is_sns_analysis_file(title1) and self._is_sns_analysis_file(title2):
            return 0.15  # SNSåˆ†æåŒå£«ï¼šå³æ ¼
        elif self._is_tech_file(title1) and self._is_tech_file(title2):
            return 0.12  # Techç³»åŒå£«ï¼šå³æ ¼
        else:
            return 0.18  # ä¸€èˆ¬ï¼šã‚ˆã‚Šå³æ ¼
    
    def _calculate_star_rating(self, score: float) -> str:
        """ã‚¹ã‚³ã‚¢ã‹ã‚‰æ˜Ÿè©•ä¾¡ã‚’è¨ˆç®—"""
        if score >= 0.7:
            return "â˜…â˜…â˜…â˜…â˜…"
        elif score >= 0.5:
            return "â˜…â˜…â˜…â˜…"
        elif score >= 0.3:
            return "â˜…â˜…â˜…"
        elif score >= 0.2:
            return "â˜…â˜…"
        else:
            return "â˜…"
    
    def _determine_relation_type_enhanced(self, content1: str, content2: str) -> str:
        """é–¢é€£ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        
        # ç°¡æ˜“çš„ãªé–¢é€£ã‚¿ã‚¤ãƒ—åˆ¤å®š
        if any(word in content1 and word in content2 for word in ['æ•™è‚²', 'æŒ‡å°', 'æˆæ¥­']):
            return 'educational'
        elif any(word in content1 and word in content2 for word in ['æŠ€è¡“', 'API', 'ã‚·ã‚¹ãƒ†ãƒ ']):
            return 'technical'
        elif any(word in content1 and word in content2 for word in ['ãƒ“ã‚¸ãƒã‚¹', 'æˆ¦ç•¥', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°']):
            return 'business'
        else:
            return 'general'
    
    def _build_preview_info(self, category_result: dict, title_result: dict, 
                           tags_result: dict, relations_result: dict) -> dict:
        """ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºç”¨æƒ…å ±æ§‹ç¯‰"""
        
        return {
            'category_display': f"{category_result['name']} (ä¿¡é ¼åº¦: {category_result['confidence']:.1%})",
            'title_display': title_result['title'],
            'tags_display': ' '.join(tags_result['tags'][:5]),  # æœ€åˆã®5å€‹
            'relations_display': f"{relations_result['count']}ä»¶ã®é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«",
            'full_analysis': {
                'category_scores': category_result.get('scores', {}),
                'title_alternatives': title_result.get('alternatives', []),
                'tag_sources': tags_result.get('sources', {}),
                'relation_details': relations_result.get('relations', [])
            }
        }
    
    # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _is_meaningless_phrase(self, phrase: str) -> bool:
        """ç„¡æ„å‘³ãªãƒ•ãƒ¬ãƒ¼ã‚ºã‹ãƒã‚§ãƒƒã‚¯"""
        meaningless = ['ã“ã®ã‚·ãƒ¼ãƒ³', 'ãã†ã§ã™ã­', 'ã‚ã‹ã‚‹', 'ã§ã™ã­', 'ã¾ã™', 'ã§ã™']
        return any(m in phrase for m in meaningless)
    
    def _clean_title_text(self, text: str) -> str:
        """ã‚¿ã‚¤ãƒˆãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        text = re.sub(r'[ã¯ãŒã‚’ã«ã§ã¨]$', '', text)
        text = re.sub(r'ã«ã¤ã„ã¦$|ã«é–¢ã—ã¦$|ã§ã¯$', '', text)
        # å¼•ç”¨ç¬¦ã®å‡¦ç†ï¼ˆé€”ä¸­ã§åˆ‡ã‚ŒãŸã‚‚ã®ã‚‚å«ã‚€ï¼‰
        text = re.sub(r'^[ã€Œã€]', '', text)
        text = re.sub(r'[ã€ã€]$', '', text)
        text = re.sub(r'ã€ã¨ã€Œ', 'ã¨', text)  # ã€ŒAã€ã¨ã€ŒBã€â†’Aã¨B
        text = re.sub(r'ã€ã¨ã€', 'ã¨', text)  # ã€Aã€ã¨ã€Bã€â†’Aã¨B
        # é©åˆ‡ãªé•·ã•ã«èª¿æ•´ï¼ˆ30æ–‡å­—ã¾ã§ï¼‰
        cleaned = text.strip()
        if len(cleaned) > 30:
            cleaned = cleaned[:27] + "..."
        return cleaned
    
    def _is_common_word(self, word: str) -> bool:
        """ä¸€èˆ¬çš„ãªèªã‹ãƒã‚§ãƒƒã‚¯"""
        common = {'ã‚ã‚‹', 'ã„ã‚‹', 'ã™ã‚‹', 'ãªã‚‹', 'ã§ã™', 'ã¾ã™', 'ã“ã®', 'ãã®', 'ã‚ã®', 'ãã‚Œ', 'ã“ã‚Œ', 'ã‚ã‚Œ'}
        return word in common
    
    def _get_category_base_tags(self, category: str) -> list:
        """ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã‚¿ã‚°"""
        base_tags = {
            'education': ['#æ•™è‚²', '#å­¦ç¿’', '#æŒ‡å°'],
            'tech': ['#Tech', '#æŠ€è¡“', '#é–‹ç™º'],
            'business': ['#ãƒ“ã‚¸ãƒã‚¹', '#æˆ¦ç•¥', '#ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°'],
            'ideas': ['#ã‚¢ã‚¤ãƒ‡ã‚¢', '#ä¼ç”»', '#å‰µä½œ'],
            'general': ['#ãƒ¡ãƒ¢', '#è¨˜éŒ²']
        }
        return base_tags.get(category, ['#ãƒ¡ãƒ¢'])
    
    def _detect_content_type(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—æ¤œå‡º"""
        if re.search(r'https?://', content):
            return 'web_reference'
        elif re.search(r'ã‚¢ã‚¤ãƒ‡ã‚¢|ä¼ç”»|ææ¡ˆ', content):
            return 'idea'
        elif re.search(r'æˆæ¥­|æŒ‡å°|å­¦ç¿’', content):
            return 'learning'
        elif re.search(r'TODO|ã‚¿ã‚¹ã‚¯|ã‚„ã‚‹ã“ã¨', content):
            return 'todo'
        else:
            return 'general'
    
    def _get_content_type_tags(self, content_type: str) -> list:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—åˆ¥ã‚¿ã‚°"""
        type_tags = {
            'web_reference': ['#å‚è€ƒè³‡æ–™', '#Webè¨˜äº‹'],
            'idea': ['#ç™ºæƒ³', '#æ€è€ƒ'],
            'learning': ['#å­¦ç¿’è¨˜éŒ²', '#æˆæ¥­ãƒãƒ¼ãƒˆ'],
            'todo': ['#TODO', '#ã‚¿ã‚¹ã‚¯'],
            'general': []
        }
        return type_tags.get(content_type, [])
    
    def _extract_frequent_word_tags(self, content: str) -> list:
        """é »å‡ºèªã‚¿ã‚°æŠ½å‡º"""
        words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,8}', content)
        word_freq = Counter(words)
        
        frequent_tags = []
        for word, freq in word_freq.items():
            if freq >= 2 and len(word) > 2 and not self._is_common_word(word):
                frequent_tags.append(f"#{word}")
        
        return frequent_tags[:3]
    
    def _extract_emotion_tags(self, content: str) -> list:
        """æ„Ÿæƒ…ã‚¿ã‚°æŠ½å‡º"""
        emotion_patterns = {
            '#ãƒã‚¸ãƒ†ã‚£ãƒ–': r'ç´ æ™´ã‚‰ã—ã„|ã„ã„|è‰¯ã„|æ¥½ã—|å¬‰ã—',
            '#ãƒã‚¬ãƒ†ã‚£ãƒ–': r'å›°ã£ãŸ|æ‚ªã„|æ®‹å¿µ|æƒœã—ã„',
            '#ç–‘å•': r'ãªãœ|ã©ã†ã—ã¦|ã‚ã‹ã‚‰ãªã„|\?|ï¼Ÿ',
            '#é‡è¦': r'é‡è¦|å¤§åˆ‡|ãƒã‚¤ãƒ³ãƒˆ|æ ¸å¿ƒ'
        }
        
        tags = []
        for tag, pattern in emotion_patterns.items():
            if re.search(pattern, content):
                tags.append(tag)
        return tags
    
    def _extract_action_tags(self, content: str) -> list:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚°æŠ½å‡º"""
        action_patterns = {
            '#å­¦ç¿’': r'å­¦ã¶|è¦šãˆã‚‹|ç†è§£|ç¿’å¾—',
            '#åˆ†æ': r'åˆ†æ|æ¤œè¨|èª¿æŸ»|è€ƒå¯Ÿ',
            '#è¨˜éŒ²': r'è¨˜éŒ²|ä¿å­˜|ãƒ¡ãƒ¢|ãƒãƒ¼ãƒˆ'
        }
        
        tags = []
        for tag, pattern in action_patterns.items():
            if re.search(pattern, content):
                tags.append(tag)
        return tags
    
    def _extract_entity_tags(self, content: str) -> list:
        """å›ºæœ‰åè©ã‚¿ã‚°æŠ½å‡º"""
        entities = []
        
        # è‹±èªå›ºæœ‰åè©
        english_entities = re.findall(r'[A-Z][a-z]+|ChatGPT|Python|API|JavaScript|React|Vue|Node|Git|GitHub|Docker|AWS|Azure|GCP', content)
        entities.extend(english_entities)
        
        # ã‚«ã‚¿ã‚«ãƒŠå›ºæœ‰åè©ï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰
        katakana_entities = re.findall(r'[ã‚¢-ãƒ¶ãƒ¼]{3,10}', content)
        # ã‚ˆãã‚ã‚‹ä¸€èˆ¬èªã‚’é™¤å¤–
        common_katakana = {'ã‚¢ã‚¤ãƒ‡ã‚¢', 'ãƒ¡ãƒ¢', 'ãƒ•ã‚¡ã‚¤ãƒ«', 'ãƒ•ã‚©ãƒ«ãƒ€', 'ãƒšãƒ¼ã‚¸', 'ã‚µã‚¤ãƒˆ', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ãƒ‡ãƒ¼ã‚¿', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'ã‚µãƒ¼ãƒ“ã‚¹'}
        katakana_entities = [e for e in katakana_entities if e not in common_katakana]
        entities.extend(katakana_entities)
        
        # é‡è¤‡é™¤å»ã—ã¦æœ€å¤§5å€‹
        unique_entities = list(dict.fromkeys(entities))[:5]
        return [f"#{entity}" for entity in unique_entities]
    
    def _prioritize_tags(self, tags: list, content: str, sources: dict) -> list:
        """ã‚¿ã‚°ã®å„ªå…ˆé †ä½ä»˜ã‘"""
        tag_scores = {}
        
        for tag in tags:
            score = 1.0
            
            # ã‚½ãƒ¼ã‚¹ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
            source = sources.get(tag, 'unknown')
            if source == 'category':
                score += 2.0
            elif source == 'frequent_words':
                score += 1.0
            elif source == 'emotion':
                score += 0.5
            
            # å‡ºç¾é »åº¦ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
            tag_word = tag.replace('#', '')
            frequency = content.lower().count(tag_word.lower())
            score += frequency * 0.3
            
            tag_scores[tag] = score
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        return sorted(tag_scores.keys(), key=lambda x: tag_scores[x], reverse=True)


    def save_memo(self, content: str) -> dict:
        """ãƒ¡ãƒ¢ã‚’å®Ÿéš›ã«ä¿å­˜"""
        try:
            # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æã‚’å†å®Ÿè¡Œ
            analysis = self.preview_analysis(content)
            
            if not analysis['success']:
                return analysis
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            file_path = self._save_memo_file(
                analysis['title']['title'], 
                content, 
                analysis['category']['name'],
                analysis['tags']['tags'],
                analysis['relations']['relations']
            )
            
            # Obsidian [[]] ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
            if analysis['relations']['relations']:
                self._add_obsidian_links(str(file_path), analysis['relations']['relations'])
            
            return {
                'success': True,
                'file_path': str(file_path),
                'title': analysis['title']['title'],
                'category': analysis['category']['name'],
                'tags_count': len(analysis['tags']['tags']),
                'relations_count': len(analysis['relations']['relations'])
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _save_memo_file(self, title: str, content: str, category: str, tags: list, relations: list) -> Path:
        """ãƒ¡ãƒ¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        folder_name = self.category_folders.get(category, '4_General')
        save_dir = Path(self.obsidian_path) / self.inbox_path / folder_name
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_title = re.sub(r'[<>:"/\\|?*]', '', title)
        safe_title = re.sub(r'\s+', '_', safe_title)
        filename = f"{timestamp}_{safe_title}.md"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        file_path = save_dir / filename
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹æ§‹ç¯‰
        file_content = self._build_markdown_content(title, content, category, tags, relations)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(file_content)
        
        return file_path
    
    def _build_markdown_content(self, title: str, content: str, category: str, tags: list, relations: list) -> str:
        """Markdownãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’æ§‹ç¯‰"""
        
        lines = []
        
        # YAMLãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼
        lines.append('---')
        lines.append(f'title: "{title}"')
        lines.append(f'category: {category}')
        lines.append(f'tags: {json.dumps(tags, ensure_ascii=False)}')
        lines.append(f'created: {datetime.now().isoformat()}')
        lines.append('---')
        lines.append('')
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        lines.append(f'# {title}')
        lines.append('')
        
        # ã‚¿ã‚°è¡¨ç¤º
        if tags:
            lines.append(f'**ã‚¿ã‚°**: {" ".join(tags)}')
            lines.append('')
        
        # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«
        if relations:
            lines.append('## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«')
            lines.append('')
            for relation in relations:
                score_pct = int(relation['score'] * 100)
                lines.append(f'- [[{relation["file_name"]}]] - {relation["relation_type"]} (é¡ä¼¼åº¦: {score_pct}%)')
            lines.append('')
        
        # ãƒ¡ã‚¤ãƒ³å†…å®¹
        lines.append('## å†…å®¹')
        lines.append('')
        lines.append(content)
        
        return '\n'.join(lines)
    
    def _add_obsidian_links(self, target_file_path: str, related_files: list):
        """Obsidianãƒ•ã‚¡ã‚¤ãƒ«ã«ç›¸äº’ãƒªãƒ³ã‚¯ã‚’è¿½åŠ """
        try:
            target_path = Path(target_file_path)
            
            if not target_path.exists():
                return
            
            # æ—¢å­˜ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿
            with open(target_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ /æ›´æ–°
            updated_content = self._add_new_links_section(content, related_files)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãæˆ»ã—
            with open(target_path, 'w', encoding='utf-8') as f:
                f.write(updated_content)
            
            print(f"ğŸ“ {target_path.name}ã«é–¢é€£ãƒªãƒ³ã‚¯ã‚’è¿½åŠ ")
            
            # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«å´ã«ã‚‚é€†ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
            self._add_reverse_links(target_path, related_files)
            
        except Exception as e:
            print(f"âš ï¸ ãƒªãƒ³ã‚¯è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _add_new_links_section(self, content: str, related_files: list) -> str:
        """æ–°ã—ã„é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ """
        
        # æ—¢å­˜ã®é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤
        content = re.sub(r'\n\n## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«\n\n.*?(?=\n\n##|\n\n---|$)', '', content, flags=re.DOTALL)
        content = re.sub(r'\n\n## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«\n\n.*?$', '', content, flags=re.DOTALL)
        
        if not related_files:
            return content
        
        # æ–°ã—ã„é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰
        links_section = "\n\n## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«\n\n"
        
        for rel_file in related_files:
            file_name = rel_file['file_name']
            star_rating = rel_file.get('star_rating', 'â˜…â˜…â˜…')
            
            # é–¢é€£ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
            relation_type = rel_file.get('relation_type', 'general')
            if relation_type == 'educational':
                comment = '(æ•™è‚²é–¢é€£)'
            elif relation_type == 'technical':
                comment = '(æŠ€è¡“é–¢é€£)'
            elif relation_type == 'business':
                comment = '(ãƒ“ã‚¸ãƒã‚¹é–¢é€£)'
            elif relation_type == 'media':
                comment = '(ãƒ¡ãƒ‡ã‚£ã‚¢é–¢é€£)'
            else:
                comment = '(ç›¸äº’ãƒªãƒ³ã‚¯)'
            
            links_section += f"- [[{file_name}]] {star_rating} {comment}\n"
        
        return content + links_section
    
    def _add_reverse_links(self, source_file: Path, related_files: list):
        """é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«å´ã«é€†ãƒªãƒ³ã‚¯ã‚’è¿½åŠ """
        source_name = source_file.stem
        
        for rel_file in related_files:
            try:
                rel_file_path = Path(rel_file['file_path'])
                
                if not rel_file_path.exists():
                    continue
                
                # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                with open(rel_file_path, 'r', encoding='utf-8') as f:
                    rel_content = f.read()
                
                # æ—¢ã«ç›¸äº’ãƒªãƒ³ã‚¯ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if f"[[{source_name}]]" in rel_content:
                    continue
                
                # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if "## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«" in rel_content:
                    # æ—¢å­˜ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
                    star_rating = rel_file.get('star_rating', 'â˜…â˜…â˜…')
                    new_link = f"- [[{source_name}]] {star_rating} (ç›¸äº’ãƒªãƒ³ã‚¯)\n"
                    
                    # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æœ€å¾Œã«è¿½åŠ 
                    rel_content = re.sub(
                        r'(## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«\n\n(?:.*\n)*)',
                        r'\1' + new_link,
                        rel_content
                    )
                else:
                    # æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
                    star_rating = rel_file.get('star_rating', 'â˜…â˜…â˜…')
                    new_section = f"\n\n## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«\n\n- [[{source_name}]] {star_rating} (ç›¸äº’ãƒªãƒ³ã‚¯)\n"
                    rel_content += new_section
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãæˆ»ã—
                with open(rel_file_path, 'w', encoding='utf-8') as f:
                    f.write(rel_content)
                
                print(f"ğŸ“ {rel_file_path.name}ã«é€†ãƒªãƒ³ã‚¯ã‚’è¿½åŠ ")
                
            except Exception as e:
                print(f"âš ï¸ é€†ãƒªãƒ³ã‚¯è¿½åŠ ã‚¨ãƒ©ãƒ¼ ({rel_file['file_name']}): {e}")
                continue


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹æ³•: python3 preview_enhanced_memo.py [preview|save] <content>")
        return
    
    command = sys.argv[1]
    memo_content = " ".join(sys.argv[2:])
    
    if not memo_content.strip():
        print("âŒ ãƒ¡ãƒ¢å†…å®¹ãŒç©ºã§ã™")
        return
    
    processor = IntegratedMemoProcessor()
    
    if command == "preview":
        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æå®Ÿè¡Œ
        result = processor.preview_analysis(memo_content)
        
        # AppleScriptç”¨ã®å˜ç´”ãªå½¢å¼ã§å‡ºåŠ›
        print("RESULT_START")
        print(f"TITLE:{result['title']['title']}")
        print(f"CATEGORY:{result['category']['name']}")
        
        # ã‚¿ã‚°ã‚’å˜ç´”ãªå½¢å¼ã§å‡ºåŠ›
        tags_list = result['tags']['tags']
        tags_str = ",".join(tags_list) if tags_list else "ãªã—"
        print(f"TAGS:{tags_str}")
        
        # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å˜ç´”ãªå½¢å¼ã§å‡ºåŠ›
        relations_list = result['relations']['relations']
        if relations_list:
            relation_names = [rel['file_name'] for rel in relations_list[:3]]
            relations_str = f"{len(relations_list)}ä»¶:" + ",".join(relation_names)
        else:
            relations_str = "ãªã—"
        print(f"RELATIONS:{relations_str}")
        print("RESULT_END")
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨ã«JSONå½¢å¼ã‚‚å‡ºåŠ›
        print("JSON_START")
        print(json.dumps(result, ensure_ascii=False, indent=2))
        print("JSON_END")
        
    elif command == "save":
        # å®Ÿéš›ã®ä¿å­˜å‡¦ç†
        result = processor.save_memo(memo_content)
        
        if result['success']:
            print(f"âœ… ãƒ¡ãƒ¢ä¿å­˜å®Œäº†!")
            print(f"ğŸ“‹ ã‚¿ã‚¤ãƒˆãƒ«: {result['title']}")
            print(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒª: {result['category']}")
            print(f"ğŸ·ï¸ ã‚¿ã‚°æ•°: {result['tags_count']}")
            print(f"ğŸ”— é–¢é€£æ•°: {result['relations_count']}")
            print(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«: {Path(result['file_path']).name}")
        else:
            print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {result['error']}")
    
    else:
        print("âŒ ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰ã€‚'preview' ã¾ãŸã¯ 'save' ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()