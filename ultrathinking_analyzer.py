#!/usr/bin/env python3
"""
Ultrathinking Analyzer - æ·±å±¤çš„ãªæ–‡æ›¸åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
æ–‡è„ˆç†è§£ã¨é–¢ä¿‚æ€§åˆ†æã«ã‚ˆã‚Šã€ã‚ˆã‚Šæ­£ç¢ºãªã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã¨ã‚¿ã‚°ä»˜ã‘ã‚’å®Ÿç¾
"""

import re
from typing import Dict, List, Set, Tuple, Any
from collections import Counter, defaultdict
import json
from datetime import datetime


class UltrathinkingAnalyzer:
    """Ultrathinking ã«ã‚ˆã‚‹æ·±å±¤çš„ãªå†…å®¹åˆ†æ"""
    
    def __init__(self):
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.document_patterns = {
            'report': {
                'markers': ['ãƒ¬ãƒãƒ¼ãƒˆ', 'åˆ†æ', 'æ¦‚è¦', 'èª¿æŸ»', 'çµæœ', 'å ±å‘Šæ›¸'],
                'structure_patterns': [
                    r'ã€.*ã€‘',  # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼
                    r'^\d+\.',  # ç•ªå·ä»˜ããƒªã‚¹ãƒˆ
                    r'â– |â–¶|â—†|ğŸ“Š|ğŸ“ˆ|ğŸ“‹',  # æ§‹é€ åŒ–ãƒãƒ¼ã‚«ãƒ¼
                    r'^#{1,3}\s',  # Markdownãƒ˜ãƒƒãƒ€ãƒ¼
                ],
                'content_patterns': [
                    'ãƒ‡ãƒ¼ã‚¿', 'æŒ‡æ¨™', 'çµæœ', 'æˆæœ', 'è©•ä¾¡', 'æ•°å€¤', 'çµ±è¨ˆ', 'KPI'
                ]
            },
            'meeting_notes': {
                'markers': ['ä¼šè­°', 'æ‰“ã¡åˆã‚ã›', 'ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°', 'è­°äº‹éŒ²', 'ç›¸è«‡', 'é¢è«‡'],
                'structure_patterns': [
                    r'æ—¥æ™‚[:ï¼š]', r'å‚åŠ è€…[:ï¼š]', r'è­°é¡Œ[:ï¼š]', r'å ´æ‰€[:ï¼š]'
                ],
                'content_patterns': [
                    'æ±ºå®šäº‹é …', 'æ¬¡å›', 'ã‚¢ã‚¯ã‚·ãƒ§ãƒ³', 'TODO', 'å®¿é¡Œ', 'ç¢ºèªäº‹é …'
                ]
            },
            'analysis': {
                'markers': ['åˆ†æ', 'è€ƒå¯Ÿ', 'è©•ä¾¡', 'æ¤œè¨¼', 'è¨ºæ–­', 'æ¤œè¨'],
                'structure_patterns': [
                    r'èª²é¡Œ[:ï¼š]', r'è§£æ±ºç­–[:ï¼š]', r'ææ¡ˆ[:ï¼š]', r'åŸå› [:ï¼š]'
                ],
                'content_patterns': [
                    'å•é¡Œç‚¹', 'æ”¹å–„', 'åŠ¹æœ', 'å½±éŸ¿', 'è¦å› ', 'å¯¾ç­–'
                ]
            },
            'plan': {
                'markers': ['è¨ˆç”»', 'äºˆå®š', 'ãƒ—ãƒ©ãƒ³', 'æˆ¦ç•¥', 'ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—'],
                'structure_patterns': [
                    r'ç›®æ¨™[:ï¼š]', r'æœŸé™[:ï¼š]', r'ã‚¹ãƒ†ãƒƒãƒ—\d+', r'ãƒ•ã‚§ãƒ¼ã‚º\d+'
                ],
                'content_patterns': [
                    'ç›®æ¨™', 'é”æˆ', 'ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³', 'æœŸé™', 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«'
                ]
            },
            'memo': {
                'markers': ['ãƒ¡ãƒ¢', 'å‚™å¿˜éŒ²', 'ãƒãƒ¼ãƒˆ', 'è¨˜éŒ²'],
                'structure_patterns': [
                    r'^ãƒ»', r'^-\s', r'^\*\s'  # ç®‡æ¡æ›¸ã
                ],
                'content_patterns': [
                    'æ€è€ƒ', 'ã‚¢ã‚¤ãƒ‡ã‚¢', 'æ°—ã¥ã', 'æ³¨æ„', 'è¦šãˆ'
                ]
            }
        }
        
        # é–¢ä¿‚æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.relation_patterns = {
            'causal': [
                r'(.+?)ã®ãŸã‚(.+?)',
                r'(.+?)ã«ã‚ˆã‚Š(.+?)',
                r'(.+?)ã®çµæœ(.+?)',
                r'(.+?)ãŒåŸå› ã§(.+?)'
            ],
            'temporal': [
                r'(.+?)ã®å¾Œ(.+?)',
                r'(.+?)ã™ã‚‹å‰ã«(.+?)',
                r'(.+?)ã—ã¦ã‹ã‚‰(.+?)',
                r'ã¾ãš(.+?)ã€æ¬¡ã«(.+?)'
            ],
            'conditional': [
                r'ã‚‚ã—(.+?)ãªã‚‰(.+?)',
                r'(.+?)ã®å ´åˆ(.+?)',
                r'(.+?)ã™ã‚‹ã¨(.+?)'
            ]
        }
    
    def analyze_content(self, content: str) -> Dict[str, Any]:
        """å¤šå±¤çš„ãªå†…å®¹åˆ†æ"""
        
        # Phase 1: è¡¨å±¤åˆ†æ
        surface_analysis = self._surface_analysis(content)
        
        # Phase 2: æ–‡è„ˆç†è§£
        context_analysis = self._context_analysis(content)
        
        # Phase 3: é–¢ä¿‚æ€§åˆ†æ
        relation_analysis = self._relation_analysis(content)
        
        # Phase 4: æ„å‘³çµ±åˆ
        semantic_integration = self._semantic_integration(
            surface_analysis, context_analysis, relation_analysis, content
        )
        
        # Phase 5: å‡ºåŠ›ç”Ÿæˆ
        return self._generate_intelligent_output(
            surface_analysis, context_analysis, relation_analysis, semantic_integration, content
        )
    
    def _surface_analysis(self, content: str) -> Dict[str, Any]:
        """è¡¨å±¤çš„ãªè¦ç´ ã®æŠ½å‡º"""
        return {
            'sentence_count': len(re.split(r'[ã€‚ï¼ï¼ï¼Ÿ\n]', content)),
            'word_frequency': self._calculate_word_frequency(content),
            'sentence_structures': self._analyze_sentence_structures(content),
            'key_phrases': self._extract_key_phrases(content)
        }
    
    def _context_analysis(self, content: str) -> Dict[str, Any]:
        """æ–‡è„ˆã®ç†è§£"""
        return {
            'document_type': self._identify_document_type(content),
            'main_topic': self._extract_main_topic(content),
            'subtopics': self._extract_subtopics(content),
            'intent': self._detect_intent(content),
            'domain': self._identify_domain(content),
            'tone': self._analyze_tone(content)
        }
    
    def _relation_analysis(self, content: str) -> Dict[str, Any]:
        """é–¢ä¿‚æ€§ã®åˆ†æ"""
        return {
            'entity_relations': self._analyze_entity_relations(content),
            'temporal_flow': self._analyze_temporal_flow(content),
            'causal_relations': self._extract_causal_relations(content),
            'hierarchical_structure': self._analyze_hierarchy(content)
        }
    
    def _semantic_integration(self, surface: Dict, context: Dict, 
                            relation: Dict, content: str) -> Dict[str, Any]:
        """æ„å‘³ã®çµ±åˆ"""
        return {
            'coherent_theme': self._integrate_themes(surface, context),
            'key_insights': self._extract_key_insights(relation),
            'implicit_meanings': self._infer_implicit_meanings(content, context),
            'action_items': self._extract_action_items(content)
        }
    
    def _identify_document_type(self, content: str) -> Dict[str, Any]:
        """æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã®é«˜ç²¾åº¦è­˜åˆ¥"""
        scores = {}
        
        for doc_type, patterns in self.document_patterns.items():
            score = 0
            matched_patterns = []
            
            # ãƒãƒ¼ã‚«ãƒ¼ã®å­˜åœ¨ç¢ºèª
            for marker in patterns['markers']:
                if marker in content:
                    score += 3
                    matched_patterns.append(f"marker:{marker}")
            
            # æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¢ºèª
            for pattern in patterns['structure_patterns']:
                matches = re.findall(pattern, content, re.MULTILINE)
                if matches:
                    score += 2 * len(matches)
                    matched_patterns.append(f"structure:{pattern}")
            
            # å†…å®¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¢ºèª
            for pattern in patterns['content_patterns']:
                if pattern in content:
                    score += 1
                    matched_patterns.append(f"content:{pattern}")
            
            scores[doc_type] = {
                'score': score,
                'matched_patterns': matched_patterns
            }
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š
        if scores:
            best_type = max(scores, key=lambda x: scores[x]['score'])
            total_score = sum(s['score'] for s in scores.values())
            confidence = scores[best_type]['score'] / total_score if total_score > 0 else 0
            
            return {
                'type': best_type,
                'confidence': confidence,
                'scores': scores,
                'matched_patterns': scores[best_type]['matched_patterns']
            }
        else:
            return {
                'type': 'general',
                'confidence': 0.1,
                'scores': {},
                'matched_patterns': []
            }
    
    def _extract_main_topic(self, content: str) -> Dict[str, Any]:
        """æ–‡æ›¸ã®ä¸»é¡Œã‚’çŸ¥çš„ã«æŠ½å‡º"""
        title_candidates = []
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ˜ç¤ºçš„ãªã‚¿ã‚¤ãƒˆãƒ«è¡¨è¨˜
        explicit_patterns = [
            (r'ã€(.+?)ã€‘', 0.9),
            (r'^#\s+(.+?)$', 0.85),
            (r'^##\s+(.+?)$', 0.8),
            (r'â– \s*(.+?)(?:\n|$)', 0.75)
        ]
        
        for pattern, confidence in explicit_patterns:
            match = re.search(pattern, content, re.MULTILINE)
            if match:
                title_candidates.append({
                    'text': match.group(1).strip(),
                    'confidence': confidence,
                    'method': 'explicit_title',
                    'position': match.start()
                })
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ç¬¬ä¸€æ–‡ã®ä¸»é¡Œï¼ˆå›ºæœ‰åè©ä¿è­·ï¼‰
        sentences = re.split(r'[ã€‚ï¼ï¼ï¼Ÿ\n]', content)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        metadata_patterns = [
            r'^ä½œæˆæ—¥[:ï¼š]\s*\d{4}-\d{2}-\d{2}',
            r'^æ›´æ–°æ—¥[:ï¼š]\s*\d{4}-\d{2}-\d{2}',
            r'^å®Ÿè£…è€…[:ï¼š]',
            r'^ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹[:ï¼š]',
            r'^æ—¥æ™‚[:ï¼š]',
            r'^å ´æ‰€[:ï¼š]',
            r'^å‚åŠ è€…[:ï¼š]',
            r'^ã‚¿ã‚°[:ï¼š]',
            r'^ã‚«ãƒ†ã‚´ãƒª[:ï¼š]',
            r'^ä½œæˆè€…[:ï¼š]',
            r'^è‘—è€…[:ï¼š]',
            r'^ä¿®æ­£æ—¥[:ï¼š]',
            r'^version[:ï¼š]',
            r'^Version[:ï¼š]',
            r'^ãƒãƒ¼ã‚¸ãƒ§ãƒ³[:ï¼š]',
            r'^Created[:ï¼š]',
            r'^Updated[:ï¼š]',
            r'^Date[:ï¼š]',
            r'^Tags[:ï¼š]',
            r'^Category[:ï¼š]'
        ]
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§ãªã„æœ€åˆã®æ„å‘³ã®ã‚ã‚‹æ–‡ã‚’è¦‹ã¤ã‘ã‚‹
        first_meaningful_sentence = None
        for sentence in sentences:
            sentence_stripped = sentence.strip()
            if not sentence_stripped:
                continue
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã‹ãƒã‚§ãƒƒã‚¯
            is_metadata = False
            for meta_pattern in metadata_patterns:
                if re.match(meta_pattern, sentence_stripped):
                    is_metadata = True
                    break
            
            if not is_metadata and 10 < len(sentence_stripped) < 100:
                first_meaningful_sentence = sentence_stripped
                break
        
        if first_meaningful_sentence:
                # å›ºæœ‰åè©ã‚’å«ã‚€å ´åˆã¯å„ªå…ˆåº¦ã‚’ä¸Šã’ã‚‹
                confidence = 0.7
                tech_keywords = ['Claude Code', 'ChatGPT', 'GitHub', 'Anthropic', 'OpenAI']
                if any(keyword in first_meaningful_sentence for keyword in tech_keywords):
                    confidence = 0.85
                
                # ä¸»èªã¨è¿°èªã®é–¢ä¿‚ã‚’åˆ†æ
                subject_predicate = self._extract_subject_predicate(first_meaningful_sentence)
                if subject_predicate:
                    title_text = self._format_title_from_sp(subject_predicate)
                    title_candidates.append({
                        'text': title_text,
                        'confidence': confidence,
                        'method': 'subject_predicate',
                        'position': 0
                    })
                else:
                    # ä¸»èªè¿°èªæŠ½å‡ºã«å¤±æ•—ã—ãŸå ´åˆã€ç¬¬ä¸€æ–‡ã‚’ãã®ã¾ã¾ä½¿ç”¨
                    title_candidates.append({
                        'text': first_meaningful_sentence,
                        'confidence': confidence * 0.8,
                        'method': 'first_sentence_direct',
                        'position': 0
                    })
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºã®çµ„ã¿åˆã‚ã›
        key_phrases = self._extract_key_phrases(content)
        if key_phrases:
            combined_title = self._combine_key_phrases_to_title(key_phrases)
            if combined_title:
                title_candidates.append({
                    'text': combined_title,
                    'confidence': 0.5,
                    'method': 'key_phrase_combination',
                    'position': -1
                })
        
        # æœ€é©ãªå€™è£œã‚’é¸æŠ
        if title_candidates:
            # ä½ç½®ï¼ˆå‰ã®æ–¹ãŒé«˜ã„ï¼‰ã¨ä¿¡é ¼åº¦ã‚’è€ƒæ…®
            best_candidate = max(title_candidates, 
                               key=lambda x: x['confidence'] - (x['position'] / 1000 if x['position'] >= 0 else 0))
            return best_candidate
        else:
            return {
                'text': 'ãƒ¡ãƒ¢',
                'confidence': 0.1,
                'method': 'fallback',
                'position': -1
            }
    
    def _extract_subject_predicate(self, sentence: str) -> Dict[str, str]:
        """æ–‡ã‹ã‚‰ä¸»èªã¨è¿°èªã‚’æŠ½å‡ºï¼ˆå“è³ªãƒã‚§ãƒƒã‚¯å¼·åŒ–ç‰ˆï¼‰"""
        # å“è³ªãƒã‚§ãƒƒã‚¯ï¼šä¸æ­£ãªæ–­ç‰‡åŒ–ã‚’é˜²ã
        if not sentence or len(sentence.strip()) < 8:
            return None
        
        # è‹±èªãƒ»æŠ€è¡“å›ºæœ‰åè©ã®ä¿è­·ãƒ‘ã‚¿ãƒ¼ãƒ³
        protected_words = [
            'Consulting', 'ChatGPT', 'Claude Code', 'Claude', 'GitHub', 'Obsidian', 
            'Twitter', 'Instagram', 'Facebook', 'LinkedIn', 'TikTok', 'YouTube', 
            'Google', 'Microsoft', 'Apple', 'Amazon', 'Netflix', 'Spotify',
            'Anthropic', 'OpenAI', 'React', 'TypeScript', 'JavaScript', 'Python',
            'Docker', 'Kubernetes', 'AWS', 'Azure', 'GCP'
        ]
        
        # ç°¡æ˜“çš„ãªä¸»èªãƒ»è¿°èªæŠ½å‡º
        patterns = [
            r'(.+?[ã¯ãŒã‚’])(.+)',
            r'(.+?)ã«ã¤ã„ã¦(.+)',
            r'(.+?)ã«é–¢ã™ã‚‹(.+)',
            r'(.+?)ã«ãŠã‘ã‚‹(.+)'
        ]
        
        for pattern in patterns:
            match = re.match(pattern, sentence)
            if match:
                subject = match.group(1).strip()
                predicate = match.group(2).strip()
                
                # ä¸»èªã‹ã‚‰åŠ©è©ã‚’é™¤å»
                subject = re.sub(r'[ã¯ãŒã‚’]$', '', subject)
                
                # å“è³ªãƒã‚§ãƒƒã‚¯ï¼šä¸æ­£ãªæ–­ç‰‡åŒ–é˜²æ­¢
                if self._is_fragmented_word(subject, protected_words):
                    continue
                
                # å“è³ªãƒã‚§ãƒƒã‚¯ï¼šçŸ­ã™ãã‚‹æ–­ç‰‡ã‚’æ’é™¤
                if len(subject) < 3 or len(predicate) < 2:
                    continue
                
                # å“è³ªãƒã‚§ãƒƒã‚¯ï¼šæ„å‘³ã®ãªã„æ–­ç‰‡ã‚’æ’é™¤
                if self._is_meaningless_fragment(subject) or self._is_meaningless_fragment(predicate):
                    continue
                
                # é•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®ï¼ˆæ—¥æœ¬èªã«é©ã—ãŸé•·ã•ï¼‰
                if len(subject) > 50:
                    subject = subject[:47] + '...'
                if len(predicate) > 60:
                    predicate = predicate[:57] + '...'
                
                return {
                    'subject': subject,
                    'predicate': predicate
                }
        
        return None
    
    def _is_fragmented_word(self, fragment: str, protected_words: List[str]) -> bool:
        """å›ºæœ‰åè©ã®æ–­ç‰‡åŒ–ã‚’æ¤œå‡º"""
        fragment_lower = fragment.lower()
        for word in protected_words:
            word_lower = word.lower()
            # å›ºæœ‰åè©ã®ä¸€éƒ¨ãŒæ–­ç‰‡ã¨ã—ã¦æŠ½å‡ºã•ã‚ŒãŸå ´åˆã‚’æ¤œå‡º
            if (fragment_lower in word_lower and 
                fragment_lower != word_lower and 
                len(fragment) < len(word) * 0.8):
                return True
        return False
    
    def _is_meaningless_fragment(self, fragment: str) -> bool:
        """æ„å‘³ã®ãªã„æ–­ç‰‡ã‚’æ¤œå‡º"""
        meaningless_patterns = [
            r'^[ã-ã‚“]{1,2}$',     # ã²ã‚‰ãŒãª1-2æ–‡å­—
            r'^[ã€‚ã€ï¼ï¼Ÿ]+$',        # å¥èª­ç‚¹ã®ã¿
            r'^[0-9]+$',           # æ•°å­—ã®ã¿
            r'^[a-zA-Z]{1,3}$',    # çŸ­ã„è‹±å­—
            r'^[ãƒ¼ãƒ»]+$'           # è¨˜å·ã®ã¿
        ]
        
        for pattern in meaningless_patterns:
            if re.match(pattern, fragment.strip()):
                return True
        return False
    
    def _format_title_from_sp(self, sp: Dict[str, str]) -> str:
        """ä¸»èªè¿°èªã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        subject = sp['subject']
        predicate = sp['predicate']
        
        # è¿°èªã®æ´»ç”¨å½¢ã‚’åè©å½¢ã«å¤‰æ›
        predicate_noun = self._convert_to_noun_form(predicate)
        
        # ã‚ˆã‚Šè‡ªç„¶ãªã‚¿ã‚¤ãƒˆãƒ«å½¢å¼ã‚’é¸æŠ
        total_length = len(subject) + len(predicate_noun)
        
        # è¿°èªãŒç©ºã®å ´åˆã¯ä¸»èªã®ã¿
        if not predicate_noun or predicate_noun.strip() == '':
            return subject
        
        if total_length < 45:
            # çŸ­ã„å ´åˆã¯ã€Œã®ã€ã§æ¥ç¶š
            if predicate_noun and predicate_noun != predicate:
                return f"{subject}ã®{predicate_noun}"
            else:
                # è¿°èªãŒå¤‰æ›ã•ã‚Œãªã‹ã£ãŸå ´åˆã¯é©åˆ‡ã«çµåˆ
                return f"{subject}ï¼š{predicate_noun}"
        else:
            # é•·ã„å ´åˆã¯ã€Œ-ã€ã§æ¥ç¶š
            return f"{subject} - {predicate_noun}"
    
    def _convert_to_noun_form(self, predicate: str) -> str:
        """è¿°èªã‚’åè©å½¢ã«å¤‰æ›"""
        # ã‚ˆã‚Šè‡ªç„¶ãªå¤‰æ›è¦å‰‡ï¼ˆç©ºã«ãªã‚‹ã“ã¨ã‚’é˜²ãï¼‰
        conversions = {
            'ã—ã¾ã—ãŸ': 'ã®å®Ÿæ–½',
            'ã—ã¾ã™': 'ã®å®Ÿæ–½', 
            'ã—ãŸ': '',
            'ã™ã‚‹': '',
            'ã§ã™': '',
            'ã¾ã™': '',
            'ã¾ã—ãŸ': 'ã®å®Œäº†',
            'ã¦ã„ã¾ã™': 'ä¸­',
            'ã¦ã„ã‚‹': 'ä¸­',
            'ã§ã—ãŸ': '',
            'ã§ã‚ã‚‹': '',
            'ã«ã¤ã„ã¦': '',
            'ã«é–¢ã—ã¦': '',
            'ã«é–¢ã™ã‚‹': ''
        }
        
        # ã¾ãšèªå°¾ã‚’é™¤å»
        cleaned = predicate
        for pattern, replacement in conversions.items():
            if cleaned.endswith(pattern):
                cleaned = cleaned[:-len(pattern)] + replacement
                break
        
        # ç©ºã«ãªã£ãŸå ´åˆã¯å…ƒã®è¿°èªã‚’ä½¿ç”¨
        if not cleaned.strip():
            cleaned = predicate
        
        return cleaned
    
    def _extract_causal_relations(self, content: str) -> List[Dict[str, Any]]:
        """å› æœé–¢ä¿‚ã®æŠ½å‡º"""
        relations = []
        
        for relation_type, patterns in self.relation_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, content)
                for match in matches:
                    if len(match.groups()) >= 2:
                        relations.append({
                            'type': relation_type,
                            'from': match.group(1).strip(),
                            'to': match.group(2).strip() if len(match.groups()) >= 2 else '',
                            'pattern': pattern,
                            'position': match.start()
                        })
        
        return relations
    
    def _generate_contextual_tags(self, analysis_results: Dict[str, Any]) -> List[str]:
        """Ultrathinking: AIã«ã‚ˆã‚‹å‹•çš„å†…å®¹ç†è§£ã«åŸºã¥ãã‚¿ã‚°ç”Ÿæˆ"""
        try:
            # å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ï¼ˆå¾Œæ–¹äº’æ›æ€§ç¢ºä¿ï¼‰
            content = analysis_results.get('original_content', '')
            if not content:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
                return self._generate_fallback_tags(analysis_results)
            
            # AIåˆ¤æ–­ã«ã‚ˆã‚‹æ„å‘³çš„ã‚¿ã‚°ç”Ÿæˆ
            semantic_tags = self._generate_semantic_tags(content)
            
            # æŠ€è¡“å›ºæœ‰åè©ã®æ¤œå‡º
            tech_tags = self._extract_technical_terms(content)
            
            # ä¸€èˆ¬çš„ã™ãã‚‹ã‚¿ã‚°ã®é™¤å¤–
            filtered_tags = self._filter_meaningful_tags(semantic_tags + tech_tags, content)
            
            # ãƒ­ã‚°å‡ºåŠ›
            print(f"ğŸ·ï¸ Ultrathinking ã‚¿ã‚°ç”Ÿæˆ: {filtered_tags[:6]}...")
            
            return filtered_tags[:8]  # ä¸Šä½8å€‹ã«é™å®š
            
        except Exception as e:
            print(f"âš ï¸ ã‚¿ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¾“æ¥æ–¹å¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._generate_fallback_tags(analysis_results)
    
    def _generate_fallback_tags(self, analysis_results: Dict[str, Any]) -> List[str]:
        """å¾“æ¥æ–¹å¼ã®ã‚¿ã‚°ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        tags = []
        tag_scores = defaultdict(float)
        
        # ä¸€èˆ¬çš„ã™ãã‚‹ã‚¿ã‚°ã®ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆæ‹¡å¼µç‰ˆï¼‰
        generic_tags = {
            # åŸºæœ¬çš„ã™ãã‚‹èªå½™
            '#ãƒ¡ãƒ¢', '#è¨˜éŒ²', '#ãƒãƒ¼ãƒˆ', '#æ€è€ƒ', '#å†…å®¹', '#æƒ…å ±', '#ãƒ‡ãƒ¼ã‚¿', 
            '#ã“ã¨', '#ã‚‚ã®', '#ã¨ã', '#ã¨ã“ã‚', '#ãŸã‚', '#ã‚ˆã†', '#æ™‚é–“',
            '#çŠ¶æ³', '#æ–¹æ³•', '#çµæœ', '#å ´åˆ', '#å•é¡Œ', '#ç†ç”±', '#å¿…è¦',
            '#é‡è¦', '#ç¢ºèª', '#é–¢ä¿‚', '#ä¼šç¤¾', '#ä»•äº‹', '#ä»Šå›', '#ä»Šæ—¥',
            '#æœ€è¿‘', '#ç¾åœ¨', '#ä»¥ä¸‹', '#ä»¥ä¸Š', '#ã«ã¤ã„ã¦', '#ä¸€èˆ¬', '#å…¨ä½“',
            '#åˆ†æ', '#ãƒ¬ãƒãƒ¼ãƒˆ', '#TODO', '#ã‚¢ã‚¯ã‚·ãƒ§ãƒ³', '#å› æœé–¢ä¿‚', '#èª²é¡Œ',
            # è¿½åŠ : éåº¦ã«ä¸€èˆ¬çš„ãªèªå½™
            '#è€ƒå¯Ÿ', '#æ¤œè¨', '#å®Ÿè·µ', '#æ´»ç”¨', '#åŠ¹ç‡', '#æ”¹å–„', '#å¯¾ç­–',
            '#éå»', '#å°†æ¥', '#ç®¡ç†', '#ã‚·ã‚¹ãƒ†ãƒ ', '#ãƒ„ãƒ¼ãƒ«', '#æ©Ÿèƒ½',
            '#ä½œæˆ', '#è¨­å®š', '#æ“ä½œ', '#å‡¦ç†', '#å¯¾å¿œ', '#å®Ÿæ–½', '#å°å…¥',
            '#ä½œæ¥­ä¸­', '#ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«', '#è©³ç´°', '#æ¦‚è¦', '#åŸºæœ¬', '#å¿œç”¨'
        }
        
        # ç‰¹å®šæ€§ã®é«˜ã„ã‚¿ã‚°ã®å„ªå…ˆã‚¹ã‚³ã‚¢
        specific_bonus = {
            'proper_nouns': 3.0,    # å›ºæœ‰åè©
            'technical_terms': 2.5, # æŠ€è¡“ç”¨èª
            'domain_specific': 2.0,  # ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–
            'unique_concepts': 1.5   # ç‹¬ç‰¹ã®æ¦‚å¿µ
        }
        
        # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã«åŸºã¥ãã‚¿ã‚°
        doc_type = analysis_results['context_analysis']['document_type']['type']
        doc_confidence = analysis_results['context_analysis']['document_type']['confidence']
        
        if doc_confidence > 0.5:
            doc_type_tags = {
                'report': ['#ãƒ¬ãƒãƒ¼ãƒˆ', '#åˆ†æ'],
                'meeting_notes': ['#ä¼šè­°', '#è­°äº‹éŒ²'],
                'analysis': ['#åˆ†æ', '#è€ƒå¯Ÿ'],
                'plan': ['#è¨ˆç”»', '#æˆ¦ç•¥'],
                'memo': ['#ãƒ¡ãƒ¢', '#å‚™å¿˜éŒ²']
            }
            for tag in doc_type_tags.get(doc_type, []):
                tag_scores[tag] += 3.0 * doc_confidence
        
        # 2. ä¸»é¡Œã«åŸºã¥ãã‚¿ã‚°
        main_topic = analysis_results['context_analysis']['main_topic']
        if main_topic['confidence'] > 0.5:
            # ä¸»é¡Œã‹ã‚‰é‡è¦ãªåè©ã‚’æŠ½å‡º
            topic_nouns = self._extract_nouns_from_text(main_topic['text'])
            for noun in topic_nouns[:3]:
                candidate_tag = f"#{noun}"
                # ä¸€èˆ¬çš„ã™ãã‚‹ã‚¿ã‚°ã¨æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
                if candidate_tag not in generic_tags and len(noun) >= 2:
                    tag_scores[candidate_tag] += 2.5
        
        # 3. ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºã‹ã‚‰ã®ã‚¿ã‚°
        key_phrases = analysis_results['surface_analysis'].get('key_phrases', [])
        for phrase in key_phrases[:5]:
            if 2 <= len(phrase) <= 10:
                candidate_tag = f"#{phrase}"
                # ä¸€èˆ¬çš„ã™ãã‚‹ã‚¿ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
                if candidate_tag not in generic_tags:
                    tag_scores[candidate_tag] += 2.0
        
        # 4. é–¢ä¿‚æ€§ã«åŸºã¥ãã‚¿ã‚°ï¼ˆä¸€èˆ¬çš„ã‚¿ã‚°ã‚’é¿ã‘ã‚‹ï¼‰
        relations = analysis_results['relation_analysis'].get('causal_relations', [])
        if relations:
            # ã€Œå› æœé–¢ä¿‚ã€ã‚„ã€Œåˆ†æã€ã¯ä¸€èˆ¬çš„ã™ãã‚‹ã®ã§é™¤å¤–
            pass
        
        # 5. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã‚¿ã‚°ï¼ˆä¸€èˆ¬çš„ã‚¿ã‚°ã‚’é¿ã‘ã‚‹ï¼‰
        action_items = analysis_results['semantic_integration'].get('action_items', [])
        if action_items:
            # ã€ŒTODOã€ã‚„ã€Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ã¯ä¸€èˆ¬çš„ã™ãã‚‹ã®ã§é™¤å¤–
            pass
        
        # 6. ç‰¹å®šæ€§è©•ä¾¡ã¨ãƒœãƒ¼ãƒŠã‚¹é©ç”¨
        for tag, score in tag_scores.items():
            tag_content = tag[1:] if tag.startswith('#') else tag
            
            # å›ºæœ‰åè©ãƒœãƒ¼ãƒŠã‚¹ï¼ˆè‹±èªã€ã‚«ã‚¿ã‚«ãƒŠã€äººåç­‰ï¼‰
            if re.match(r'^[A-Z][a-zA-Z]+$', tag_content) or re.match(r'^[ã‚¡-ãƒ¶ãƒ¼]{3,}$', tag_content):
                tag_scores[tag] += specific_bonus['proper_nouns']
            
            # æŠ€è¡“ç”¨èªãƒœãƒ¼ãƒŠã‚¹ï¼ˆå¼·åŒ–ç‰ˆï¼‰
            tech_patterns = [
                'AI', 'DX', 'API', 'ChatGPT', 'Claude Code', 'Claude', 'GitHub', 
                'Anthropic', 'OpenAI', 'Python', 'JavaScript', 'TypeScript', 'React',
                'Docker', 'Kubernetes', 'AWS', 'Azure', 'GCP', 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ',
                'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£', 'ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯',
                'ãƒ©ã‚¤ãƒ–ãƒ©ãƒª', 'ãƒªãƒã‚¸ãƒˆãƒª', 'ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³', 'CLI', 'IDE',
                'ãƒ‡ãƒãƒƒã‚°', 'ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°', 'CI/CD', 'DevOps'
            ]
            if any(pattern in tag_content for pattern in tech_patterns):
                tag_scores[tag] += specific_bonus['technical_terms']
            
            # ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒœãƒ¼ãƒŠã‚¹
            domain_patterns = ['å¡¾', 'è¬›å¸«', 'ç‹¬ç«‹', 'æ•™è‚²', 'Consulting', 'EdTech']
            if any(pattern in tag_content for pattern in domain_patterns):
                tag_scores[tag] += specific_bonus['domain_specific']
        
        # 7. æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: ä¸€èˆ¬çš„ã™ãã‚‹ã‚¿ã‚°ã‚’é™¤å¤–
        filtered_tag_scores = {}
        for tag, score in tag_scores.items():
            # ä¸€èˆ¬çš„ã‚¿ã‚°ã¨ä½ã‚¹ã‚³ã‚¢ã‚¿ã‚°ã‚’é™¤å¤–
            if tag not in generic_tags and score >= 2.0:  # é–¾å€¤ã‚’2.0ã«ä¸Šã’ã¦å“è³ªé‡è¦–
                # å˜ä¸€æ–‡å­—ã‚„è¨˜å·ã®ã¿ã®ã‚¿ã‚°ã‚‚é™¤å¤–
                tag_content = tag[1:] if tag.startswith('#') else tag
                if len(tag_content) >= 2 and not tag_content.isdigit():
                    # è¿½åŠ ã®å“è³ªãƒã‚§ãƒƒã‚¯
                    if not self._is_too_generic(tag_content):
                        filtered_tag_scores[tag] = score
        
        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’é¸æŠ
        sorted_tags = sorted(filtered_tag_scores.items(), key=lambda x: x[1], reverse=True)
        unique_tags = []
        seen = set()
        
        for tag, score in sorted_tags:
            if tag not in seen and len(unique_tags) < 6:  # æœ€å¤§6å€‹ã«åˆ¶é™
                unique_tags.append(tag)
                seen.add(tag)
        
        return unique_tags
    
    def _generate_semantic_tags(self, content: str) -> List[str]:
        """Ultrathinking: å†…å®¹ã®æ„å‘³çš„ç†è§£ã«åŸºã¥ãã‚¿ã‚°ç”Ÿæˆ"""
        tags = []
        content_lower = content.lower()
        
        # å›ºæœ‰åè©ãƒ»æŠ€è¡“ç”¨èªã®å„ªå…ˆæŠ½å‡º
        if 'Claude Code' in content:
            tags.append('#Claude Code')
        if 'ChatGPT' in content:
            tags.append('#ChatGPT')
        if 'Anthropic' in content:
            tags.append('#Anthropic')
        if 'OpenAI' in content:
            tags.append('#OpenAI')
        
        # æŠ€è¡“é ˜åŸŸã®ç‰¹å®šçš„ã‚¿ã‚°
        if any(term in content_lower for term in ['ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°', 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ']):
            tags.append('#ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°')
        if any(term in content_lower for term in ['ãƒŠãƒ¬ãƒƒã‚¸ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ', 'çŸ¥è¦‹ç®¡ç†', 'çŸ¥è­˜è“„ç©']):
            tags.append('#ãƒŠãƒ¬ãƒƒã‚¸ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ')
        if any(term in content_lower for term in ['é–‹ç™ºæ‰‹æ³•', 'é–‹ç™ºãƒ—ãƒ­ã‚»ã‚¹']):
            tags.append('#é–‹ç™ºæ‰‹æ³•')
        
        # ãƒ“ã‚¸ãƒã‚¹é ˜åŸŸã®ç‰¹å®šçš„ã‚¿ã‚°
        if any(term in content_lower for term in ['snsãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'snsæˆ¦ç•¥', 'ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢']):
            tags.append('#SNSãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°')
        if any(term in content_lower for term in ['ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ç²å¾—', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæˆ¦ç•¥']):
            tags.append('#SNSæˆ¦ç•¥')
        if any(term in content_lower for term in ['æ•™è‚²ãƒ“ã‚¸ãƒã‚¹', 'å¡¾çµŒå–¶', 'è¬›å¸«ãƒ“ã‚¸ãƒã‚¹']):
            tags.append('#æ•™è‚²ãƒ“ã‚¸ãƒã‚¹')
        
        # å­¦ç¿’ãƒ»æ•™è‚²é ˜åŸŸã®ç‰¹å®šçš„ã‚¿ã‚°
        if any(term in content_lower for term in ['æ•™è‚²dx', 'å­¦ç¿’å¡¾dx', 'edtech']):
            tags.append('#æ•™è‚²DX')
        if any(term in content_lower for term in ['è¬›å¸«ç‹¬ç«‹', 'æ•™è‚²èµ·æ¥­', 'å¡¾è¬›å¸«ç‹¬ç«‹']):
            tags.append('#è¬›å¸«ç‹¬ç«‹')
        if any(term in content_lower for term in ['å­¦ç¿’æ”¯æ´', 'å€‹åˆ¥æŒ‡å°', 'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æˆæ¥­']):
            tags.append('#å­¦ç¿’æ”¯æ´')
        
        # ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°é ˜åŸŸã®ç‰¹å®šçš„ã‚¿ã‚°
        if any(term in content_lower for term in ['æˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°', 'ãƒ“ã‚¸ãƒã‚¹æˆ¦ç•¥']):
            tags.append('#æˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°')
        if any(term in content_lower for term in ['æ¥­å‹™æ”¹å–„', 'ãƒ—ãƒ­ã‚»ã‚¹æ”¹å–„']):
            tags.append('#æ¥­å‹™æ”¹å–„')
        
        # æƒ…å ±ç®¡ç†ãƒ»æ€è€ƒæ³•é ˜åŸŸã®ç‰¹å®šçš„ã‚¿ã‚°
        if any(term in content_lower for term in ['æ‰‹æ›¸ã', 'æ‰‹æ›¸ãã®æœ‰ç”¨æ€§', 'æ‰‹æ›¸ããƒ¡ãƒ¢']):
            tags.append('#æ‰‹æ›¸ã')
        if any(term in content_lower for term in ['ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–', 'ãƒ‡ã‚¸ã‚¿ãƒ«ç®¡ç†', 'ã‚¢ãƒŠãƒ­ã‚°ã¨ãƒ‡ã‚¸ã‚¿ãƒ«']):
            tags.append('#ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–')
        if any(term in content_lower for term in ['æƒ…å ±æ•´ç†', 'æƒ…å ±ç®¡ç†', 'æ•´ç†æ‰‹æ³•']):
            tags.append('#æƒ…å ±æ•´ç†æ‰‹æ³•')
        if any(term in content_lower for term in ['ã‚¢ãƒŠãƒ­ã‚°', 'ã‚¢ãƒŠãƒ­ã‚°æ€è€ƒ', 'æ‰‹æ›¸ãæ€è€ƒ']):
            tags.append('#ã‚¢ãƒŠãƒ­ã‚°æ€è€ƒ')
        if any(term in content_lower for term in ['æ€è€ƒæ•´ç†', 'ç™ºæƒ³æ³•', 'æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹']):
            tags.append('#æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹')
        
        return tags[:4]  # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¿ã‚°ã¯æœ€å¤§4å€‹
    
    def _extract_technical_terms(self, content: str) -> List[str]:
        """Ultrathinking: æŠ€è¡“å›ºæœ‰åè©ã®æŠ½å‡º"""
        tags = []
        
        # æŠ€è¡“ãƒ„ãƒ¼ãƒ«ãƒ»ã‚µãƒ¼ãƒ“ã‚¹
        tech_terms = {
            'Claude Code': '#Claude Code',
            'Claude': '#Claude', 
            'ChatGPT': '#ChatGPT',
            'GitHub': '#GitHub',
            'Obsidian': '#Obsidian',
            'Anthropic': '#Anthropic',
            'OpenAI': '#OpenAI',
            'Python': '#Python',
            'JavaScript': '#JavaScript',
            'TypeScript': '#TypeScript',
            'React': '#React',
            'Docker': '#Docker',
            'Kubernetes': '#Kubernetes'
        }
        
        for term, tag in tech_terms.items():
            if term in content:
                tags.append(tag)
        
        # æŠ€è¡“æ¦‚å¿µ
        tech_concepts = {
            'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°': '#ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°',
            'AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ': '#AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ',
            'ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³': '#CLI',
            'ãƒ‡ãƒãƒƒã‚°': '#ãƒ‡ãƒãƒƒã‚°',
            'ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°': '#ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°',
            'ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£': '#ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£'
        }
        
        for concept, tag in tech_concepts.items():
            if concept in content:
                tags.append(tag)
        
        return tags[:3]  # æŠ€è¡“ç”¨èªã‚¿ã‚°ã¯æœ€å¤§3å€‹
    
    def _filter_meaningful_tags(self, candidate_tags: List[str], content: str) -> List[str]:
        """Ultrathinking: æ„å‘³ã®ã‚ã‚‹ã‚¿ã‚°ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        
        # åŒ…æ‹¬çš„ãªä¸€èˆ¬çš„ã‚¿ã‚°ãƒ»æ–­ç‰‡ã‚¿ã‚°ã®é™¤å¤–ãƒªã‚¹ãƒˆ
        generic_blacklist = {
            # åŸºæœ¬çš„ã™ãã‚‹èªå½™ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡æ‘˜ã®ã€Œé‡è¦ã€ã€Œã‚¢ã‚¤ãƒ‡ã‚¢ã€ã€Œè¨˜éŒ²ã€ã‚’å¼·åŒ–ï¼‰
            '#ãƒ¡ãƒ¢', '#è¨˜éŒ²', '#ãƒãƒ¼ãƒˆ', '#æ€è€ƒ', '#å†…å®¹', '#æƒ…å ±', '#ãƒ‡ãƒ¼ã‚¿',
            '#åˆ†æ', '#ãƒ¬ãƒãƒ¼ãƒˆ', '#TODO', '#ã‚¢ã‚¯ã‚·ãƒ§ãƒ³', '#å› æœé–¢ä¿‚', '#èª²é¡Œ',
            '#è€ƒå¯Ÿ', '#æ¤œè¨', '#å®Ÿè·µ', '#æ´»ç”¨', '#åŠ¹ç‡', '#æ”¹å–„', '#å¯¾ç­–',
            '#ç®¡ç†', '#ã‚·ã‚¹ãƒ†ãƒ ', '#ãƒ„ãƒ¼ãƒ«', '#æ©Ÿèƒ½', '#å‡¦ç†', '#å¯¾å¿œ', '#å®Ÿè¡Œ',
            '#ä½œæˆ', '#è¨­å®š', '#æ“ä½œ', '#ç¢ºèª', '#é‡è¦', '#è©•ä¾¡', '#å“è³ª',
            '#ã‚¢ã‚¤ãƒ‡ã‚¢', '#ç™ºæƒ³', '#æ€è€ƒ', '#æ„Ÿæƒ…', '#å°è±¡', '#é•å’Œæ„Ÿ', '#ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹',
            '#ä¸€èˆ¬', '#æ™®é', '#åŸºæœ¬', '#å˜èª', '#æ¦‚å¿µ', '#è¦ç´ ', '#é …ç›®',
            
            # æ–‡è„ˆæ–­ç‰‡ã‚¿ã‚°ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡æ‘˜äº‹é …ï¼‰
            '#éå»ã®è©¦è¡ŒéŒ¯èª¤', '#ã§åŠ¹ç‡çš„ã«é–‹ç™ºã™ã‚‹ãŸ', '#ã‚ã®çŸ¥è¦‹ç®¡ç†',
            '#ä½œæ¥­ä¸­ã®ä¸€æ™‚ãƒ•ã‚¡', '#ã®èƒŒæ™¯', '#ãƒãƒ¼ãƒ é€£æºã®æ”¹å–„',
            '#æŠ€è¡“é¸å®šç†ç”±', '#å“è³ªã®å‘ä¸Š',
            
            # æ™‚é–“ãƒ»çŠ¶æ³ã®ä¸€èˆ¬èª
            '#ä»Šå›', '#ä»Šæ—¥', '#æœ€è¿‘', '#ç¾åœ¨', '#ä»¥ä¸‹', '#ä»¥ä¸Š', '#ã«ã¤ã„ã¦',
            '#ä¸€èˆ¬', '#å…¨ä½“', '#å ´åˆ', '#æ™‚é–“', '#çŠ¶æ³', '#æ–¹æ³•', '#çµæœ',
            
            # å‹•ä½œã®ä¸€èˆ¬èª
            '#ã™ã‚‹', '#ã—ãŸ', '#ãªã‚‹', '#ã‚ã‚‹', '#ã„ã‚‹', '#ä½¿ã†', '#è¦‹ã‚‹'
        }
        
        # é‡è¤‡é™¤å»ã¨å“è³ªãƒã‚§ãƒƒã‚¯
        filtered_tags = []
        seen_tags = set()
        
        for tag in candidate_tags:
            if tag and tag not in seen_tags and tag not in generic_blacklist:
                # æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆçŸ­ã™ãã‚‹ãƒ»é•·ã™ãã‚‹ã‚¿ã‚°ã‚’é™¤å¤–ï¼‰
                tag_content = tag[1:] if tag.startswith('#') else tag
                if 2 <= len(tag_content) <= 20:  # ç¯„å›²ã‚’æ‹¡å¼µ
                    # æ–­ç‰‡ãƒ»ä¸€èˆ¬èªãƒã‚§ãƒƒã‚¯
                    if not self._is_meaningless_fragment(tag_content):
                        filtered_tags.append(tag)
                        seen_tags.add(tag)
        
        return filtered_tags[:6]  # æœ€çµ‚çš„ã«æœ€å¤§6å€‹ã¾ã§
    
    def _is_too_generic(self, tag_content: str) -> bool:
        """ã‚¿ã‚°ãŒä¸€èˆ¬çš„ã™ãã‚‹ã‹ã‚’åˆ¤å®š"""
        # ä¸€èˆ¬çš„ã™ãã‚‹å‹•è©
        generic_verbs = ['ã™ã‚‹', 'ã—ãŸ', 'ãªã‚‹', 'ã‚ã‚‹', 'ã„ã‚‹', 'ä½¿ã†', 'è¦‹ã‚‹', 'æ¥ã‚‹']
        
        # ä¸€èˆ¬çš„ã™ãã‚‹åè©
        generic_nouns = ['äº‹', 'ç‰©', 'äºº', 'æ‰€', 'æ™‚', 'è©±', 'äº‹é …', 'å†…å®¹', 'çŠ¶æ…‹', 'çŠ¶æ³']
        
        # çŸ­ã™ãã‚‹ã€ã¾ãŸã¯ä¸€èˆ¬çš„ã™ãã‚‹å ´åˆã¯é™¤å¤–
        if len(tag_content) <= 1:
            return True
        
        if tag_content in generic_verbs or tag_content in generic_nouns:
            return True
        
        # ã²ã‚‰ãŒãªã®ã¿ã§3æ–‡å­—ä»¥ä¸‹ã¯é™¤å¤–
        if re.match(r'^[ã-ã‚“]{1,3}$', tag_content):
            return True
        
        # æ•°å­—ã®ã¿ã€è¨˜å·ã®ã¿ã¯é™¤å¤–
        if tag_content.isdigit() or not re.search(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯A-Za-z]', tag_content):
            return True
        
        return False
    
    def _is_meaningless_fragment(self, tag_content: str) -> bool:
        """æ„å‘³ã®ãªã„æ–­ç‰‡ã‚¿ã‚°ã‹ã‚’åˆ¤å®š"""
        # éƒ¨åˆ†çš„ãªèªå¥ã‚„åŠ©è©ãŒå«ã¾ã‚Œã¦ã„ã‚‹æ–­ç‰‡
        fragment_patterns = [
            r'^(ã§|ã®|ã«|ãŒ|ã‚’|ã¯|ã¨|ã‹ã‚‰|ã¾ã§|ã‚ˆã‚Š|ãªã©).*',  # åŠ©è©ã§å§‹ã¾ã‚‹
            r'.*(ã§|ã®|ã«|ãŒ|ã‚’|ã¯|ã¨|ã‹ã‚‰|ã¾ã§|ã‚ˆã‚Š)$',     # åŠ©è©ã§çµ‚ã‚ã‚‹
            r'^(ãŸ|ã¦|ã |ã§|ã™ã‚‹|ã—ãŸ|ãªã‚‹|ã‚ã‚‹)$',          # å‹•è©æ´»ç”¨ã®ã¿
            r'^[ã-ã‚“]{1,2}$',                           # ã²ã‚‰ãŒãª1-2æ–‡å­—
            r'^(ä¸€æ™‚|ä½œæ¥­ä¸­|èƒŒæ™¯|ç†ç”±|å‘ä¸Š|æ”¹å–„)$'            # ä¸€èˆ¬çš„ã™ãã‚‹å˜èª
        ]
        
        for pattern in fragment_patterns:
            if re.match(pattern, tag_content):
                return True
        
        # æ„å‘³ä¸æ˜ãªçŸ­ç¸®å½¢
        if len(tag_content) <= 2 and not re.match(r'^[A-Z]{2,}$|^[ã‚¡-ãƒ¶ãƒ¼]{2,}$', tag_content):
            return True
        
        return False
    
    def _extract_proper_nouns(self, content: str) -> List[str]:
        """å›ºæœ‰åè©ã®æŠ½å‡ºï¼ˆæŠ€è¡“ç”¨èªãƒ»ã‚µãƒ¼ãƒ“ã‚¹åå„ªå…ˆï¼‰"""
        proper_nouns = []
        
        # æŠ€è¡“ã‚µãƒ¼ãƒ“ã‚¹ãƒ»ãƒ„ãƒ¼ãƒ«åï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
        tech_services = [
            'Claude Code', 'ChatGPT', 'GitHub Copilot', 'Anthropic', 'OpenAI',
            'Google Bard', 'Microsoft Copilot', 'Notion AI', 'Cursor', 'Replit',
            'Vercel', 'Netlify', 'Supabase', 'Firebase', 'AWS', 'Azure', 'GCP'
        ]
        
        for service in tech_services:
            if service in content:
                proper_nouns.append(service)
        
        # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªãƒ»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
        tech_terms = [
            'Python', 'JavaScript', 'TypeScript', 'React', 'Vue.js', 'Angular',
            'Node.js', 'Express', 'Django', 'Flask', 'Spring Boot', 'Laravel',
            'Docker', 'Kubernetes', 'PostgreSQL', 'MongoDB', 'Redis'
        ]
        
        for term in tech_terms:
            if term in content:
                proper_nouns.append(term)
        
        # ã‚«ã‚¿ã‚«ãƒŠå›ºæœ‰åè©ï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰
        katakana_pattern = r'[ã‚¡-ãƒ¶ãƒ¼]{3,}(?:[ã‚¡-ãƒ¶ãƒ¼\s]*[ã‚¡-ãƒ¶ãƒ¼]{1,})*'
        katakana_matches = re.findall(katakana_pattern, content)
        for match in katakana_matches:
            clean_match = match.strip()
            if len(clean_match) >= 3:
                proper_nouns.append(clean_match)
        
        # è‹±èªå›ºæœ‰åè©ï¼ˆå¤§æ–‡å­—é–‹å§‹ã€2æ–‡å­—ä»¥ä¸Šï¼‰
        english_pattern = r'\b[A-Z][a-zA-Z]{1,}(?:\s+[A-Z][a-zA-Z]+)*\b'
        english_matches = re.findall(english_pattern, content)
        for match in english_matches:
            if len(match) >= 2 and not match.lower() in ['the', 'and', 'for', 'with']:
                proper_nouns.append(match)
        
        # é‡è¤‡é™¤å»ã¨å„ªå…ˆåº¦é †ã‚½ãƒ¼ãƒˆ
        unique_nouns = []
        seen = set()
        
        # æŠ€è¡“ã‚µãƒ¼ãƒ“ã‚¹å„ªå…ˆ
        for noun in proper_nouns:
            if noun not in seen and noun in tech_services:
                unique_nouns.append(noun)
                seen.add(noun)
        
        # ãã®ä»–ã®å›ºæœ‰åè©
        for noun in proper_nouns:
            if noun not in seen:
                unique_nouns.append(noun)
                seen.add(noun)
        
        return unique_nouns[:3]  # ä¸Šä½3ã¤ã¾ã§
    
    def _extract_meaningful_title(self, content: str, proper_nouns: List[str]) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä¸­æ ¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨é–¢é€£èªã§å®Œçµã—ãŸæ„å‘³ã®ã‚ã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        content_lower = content.lower()
        
        # ä¸­æ ¸ã¨ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç‰¹å®š
        core_keywords = self._identify_core_keywords(content, proper_nouns)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ç›®çš„ãƒ»æ–¹æ³•è«–ã‚’ç¤ºã™ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆå®Œçµã—ãŸæ–‡ã¨ã—ã¦ï¼‰
        purpose_patterns = [
            r'(.{10,50}?)ã®ãŸã‚ã®(.{5,30}?)(?:æ–¹æ³•|æ‰‹æ³•|ã‚·ã‚¹ãƒ†ãƒ |ãƒ„ãƒ¼ãƒ«|ã‚¬ã‚¤ãƒ‰|ã‚³ãƒ„)',
            r'(.{5,30}?)ã§(.{5,30}?)(?:ã‚’å®Ÿç¾|ã‚’åŠ¹ç‡åŒ–|ã‚’ç®¡ç†|ã‚’æ´»ç”¨)(?:ã™ã‚‹|ã•ã›ã‚‹)?',
            r'(.{5,30}?)ã«ã‚ˆã‚‹(.{5,30}?)(?:ã®|ã‚’)(.{5,30}?)(?:ç®¡ç†|é‹ç”¨|æ´»ç”¨|æ§‹ç¯‰)',
        ]
        
        for pattern in purpose_patterns:
            matches = re.findall(pattern, content, re.MULTILINE)
            for match in matches:
                if len(match) >= 2:
                    parts = [part.strip() for part in match if part.strip()]
                    # å›ºæœ‰åè©ã‚’å«ã‚€å ´åˆã¯å„ªå…ˆ
                    if any(any(noun in part for noun in proper_nouns) for part in parts):
                        if len(parts) == 2:
                            return f"{parts[0]}ã«ã‚ˆã‚‹{parts[1]}"
                        elif len(parts) >= 3:
                            return f"{parts[0]}ã«ã‚ˆã‚‹{parts[1]}ã®{parts[2]}"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: å®Œçµã—ãŸèª¬æ˜æ–‡æ§‹é€ 
        explanation_patterns = [
            r'(.{5,40}?)(?:ã«ã¤ã„ã¦|ã«é–¢ã™ã‚‹|ã«ãŠã‘ã‚‹)(.{5,30}?)(?:ã®|ã‚’)(.{5,30}?)(?:æ–¹æ³•|æ‰‹æ³•|ã‚¬ã‚¤ãƒ‰|ã‚·ã‚¹ãƒ†ãƒ )',
            r'(.{5,40}?)(?:ã§ã®|ã«ã‚ˆã‚‹|ã‚’ä½¿ã£ãŸ)(.{5,30}?)(?:ã®|ã‚’)(.{5,30}?)(?:å®Ÿè·µ|æ´»ç”¨|é‹ç”¨|ç®¡ç†)',
        ]
        
        for pattern in explanation_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                parts = [part.strip() for part in match if part.strip()]
                # å›ºæœ‰åè©ã‚’å«ã‚€å ´åˆã¯å„ªå…ˆ
                if any(any(noun in part for noun in proper_nouns) for part in parts):
                    if len(parts) >= 3:
                        return f"{parts[0]}ã«ã‚ˆã‚‹{parts[1]}ã®{parts[2]}"
                    elif len(parts) == 2:
                        return f"{parts[0]}ã§ã®{parts[1]}"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: è¨˜äº‹æ§‹é€ ã‹ã‚‰ä¸­æ ¸å†…å®¹ã‚’æŠ½å‡ºï¼ˆã€Œæœ¬è¨˜äº‹ã§ã¯ã€ã‚’é™¤å»ï¼‰
        if any(phrase in content for phrase in ['æœ¬è¨˜äº‹ã§ã¯', 'æœ¬ç¨¿ã§ã¯', 'ã“ã®è¨˜äº‹ã§ã¯']):
            # è¨˜äº‹å°å…¥éƒ¨ã‹ã‚‰å®Ÿè³ªçš„ãªå†…å®¹ã‚’æŠ½å‡º
            content_extraction_patterns = [
                r'(?:æœ¬è¨˜äº‹|æœ¬ç¨¿|ã“ã®è¨˜äº‹)ã§ã¯[ã€ï¼Œ]?(.{5,40}?)(?:ã‚’ä½¿ã£ãŸ|ã«ã‚ˆã‚‹|ã§ã®)(.{5,30}?)(?:ã®|ã‚’)(.{5,30}?)(?:ã‚’|ã«ã¤ã„ã¦)?(.{5,30}?)(?:ç´¹ä»‹|èª¬æ˜|è§£èª¬)',
                r'(?:æœ¬è¨˜äº‹|æœ¬ç¨¿|ã“ã®è¨˜äº‹)ã§ã¯[ã€ï¼Œ]?(.{5,40}?)(?:ã®|ã«ã‚ˆã‚‹)(.{5,30}?)(?:ã‚’|ã«ã¤ã„ã¦)(.{5,30}?)(?:ç´¹ä»‹|èª¬æ˜|è§£èª¬)',
            ]
            
            for pattern in content_extraction_patterns:
                match = re.search(pattern, content)
                if match:
                    groups = [g.strip() for g in match.groups() if g and g.strip()]
                    # å›ºæœ‰åè©ã‚’å«ã‚€å ´åˆã¯å„ªå…ˆ
                    if any(any(noun in group for noun in proper_nouns) for group in groups):
                        if len(groups) >= 3:
                            # å®Œçµã—ãŸã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦æ§‹æˆ
                            return f"{groups[0]}ã«ã‚ˆã‚‹{groups[1]}ã®{groups[2]}"
                        elif len(groups) == 2:
                            return f"{groups[0]}ã§ã®{groups[1]}"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: èª²é¡Œè§£æ±ºå‹ã‚¿ã‚¤ãƒˆãƒ«
        problem_solution_patterns = [
            r'(.{5,40}?)(?:ã®|ã‚’)(?:èª²é¡Œ|å•é¡Œ)(?:ã‚’|ã«ã¤ã„ã¦)(.{5,30}?)(?:è§£æ±º|å¯¾å¿œ|æ”¹å–„)(?:ã™ã‚‹|ã•ã›ã‚‹)?',
            r'(.{5,40}?)(?:ä½¿ã„å§‹ã‚ã‚‹ã¨|ã‚’å°å…¥ã™ã‚‹ã¨)[ã€ï¼Œ]?(.{5,30}?)(?:èª²é¡Œ|å•é¡Œ)(?:ã«|ã¨)(.{5,30}?)',
        ]
        
        for pattern in problem_solution_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                parts = [part.strip() for part in match if part.strip()]
                if any(any(noun in part for noun in proper_nouns) for part in parts):
                    if len(parts) >= 2:
                        return f"{parts[0]}ã«ãŠã‘ã‚‹{parts[1]}ã®è§£æ±ºæ–¹æ³•"
        
        return ''
    
    def _identify_core_keywords(self, content: str, proper_nouns: List[str]) -> List[str]:
        """Ultrathinking: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä¸­æ ¸ã¨ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç‰¹å®š"""
        core_keywords = []
        content_lower = content.lower()
        
        # å›ºæœ‰åè©ã¯æœ€å„ªå…ˆ
        core_keywords.extend(proper_nouns[:2])
        
        # å‹•ä½œãƒ»ç›®çš„èªã®æŠ½å‡º
        action_patterns = [
            r'(?:ã‚’|ã«ã¤ã„ã¦)(.{3,15}?)(?:ã™ã‚‹|ã•ã›ã‚‹|å®Ÿç¾|ç®¡ç†|é‹ç”¨|æ´»ç”¨|æ§‹ç¯‰|é–‹ç™º)',
            r'(.{3,15}?)(?:ã®|ã‚’)(?:ç®¡ç†|é‹ç”¨|æ´»ç”¨|æ§‹ç¯‰|é–‹ç™º|æ”¹å–„|è§£æ±º)',
            r'(?:åŠ¹ç‡çš„ã«|ä¸Šæ‰‹ã|é©åˆ‡ã«)(.{3,15}?)(?:ã™ã‚‹|æ´»ç”¨|é‹ç”¨)',
        ]
        
        for pattern in action_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                keyword = match.strip()
                if len(keyword) >= 3 and keyword not in core_keywords:
                    core_keywords.append(keyword)
        
        # æ¦‚å¿µãƒ»æ‰‹æ³•ã®æŠ½å‡º
        concept_patterns = [
            r'(.{3,15}?)(?:ã‚·ã‚¹ãƒ†ãƒ |æ‰‹æ³•|æ–¹æ³•|ã‚¬ã‚¤ãƒ‰|ã‚³ãƒ„|æˆ¦ç•¥|ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ)',
            r'(.{3,15}?)(?:ã«ã¤ã„ã¦|ã«é–¢ã™ã‚‹|ã«ãŠã‘ã‚‹)(?:èª²é¡Œ|å•é¡Œ|è§£æ±º)',
        ]
        
        for pattern in concept_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                keyword = match.strip()
                if len(keyword) >= 3 and keyword not in core_keywords:
                    core_keywords.append(keyword)
        
        return core_keywords[:4]
    
    def _is_fragmented_title(self, title: str) -> bool:
        """ã‚¿ã‚¤ãƒˆãƒ«ãŒæ–­ç‰‡åŒ–ãƒ»ä¸å®Œå…¨ãƒ»ä¸é©åˆ‡ã‹ãƒã‚§ãƒƒã‚¯"""
        if not title or not title.strip():
            return True
            
        title_clean = title.strip()
        
        # åŸºæœ¬çš„ãªæ–­ç‰‡åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
        fragment_indicators = [
            r'^[ã€ï¼Œã€‚ï¼]',  # å¥èª­ç‚¹ã§å§‹ã¾ã‚‹
            r'[ã€ï¼Œã€‚ï¼]$',  # å¥èª­ç‚¹ã§çµ‚ã‚ã‚‹
            r'^[ã-ã‚“]{1,2}$',  # ã²ã‚‰ãŒãª1-2æ–‡å­—ã®ã¿
            r'^[ã€ï¼Œ].*',  # ã‚«ãƒ³ãƒã§å§‹ã¾ã‚‹
            r'.*[ã€ï¼Œ]{2,}.*',  # é€£ç¶šã‚«ãƒ³ãƒ
            r'.*ãƒ»$',  # ä¸­ç‚¹ã§çµ‚ã‚ã‚‹ï¼ˆä¸å®Œå…¨ï¼‰
        ]
        
        for pattern in fragment_indicators:
            if re.match(pattern, title_clean):
                return True
        
        # ä¸é©åˆ‡ãªæ¥é ­èª
        inappropriate_prefixes = [
            r'^æœ¬è¨˜äº‹ã§ã¯[ã€ï¼Œ]?',
            r'^ã“ã®è¨˜äº‹ã§ã¯[ã€ï¼Œ]?',
            r'^æœ¬ç¨¿ã§ã¯[ã€ï¼Œ]?',
            r'^ä»¥ä¸‹[ã€ï¼Œ]?',
        ]
        
        for pattern in inappropriate_prefixes:
            if re.match(pattern, title_clean):
                return True
        
        # æ–‡ã¨ã—ã¦ä¸å®Œå…¨ï¼ˆå‹•è©ãªã—ã€åŠ©è©ã§çµ‚ã‚ã‚‹ç­‰ï¼‰
        incomplete_patterns = [
            r'.*[ã®ã‚’ã«ãŒã¯ã§]$',  # åŠ©è©ã§çµ‚ã‚ã‚‹
            r'.*ã«ã¤ã„ã¦$',  # ã€Œã«ã¤ã„ã¦ã€ã§çµ‚ã‚ã‚‹ï¼ˆä¸å®Œå…¨ï¼‰
            r'.*ã«é–¢ã™ã‚‹$',  # ã€Œã«é–¢ã™ã‚‹ã€ã§çµ‚ã‚ã‚‹ï¼ˆä¸å®Œå…¨ï¼‰
            r'.*ãƒ»$',  # ä¸­ç‚¹ã§çµ‚ã‚ã‚‹
        ]
        
        for pattern in incomplete_patterns:
            if re.match(pattern, title_clean):
                return True
        
        # çŸ­ã™ãã‚‹ã€ã¾ãŸã¯é•·ã™ãã‚‹
        if len(title_clean) <= 3 or len(title_clean) > 60:
            return True
            
        return False
    
    def _generate_title_from_content_meaning(self, content: str, main_noun: str) -> str:
        """Ultrathinking + ãƒ©ãƒ†ãƒ©ãƒ«ã‚·ãƒ³ã‚­ãƒ³ã‚°: æ™®éçš„ãªè¦–ç‚¹ã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        content_lower = content.lower()
        
        # ãƒ©ãƒ†ãƒ©ãƒ«ã‚·ãƒ³ã‚­ãƒ³ã‚°: å•é¡Œã‚’ä¸€èˆ¬åŒ–
        universal_patterns = self._identify_universal_patterns(content_lower)
        core_keywords = self._identify_core_keywords(content, [main_noun])
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: èª²é¡Œè§£æ±ºå‹ï¼ˆæ™®éçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
        if any(word in content_lower for word in ['èª²é¡Œ', 'å•é¡Œ', 'è§£æ±º', 'æ”¹å–„', 'å¯¾ç­–']):
            if len(core_keywords) >= 2:
                # æ–‡æ³•ãƒã‚§ãƒƒã‚¯: åŠ©è©é‡è¤‡ã‚’é˜²ã
                clean_keyword = self._clean_grammatical_particles(core_keywords[1])
                return f"{main_noun}ã«ã‚ˆã‚‹{clean_keyword}èª²é¡Œã®è§£æ±ºæ‰‹æ³•"
            else:
                return f"{main_noun}ã‚’æ´»ç”¨ã—ãŸèª²é¡Œè§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: çŸ¥è¦‹ãƒ»ãƒŠãƒ¬ãƒƒã‚¸ç³»ï¼ˆæ™®éçš„çŸ¥è­˜ç®¡ç†ï¼‰
        if any(word in content_lower for word in ['çŸ¥è¦‹', 'ãƒŠãƒ¬ãƒƒã‚¸', 'ç®¡ç†', 'è“„ç©', 'æ´»ç”¨']):
            action_words = self._extract_action_concepts(content_lower)
            if action_words and 'é‹ç”¨' in action_words:
                return f"{main_noun}ã«ã‚ˆã‚‹åŠ¹æœçš„ãªé‹ç”¨ç®¡ç†æ‰‹æ³•"
            elif action_words:
                return f"{main_noun}ã§ã®å®Ÿè·µçš„ãª{action_words[0]}ã‚·ã‚¹ãƒ†ãƒ "
            else:
                return f"{main_noun}ã«ã‚ˆã‚‹çŸ¥è¦‹æ´»ç”¨ã®å®Ÿè·µæ–¹æ³•"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: é–‹ç™ºãƒ»å®Ÿè£…ç³»ï¼ˆæ™®éçš„æ‰‹æ³•è«–ï¼‰
        if any(word in content_lower for word in ['é–‹ç™º', 'å®Ÿè£…', 'æ§‹ç¯‰', 'è¨­è¨ˆ']):
            if 'åŠ¹ç‡' in content_lower:
                return f"{main_noun}ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªé–‹ç™ºæ‰‹æ³•"
            else:
                return f"{main_noun}ã‚’ä½¿ã£ãŸå®Ÿè·µçš„ãªé–‹ç™ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: åˆ†æãƒ»è©•ä¾¡ç³»ï¼ˆæ™®éçš„åˆ†ææ‰‹æ³•ï¼‰
        if any(word in content_lower for word in ['åˆ†æ', 'ãƒ¬ãƒãƒ¼ãƒˆ', 'ãƒ‡ãƒ¼ã‚¿', 'è§£æ', 'è©•ä¾¡']):
            return f"{main_noun}ã«ã‚ˆã‚‹åŒ…æ‹¬çš„åˆ†ææ‰‹æ³•"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³5: æˆ¦ç•¥ãƒ»è¨ˆç”»ç³»ï¼ˆæ™®éçš„æˆ¦ç•¥è«–ï¼‰
        if any(word in content_lower for word in ['æˆ¦ç•¥', 'è¨ˆç”»', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'ãƒ“ã‚¸ãƒã‚¹']):
            return f"{main_noun}ã‚’æ´»ç”¨ã—ãŸæˆ¦ç•¥çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³6: å­¦ç¿’ãƒ»æ•™è‚²ç³»ï¼ˆæ™®éçš„å­¦ç¿’è«–ï¼‰
        if any(word in content_lower for word in ['å­¦ç¿’', 'æ•™è‚²', 'æŒ‡å°', 'ç ”ä¿®']):
            return f"{main_noun}ã«ã‚ˆã‚‹åŠ¹æœçš„ãªå­¦ç¿’æ‰‹æ³•"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³7: æ™®éçš„æ´»ç”¨è«–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        if core_keywords and len(core_keywords) >= 2:
            # æ–‡æ³•ãƒã‚§ãƒƒã‚¯é©ç”¨
            clean_keyword = self._clean_grammatical_particles(core_keywords[1])
            return f"{main_noun}ã«ã‚ˆã‚‹{clean_keyword}ã®å®Ÿè·µæ´»ç”¨æ³•"
        else:
            return f"{main_noun}ã®åŠ¹æœçš„ãªæ´»ç”¨æ‰‹æ³•"
    
    def _identify_universal_patterns(self, content_lower: str) -> List[str]:
        """ãƒ©ãƒ†ãƒ©ãƒ«ã‚·ãƒ³ã‚­ãƒ³ã‚°: æ™®éçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š"""
        patterns = []
        
        # æ™®éçš„ãªæ¦‚å¿µã‚«ãƒ†ã‚´ãƒª
        universal_concepts = {
            'problem_solving': ['èª²é¡Œ', 'å•é¡Œ', 'è§£æ±º', 'æ”¹å–„', 'å¯¾ç­–', 'ä¿®æ­£'],
            'knowledge_management': ['çŸ¥è¦‹', 'ãƒŠãƒ¬ãƒƒã‚¸', 'è“„ç©', 'ç®¡ç†', 'æ´»ç”¨', 'å…±æœ‰'],
            'efficiency': ['åŠ¹ç‡', 'æœ€é©åŒ–', 'æ”¹å–„', 'å‘ä¸Š', 'ã‚¹ãƒ”ãƒ¼ãƒ‰', 'ç”Ÿç”£æ€§'],
            'methodology': ['æ‰‹æ³•', 'æ–¹æ³•', 'ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ', 'æˆ¦ç•¥', 'ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯'],
            'learning': ['å­¦ç¿’', 'ç¿’å¾—', 'ç†è§£', 'æŠŠæ¡', 'èº«ã«ã¤ã‘ã‚‹'],
            'implementation': ['å®Ÿè£…', 'å®Ÿè·µ', 'å°å…¥', 'æ§‹ç¯‰', 'é–‹ç™º', 'é©ç”¨']
        }
        
        for pattern_type, keywords in universal_concepts.items():
            if any(keyword in content_lower for keyword in keywords):
                patterns.append(pattern_type)
        
        return patterns
    
    def _extract_action_concepts(self, content_lower: str) -> List[str]:
        """è¡Œå‹•ãƒ»å‹•ä½œã«é–¢ã™ã‚‹æ¦‚å¿µã‚’æŠ½å‡º"""
        action_concepts = []
        
        # å‹•ä½œã‚’è¡¨ã™èªå½™
        action_words = ['é‹ç”¨', 'ç®¡ç†', 'æ´»ç”¨', 'æ§‹ç¯‰', 'é–‹ç™º', 'æ”¹å–„', 'è§£æ±º', 'å®Ÿè·µ', 'å°å…¥']
        
        for word in action_words:
            if word in content_lower:
                action_concepts.append(word)
        
        return action_concepts[:2]  # ä¸Šä½2ã¤ã¾ã§
    
    def _clean_grammatical_particles(self, text: str) -> str:
        """åŒ…æ‹¬çš„æ–‡æ³•ãƒã‚§ãƒƒã‚¯: åŠ©è©ã®é‡è¤‡ã‚„ãŠã‹ã—ãªæ–‡æ³•ã‚’ä¿®æ­£"""
        if not text or not text.strip():
            return text
        
        cleaned = text.strip()
        
        # æ—¥æœ¬èªåŠ©è©ã®é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºãƒ»ä¿®æ­£
        particle_duplications = [
            # ã€Œã«ã‚ˆã‚‹ã€+ã€Œä½¿ã£ãŸã€â†’ã€Œä½¿ã£ãŸã€
            (r'ã«ã‚ˆã‚‹(.{0,5}?)ä½¿ã£ãŸ', r'\1ä½¿ã£ãŸ'),
            (r'ã«ã‚ˆã‚‹(.{0,5}?)ã§ã®', r'\1ã§ã®'),
            (r'ã«ã‚ˆã‚‹(.{0,5}?)ã‚’ä½¿ã£ãŸ', r'\1ã‚’ä½¿ã£ãŸ'),
            (r'ã«ã‚ˆã‚‹(.{0,5}?)ã«ã‚ˆã‚‹', r'\1ã«ã‚ˆã‚‹'),
            
            # ã€Œã‚’ã€ã®é‡è¤‡
            (r'ã‚’(.{0,5}?)ã‚’', r'ã‚’\1'),
            
            # ã€Œã§ã€ã®é‡è¤‡
            (r'ã§(.{0,5}?)ã§', r'ã§\1'),
            
            # ã€Œã«ã€ã®é‡è¤‡
            (r'ã«(.{0,5}?)ã«', r'ã«\1'),
            
            # ã€Œã®ã€ã®é‡è¤‡ï¼ˆ3å›ä»¥ä¸Šï¼‰
            (r'ã®(.{0,5}?)ã®(.{0,5}?)ã®', r'ã®\1\2'),
        ]
        
        for pattern, replacement in particle_duplications:
            cleaned = re.sub(pattern, replacement, cleaned)
        
        # ä¸è‡ªç„¶ãªåŠ©è©é€£ç¶šã‚’ä¿®æ­£
        unnatural_sequences = [
            # ã€Œã«ã‚ˆã‚‹ä½¿ã£ãŸã€â†’ã€Œã‚’ä½¿ã£ãŸã€
            (r'ã«ã‚ˆã‚‹ä½¿ã£ãŸ', r'ã‚’ä½¿ã£ãŸ'),
            # ã€Œã§ã®ä½¿ã£ãŸã€â†’ã€Œã§ä½¿ã£ãŸã€  
            (r'ã§ã®ä½¿ã£ãŸ', r'ã§ä½¿ã£ãŸ'),
            # ã€Œã‚’ä½¿ã£ãŸã®ã€â†’ã€Œã‚’ä½¿ã£ãŸã€
            (r'ã‚’ä½¿ã£ãŸã®(?=èª²é¡Œ|å•é¡Œ|è§£æ±º)', r'ã‚’ä½¿ã£ãŸ'),
        ]
        
        for pattern, replacement in unnatural_sequences:
            cleaned = re.sub(pattern, replacement, cleaned)
        
        # åŠ©è©ã®æ¬ è½ã‚’ä¿®æ­£ï¼ˆé‡è¦ãªè¿½åŠ ï¼‰
        missing_particles = [
            # ã€ŒClaude Codeä½¿ã£ãŸã€â†’ã€ŒClaude Codeã‚’ä½¿ã£ãŸã€
            (r'([A-Za-z\s]+)ä½¿ã£ãŸ', r'\1ã‚’ä½¿ã£ãŸ'),
            # ã€Œã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºã—ãŸã€â†’ã€Œã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹ç™ºã—ãŸã€  
            (r'([ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,})é–‹ç™ºã—ãŸ', r'\1ã‚’é–‹ç™ºã—ãŸ'),
            # ã€Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã™ã‚‹ã€â†’ã€Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç®¡ç†ã™ã‚‹ã€
            (r'([ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,})ç®¡ç†ã™ã‚‹', r'\1ã‚’ç®¡ç†ã™ã‚‹'),
            # ã€Œãƒ‡ãƒ¼ã‚¿åˆ†æã™ã‚‹ã€â†’ã€Œãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹ã€
            (r'([ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,})åˆ†æã™ã‚‹', r'\1ã‚’åˆ†æã™ã‚‹'),
        ]
        
        for pattern, replacement in missing_particles:
            cleaned = re.sub(pattern, replacement, cleaned)
        
        # èªå½™ã®é‡è¤‡é™¤å»
        word_duplications = [
            # åŒã˜å˜èªã®é‡è¤‡
            (r'(.{2,}?)\1', r'\1'),  # ã€Œé–‹ç™ºé–‹ç™ºã€â†’ã€Œé–‹ç™ºã€
        ]
        
        for pattern, replacement in word_duplications:
            cleaned = re.sub(pattern, replacement, cleaned)
        
        return cleaned
    
    def _validate_japanese_grammar(self, title: str) -> bool:
        """æ—¥æœ¬èªæ–‡æ³•ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        if not title or not title.strip():
            return False
        
        # åŸºæœ¬çš„ãªæ–‡æ³•ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
        grammar_errors = [
            r'ã«ã‚ˆã‚‹.{0,10}?ä½¿ã£ãŸ',  # ã€Œã«ã‚ˆã‚‹ã€+ã€Œä½¿ã£ãŸã€
            r'ã‚’.{0,5}?ã‚’',           # ã€Œã‚’ã€ã®é‡è¤‡
            r'ã§.{0,5}?ã§',           # ã€Œã§ã€ã®é‡è¤‡  
            r'ã«.{0,5}?ã«',           # ã€Œã«ã€ã®é‡è¤‡
            r'ã®.{0,5}?ã®.{0,5}?ã®',  # ã€Œã®ã€ã®3å›ä»¥ä¸Šé‡è¤‡
        ]
        
        for pattern in grammar_errors:
            if re.search(pattern, title):
                return False
        
        # åŠ©è©æ¬ è½ã®æ¤œå‡º
        missing_particle_errors = [
            r'[A-Za-z\s]+(?<!ã‚’)ä½¿ã£ãŸ',  # è‹±èªåè©+ä½¿ã£ãŸï¼ˆã‚’ãªã—ï¼‰
            r'[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{2,}(?<!ã‚’)é–‹ç™ºã—ãŸ',  # æ—¥æœ¬èªåè©+é–‹ç™ºã—ãŸï¼ˆã‚’ãªã—ï¼‰
            r'[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]{3,}(?<!ã‚’)ç®¡ç†ã™ã‚‹',  # æ—¥æœ¬èªåè©+ç®¡ç†ã™ã‚‹ï¼ˆã‚’ãªã—ï¼‰
        ]
        
        for pattern in missing_particle_errors:
            if re.search(pattern, title):
                return False
        
        return True
    
    def _extract_nouns_from_text(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åè©ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã€ã²ã‚‰ãŒãªã®çµ„ã¿åˆã‚ã›ã§2-10æ–‡å­—ã®å˜èªã‚’æŠ½å‡º
        pattern = r'[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯ã-ã‚“]{2,10}'
        candidates = re.findall(pattern, text)
        
        # ä¸€èˆ¬çš„ã™ãã‚‹å˜èªã‚’é™¤å¤–
        common_words = {'ã“ã¨', 'ã‚‚ã®', 'ã¨ã', 'ã¨ã“ã‚', 'ãŸã‚', 'ã‚ˆã†', 'ã•ã‚“', 'ãã‚“', 'ã¡ã‚ƒã‚“'}
        
        return [word for word in candidates if word not in common_words]
    
    def _extract_key_phrases(self, content: str) -> List[str]:
        """ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºã®æŠ½å‡º"""
        # åè©å¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        noun_phrase_patterns = [
            r'[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]+(?:ã®[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]+)+',  # ã€Œã€œã®ã€œã€
            r'[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]+(?:[åŒ–æˆå‹å¼])',  # ã€Œã€œåŒ–ã€ã€Œã€œæˆã€ãªã©
            r'[A-Za-z]+[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]+',  # è‹±èª+æ—¥æœ¬èª
            r'[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]+[A-Za-z]+',  # æ—¥æœ¬èª+è‹±èª
        ]
        
        phrases = []
        for pattern in noun_phrase_patterns:
            matches = re.findall(pattern, content)
            phrases.extend(matches)
        
        # é »åº¦ã§ã‚½ãƒ¼ãƒˆ
        phrase_counter = Counter(phrases)
        return [phrase for phrase, count in phrase_counter.most_common(10) if count >= 1]
    
    def _calculate_word_frequency(self, content: str) -> Dict[str, int]:
        """å˜èªé »åº¦ã®è¨ˆç®—"""
        # å˜èªæŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        words = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯ã-ã‚“A-Za-z]+', content)
        return dict(Counter(words).most_common(20))
    
    def _analyze_sentence_structures(self, content: str) -> List[str]:
        """æ–‡æ§‹é€ ã®åˆ†æ"""
        structures = []
        sentences = re.split(r'[ã€‚ï¼ï¼ï¼Ÿ\n]', content)
        
        for sentence in sentences[:10]:  # æœ€åˆã®10æ–‡ã¾ã§
            if not sentence.strip():
                continue
                
            structure = []
            if re.search(r'[ã¯ãŒã‚’]', sentence):
                structure.append('ä¸»èªã‚ã‚Š')
            if re.search(r'[ã€‚ï¼ï¼ï¼Ÿ]$', sentence):
                structure.append('å®Œçµæ–‡')
            if re.search(r'ã®ã§|ãŸã‚|ã‹ã‚‰', sentence):
                structure.append('ç†ç”±èª¬æ˜')
            if re.search(r'ã—ã‹ã—|ã§ã‚‚|ãŸã ã—', sentence):
                structure.append('é€†æ¥')
                
            structures.append('_'.join(structure) if structure else 'å˜ç´”æ–‡')
        
        return structures
    
    def _extract_subtopics(self, content: str) -> List[str]:
        """ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã®æŠ½å‡º"""
        subtopics = []
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ã®æŠ½å‡º
        section_patterns = [
            r'â– \s*(.+?)(?:\n|$)',
            r'â–¶\s*(.+?)(?:\n|$)',
            r'ãƒ»\s*(.+?)(?:\n|$)',
            r'\d+\.\s*(.+?)(?:\n|$)'
        ]
        
        for pattern in section_patterns:
            matches = re.findall(pattern, content, re.MULTILINE)
            subtopics.extend(matches)
        
        return subtopics[:10]  # æœ€å¤§10å€‹
    
    def _detect_intent(self, content: str) -> str:
        """æ–‡æ›¸ã®æ„å›³ã‚’æ¤œå‡º"""
        intent_patterns = {
            'inform': ['ãŠçŸ¥ã‚‰ã›', 'å ±å‘Š', 'å…±æœ‰', 'é€£çµ¡'],
            'request': ['ãŠé¡˜ã„', 'ä¾é ¼', 'ã—ã¦ãã ã•ã„', 'ã—ã¦ã„ãŸã ã‘'],
            'analyze': ['åˆ†æ', 'æ¤œè¨¼', 'è€ƒå¯Ÿ', 'è©•ä¾¡'],
            'plan': ['è¨ˆç”»', 'äºˆå®š', 'ä¼ç”»', 'ã™ã‚‹äºˆå®š'],
            'record': ['è¨˜éŒ²', 'ãƒ¡ãƒ¢', 'å‚™å¿˜', 'è¦šãˆ']
        }
        
        intent_scores = {}
        for intent, keywords in intent_patterns.items():
            score = sum(1 for keyword in keywords if keyword in content)
            intent_scores[intent] = score
        
        if intent_scores:
            return max(intent_scores, key=intent_scores.get)
        return 'general'
    
    def _identify_domain(self, content: str) -> str:
        """ãƒ‰ãƒ¡ã‚¤ãƒ³ã®è­˜åˆ¥"""
        # X/SNSé–¢é€£ã®ç‰¹åˆ¥åˆ¤å®šã‚’æœ€åˆã«å®Ÿè¡Œ
        x_sns_patterns = [
            r'(?:X|Twitter|SNS|ã‚½ãƒ¼ã‚·ãƒ£ãƒ«).*?(?:åˆ†æ|ãƒ¬ãƒãƒ¼ãƒˆ|ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ|æˆ¦ç•¥)',
            r'(?:ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼|ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ|ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³).*?(?:åˆ†æ|ç²å¾—|æ•°å€¤)',
            r'(?:ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°|ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³|ãƒ–ãƒ©ãƒ³ãƒ‰).*?(?:æˆ¦ç•¥|åˆ†æ|åŠ¹æœ)',
            r'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ.*?(?:åˆ†æ|å®Œå…¨åˆ†æ|ãƒ¬ãƒãƒ¼ãƒˆ)',
            r'(?:æŠ•ç¨¿|ãƒã‚¹ãƒˆ).*?(?:åˆ†æ|æˆ¦ç•¥|å‚¾å‘)'
        ]
        
        for pattern in x_sns_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                return 'business'  # ç¢ºå®Ÿã«ãƒ“ã‚¸ãƒã‚¹ã¨åˆ¤å®š
        
        domain_keywords = {
            'business': [
                'ãƒ“ã‚¸ãƒã‚¹', 'å–¶æ¥­', 'å£²ä¸Š', 'æˆ¦ç•¥', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'çµŒå–¶',
                'SNS', 'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ', 'ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³',
                'ãƒªãƒ¼ãƒ', 'ãƒ–ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°', 'é›†å®¢', 'é¡§å®¢', 'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ', 'ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³',
                'ROI', 'KPI', 'åŠ¹æœæ¸¬å®š', 'ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³', 'åºƒå‘Š', 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³',
                'Twitter', 'X', 'Instagram', 'Facebook', 'LinkedIn', 'TikTok',
                'åˆ†æãƒ¬ãƒãƒ¼ãƒˆ', 'ãƒ‡ãƒ¼ã‚¿åˆ†æ', 'ç«¶åˆåˆ†æ', 'å¸‚å ´åˆ†æ', 'ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ'
            ],
            'tech': ['ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'AI', 'ã‚·ã‚¹ãƒ†ãƒ ', 'é–‹ç™º', 'ã‚³ãƒ¼ãƒ‰', 'API'],
            'education': ['æ•™è‚²', 'å­¦ç¿’', 'æˆæ¥­', 'ç”Ÿå¾’', 'è¬›å¸«', 'å¡¾'],
            'health': ['å¥åº·', 'åŒ»ç™‚', 'è¨ºæ–­', 'æ²»ç™‚', 'ç—‡çŠ¶'],
            'finance': ['é‡‘è', 'æŠ•è³‡', 'äºˆç®—', 'è³‡é‡‘', 'åæ”¯']
        }
        
        # è¤‡åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡ã¿ä»˜ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³
        compound_patterns = {
            'business': [
                (r'(?:X|Twitter|SNS|ã‚½ãƒ¼ã‚·ãƒ£ãƒ«).*?(?:åˆ†æ|ãƒ¬ãƒãƒ¼ãƒˆ|ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ)', 5),
                (r'(?:ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼|ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ|ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³).*?(?:åˆ†æ|æ•°å€¤|ãƒ‡ãƒ¼ã‚¿)', 4),
                (r'(?:ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°|ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³|ãƒ–ãƒ©ãƒ³ãƒ‰).*?(?:æˆ¦ç•¥|åˆ†æ|åŠ¹æœ)', 4),
                (r'(?:ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ|ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«).*?(?:åˆ†æ|ãƒ¬ãƒãƒ¼ãƒˆ|è¨ºæ–­)', 3)
            ]
        }
        
        domain_scores = {}
        
        # åŸºæœ¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        for domain, keywords in domain_keywords.items():
            score = sum(2 if keyword in content else 0 for keyword in keywords)
            domain_scores[domain] = score
        
        # è¤‡åˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        for domain, patterns in compound_patterns.items():
            if domain not in domain_scores:
                domain_scores[domain] = 0
            for pattern, weight in patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    domain_scores[domain] += weight
        
        # ç‰¹åˆ¥ãªåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼šX/Twitteré–¢é€£ã®åˆ†æã¯å¿…ãšbusiness
        x_twitter_patterns = [
            r'(?:X|Twitter).*?(?:ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ|åˆ†æ|ãƒ¬ãƒãƒ¼ãƒˆ)',
            r'(?:ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼|ãƒ„ã‚¤ãƒ¼ãƒˆ|ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ).*?(?:åˆ†æ|ãƒ‡ãƒ¼ã‚¿)',
            r'(?:SNS|ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢).*?(?:åˆ†æ|æˆ¦ç•¥|ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°)'
        ]
        
        for pattern in x_twitter_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                domain_scores['business'] = max(domain_scores.get('business', 0), 10)
                break
        
        if domain_scores:
            best_domain = max(domain_scores, key=domain_scores.get)
            if domain_scores[best_domain] > 0:
                return best_domain
        
        return 'general'
    
    def _analyze_tone(self, content: str) -> str:
        """æ–‡æ›¸ã®ãƒˆãƒ¼ãƒ³ã‚’åˆ†æ"""
        tone_patterns = {
            'formal': ['ã§ã™', 'ã¾ã™', 'ã”ã–ã„ã¾ã™', 'ã„ãŸã—ã¾ã™'],
            'casual': ['ã ã­', 'ã‹ãª', 'ã‚ˆã­', 'ã£ã½ã„'],
            'analytical': ['åˆ†æ', 'è€ƒå¯Ÿ', 'çµæœ', 'è©•ä¾¡'],
            'instructional': ['ã—ã¦ãã ã•ã„', 'å¿…è¦ã§ã™', 'é‡è¦ã§ã™', 'ã¹ã']
        }
        
        tone_scores = {}
        for tone, patterns in tone_patterns.items():
            score = sum(1 for pattern in patterns if pattern in content)
            tone_scores[tone] = score
        
        if tone_scores:
            return max(tone_scores, key=tone_scores.get)
        return 'neutral'
    
    def _analyze_entity_relations(self, content: str) -> List[Dict[str, Any]]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®é–¢ä¿‚ã‚’åˆ†æ"""
        # ç°¡æ˜“çš„ãªå®Ÿè£…
        entities = self._extract_entities(content)
        relations = []
        
        # åŒã˜æ–‡ã«å‡ºç¾ã™ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¯é–¢é€£ãŒã‚ã‚‹ã¨ä»®å®š
        sentences = re.split(r'[ã€‚ï¼ï¼ï¼Ÿ\n]', content)
        for sentence in sentences:
            sentence_entities = [e for e in entities if e in sentence]
            if len(sentence_entities) >= 2:
                for i in range(len(sentence_entities)):
                    for j in range(i + 1, len(sentence_entities)):
                        relations.append({
                            'entity1': sentence_entities[i],
                            'entity2': sentence_entities[j],
                            'type': 'co-occurrence',
                            'context': sentence[:50] + '...' if len(sentence) > 50 else sentence
                        })
        
        return relations[:10]  # æœ€å¤§10å€‹
    
    def _extract_entities(self, content: str) -> List[str]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®æŠ½å‡º"""
        # å›ºæœ‰åè©ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        patterns = [
            r'[A-Z][a-zA-Z]+',  # è‹±èªã®å›ºæœ‰åè©
            r'[ã‚¡-ãƒ¶ãƒ¼]{3,}',  # ã‚«ã‚¿ã‚«ãƒŠã®å›ºæœ‰åè©
            r'(?:æ ªå¼ä¼šç¤¾|æœ‰é™ä¼šç¤¾)[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]+',  # ä¼šç¤¾å
            r'[ä¸€-é¾¯]{2,4}(?:ã•ã‚“|æ§˜|æ°)',  # äººå
        ]
        
        entities = []
        for pattern in patterns:
            matches = re.findall(pattern, content)
            entities.extend(matches)
        
        return list(set(entities))
    
    def _analyze_temporal_flow(self, content: str) -> List[Dict[str, str]]:
        """æ™‚ç³»åˆ—ã®æµã‚Œã‚’åˆ†æ"""
        temporal_markers = {
            'past': ['æ˜¨æ—¥', 'å…ˆé€±', 'å…ˆæœˆ', 'ä»¥å‰', 'éå»'],
            'present': ['ä»Šæ—¥', 'ç¾åœ¨', 'ä»Š', 'æœ¬æ—¥'],
            'future': ['æ˜æ—¥', 'æ¥é€±', 'æ¥æœˆ', 'ä»Šå¾Œ', 'å°†æ¥']
        }
        
        temporal_flow = []
        sentences = re.split(r'[ã€‚ï¼ï¼ï¼Ÿ\n]', content)
        
        for i, sentence in enumerate(sentences):
            for time_type, markers in temporal_markers.items():
                if any(marker in sentence for marker in markers):
                    temporal_flow.append({
                        'sentence_index': i,
                        'time_type': time_type,
                        'sentence': sentence[:50] + '...' if len(sentence) > 50 else sentence
                    })
                    break
        
        return temporal_flow
    
    def _analyze_hierarchy(self, content: str) -> Dict[str, Any]:
        """éšå±¤æ§‹é€ ã®åˆ†æ"""
        hierarchy = {
            'depth': 0,
            'structure': []
        }
        
        # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®æ¤œå‡º
        lines = content.split('\n')
        current_depth = 0
        
        for line in lines:
            if not line.strip():
                continue
                
            # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã®æ·±ã•ã‚’è¨ˆç®—
            indent = len(line) - len(line.lstrip())
            
            # æ§‹é€ ãƒãƒ¼ã‚«ãƒ¼ã®æ¤œå‡º
            if re.match(r'^#{1,6}\s', line):
                level = len(re.match(r'^(#+)', line).group(1))
                hierarchy['structure'].append({
                    'type': 'heading',
                    'level': level,
                    'text': line.strip()
                })
                hierarchy['depth'] = max(hierarchy['depth'], level)
            elif re.match(r'^\s*[-*+]\s', line):
                hierarchy['structure'].append({
                    'type': 'list_item',
                    'level': indent // 2,
                    'text': line.strip()
                })
            elif re.match(r'^\s*\d+\.\s', line):
                hierarchy['structure'].append({
                    'type': 'numbered_item',
                    'level': indent // 2,
                    'text': line.strip()
                })
        
        return hierarchy
    
    def _integrate_themes(self, surface: Dict, context: Dict) -> str:
        """ãƒ†ãƒ¼ãƒã®çµ±åˆï¼ˆç°¡æ½”ç‰ˆï¼‰"""
        # ä¸»é¡Œã‚’ä¸­å¿ƒã¨ã—ãŸçµ±åˆï¼ˆéåº¦ãªè¿½åŠ èªå½™ã‚’é¿ã‘ã‚‹ï¼‰
        main_topic = context.get('main_topic', {}).get('text', '')
        
        # ä¸»é¡ŒãŒååˆ†ã«å…·ä½“çš„ãªå ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
        if main_topic and len(main_topic) > 8:
            return main_topic
        
        # ä¸»é¡ŒãŒçŸ­ã„å ´åˆã®ã¿ã€é¸æŠçš„ã«é«˜é »åº¦èªã‚’è¿½åŠ 
        high_freq_words = list(surface.get('word_frequency', {}).keys())[:3]
        if main_topic and high_freq_words:
            # æ„å‘³ã®ã‚ã‚‹èªå¥ã®ã¿ã‚’é¸æŠçš„ã«è¿½åŠ 
            meaningful_words = [
                w for w in high_freq_words 
                if (w not in main_topic and 
                    len(w) > 2 and 
                    w not in ['ã«ã¤ã„ã¦', 'ã«é–¢ã™ã‚‹', 'ã§ã‚ã‚‹', 'ã§ã™', 'ã¾ã™', 'ã—ãŸ', 'ã™ã‚‹'])
            ][:1]  # æœ€å¤§1èªã®ã¿
            
            if meaningful_words:
                return f"{main_topic}ãƒ»{meaningful_words[0]}"
        
        return main_topic or 'ãƒ¡ãƒ¢'
    
    def _extract_key_insights(self, relation: Dict) -> List[str]:
        """é‡è¦ãªæ´å¯Ÿã®æŠ½å‡º"""
        insights = []
        
        # å› æœé–¢ä¿‚ã‹ã‚‰ã®æ´å¯Ÿ
        causal_relations = relation.get('causal_relations', [])
        if causal_relations:
            for rel in causal_relations[:3]:
                if rel['type'] == 'causal':
                    insights.append(f"{rel['from']} â†’ {rel['to']}")
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–¢ä¿‚ã‹ã‚‰ã®æ´å¯Ÿ
        entity_relations = relation.get('entity_relations', [])
        if entity_relations:
            for rel in entity_relations[:2]:
                insights.append(f"{rel['entity1']} - {rel['entity2']}ã®é–¢é€£")
        
        return insights
    
    def _infer_implicit_meanings(self, content: str, context: Dict) -> List[str]:
        """æš—é»™çš„ãªæ„å‘³ã®æ¨è«–"""
        implicit_meanings = []
        
        # æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã¨æ„å›³ã‹ã‚‰æ¨è«–
        doc_type = context.get('document_type', {}).get('type', '')
        intent = context.get('intent', '')
        
        if doc_type == 'meeting_notes' and intent == 'record':
            implicit_meanings.append('æ±ºå®šäº‹é …ã®ç¢ºèªã¨å…±æœ‰ãŒå¿…è¦')
        elif doc_type == 'analysis' and intent == 'analyze':
            implicit_meanings.append('æ”¹å–„ç­–ã®æ¤œè¨ãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã‚‹')
        elif doc_type == 'report' and 'å•é¡Œ' in content:
            implicit_meanings.append('èª²é¡Œè§£æ±ºã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒå¿…è¦')
        
        return implicit_meanings
    
    def _extract_action_items(self, content: str) -> List[str]:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®æŠ½å‡º"""
        action_items = []
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¤ºã™ãƒ‘ã‚¿ãƒ¼ãƒ³
        action_patterns = [
            r'(?:TODO|ToDo|todo)[:ï¼š]\s*(.+?)(?:\n|$)',
            r'(?:å®¿é¡Œ|èª²é¡Œ|ã‚¿ã‚¹ã‚¯)[:ï¼š]\s*(.+?)(?:\n|$)',
            r'(.+?)(?:ã™ã‚‹å¿…è¦ãŒã‚ã‚‹|ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„|ã™ã‚‹ã“ã¨)',
            r'(.+?)(?:ã‚’æ¤œè¨|ã‚’ç¢ºèª|ã‚’æº–å‚™|ã‚’ä½œæˆ|ã‚’ä¿®æ­£)'
        ]
        
        for pattern in action_patterns:
            matches = re.findall(pattern, content, re.MULTILINE)
            action_items.extend(matches)
        
        # é‡è¤‡ã‚’é™¤å»ã—ã¦è¿”ã™
        return list(dict.fromkeys(action_items))[:5]
    
    def _generate_intelligent_output(self, surface: Dict, context: Dict, 
                                   relation: Dict, semantic: Dict, content: str) -> Dict[str, Any]:
        """åˆ†æçµæœã‚’çµ±åˆã—ã¦å‡ºåŠ›ã‚’ç”Ÿæˆ"""
        
        # ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆå›ºæœ‰åè©å„ªå…ˆï¼‰
        title = self._generate_final_title(context, semantic, content)
        
        # ã‚¿ã‚°ç”Ÿæˆï¼ˆå…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚‚æ¸¡ã™ï¼‰
        tags = self._generate_contextual_tags({
            'surface_analysis': surface,
            'context_analysis': context,
            'relation_analysis': relation,
            'semantic_integration': semantic,
            'original_content': content  # å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿½åŠ 
        })
        
        # ã‚«ãƒ†ã‚´ãƒªæ±ºå®šï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚‚è€ƒæ…®ï¼‰
        category = self._determine_final_category(context, content)
        
        # è¦ç´„ç”Ÿæˆ
        summary = self._generate_summary(semantic, context)
        
        return {
            'title': title,
            'tags': tags,
            'category': category,
            'summary': summary,
            'analysis_details': {
                'document_type': context.get('document_type', {}),
                'main_topic': context.get('main_topic', {}),
                'key_insights': semantic.get('key_insights', []),
                'action_items': semantic.get('action_items', [])
            }
        }
    
    def _generate_final_title(self, context: Dict, semantic: Dict, content: str = '') -> str:
        """æ„å‘³ç†è§£ã«åŸºã¥ãã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ"""
        # å›ºæœ‰åè©ã®æ¤œå‡º
        proper_nouns = self._extract_proper_nouns(content)
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®çœŸã®æ„å‘³ã‚’åˆ†æ
        title = self._extract_meaningful_title(content, proper_nouns)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯
        if not title or len(title.strip()) < 5:
            main_topic = context.get('main_topic', {})
            if main_topic.get('confidence', 0) > 0.7:
                candidate_title = main_topic.get('text', '')
                # æ–­ç‰‡åŒ–ãƒã‚§ãƒƒã‚¯
                if len(candidate_title) > 5 and not self._is_fragmented_title(candidate_title):
                    title = candidate_title
                else:
                    title = semantic.get('coherent_theme', '')
            else:
                title = semantic.get('coherent_theme', '')
        
        # æœ€çµ‚å“è³ªãƒã‚§ãƒƒã‚¯
        if not title or len(title.strip()) < 3 or self._is_fragmented_title(title):
            # å›ºæœ‰åè©ãƒ™ãƒ¼ã‚¹ã§ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ
            if proper_nouns:
                title = self._generate_title_from_content_meaning(content, proper_nouns[0])
            else:
                title = 'ãƒ¡ãƒ¢'
        
        title = title.strip()
        
        # åŒ…æ‹¬çš„æ–‡æ³•ãƒã‚§ãƒƒã‚¯: åŠ©è©é‡è¤‡ãƒ»æ–‡æ³•ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£
        title = self._clean_grammatical_particles(title)
        
        # æ–‡æ³•å¦¥å½“æ€§ã®æœ€çµ‚ç¢ºèª
        if not self._validate_japanese_grammar(title):
            # æ–‡æ³•ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯å®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if proper_nouns:
                title = f"{proper_nouns[0]}æ´»ç”¨ã‚¬ã‚¤ãƒ‰"
            else:
                title = 'ãƒ¡ãƒ¢'
        
        # ä¸æ­£ãªæ¥å°¾è¾ã®é™¤å»
        unwanted_suffixes = ['ä»– -', 'ä»–-', 'ä»–ã€', 'ä»–ã€‚', 'ã®ä»–', ' -', '- ', 'ä»–']
        for suffix in unwanted_suffixes:
            if title.endswith(suffix):
                title = title[:-len(suffix)].strip()
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®é•·ã•èª¿æ•´ï¼ˆæ—¥æœ¬èªã«é©ã—ãŸé•·ã•ï¼‰
        if len(title) > 45:
            # å˜èªã®å¢ƒç•Œã§åˆ‡ã‚‹
            cut_point = 42
            while cut_point > 20 and title[cut_point] not in ['ã®', 'ã‚’', 'ã«', 'ã¨', 'ã§', ' ', 'ã€', 'ã¦', 'ãŸ', 'ãƒ»']:
                cut_point -= 1
            title = title[:cut_point]
            # æœ«å°¾ã®å¥èª­ç‚¹ã‚’æ•´ç†
            title = title.rstrip('ãƒ»ã€ã€‚')
        
        # æœ€çµ‚çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯
        if len(title) < 3:
            title = 'ãƒ¡ãƒ¢'
        
        return title
    
    def _determine_final_category(self, context: Dict, content: str = '') -> str:
        """åˆ†é‡æ¨ªæ–­çš„ãªã‚«ãƒ†ã‚´ãƒªæ±ºå®šã‚·ã‚¹ãƒ†ãƒ """
        content_lower = content.lower()
        main_topic = context.get('main_topic', {}).get('text', '').lower()
        
        # æŠ€è¡“ãƒ»é–‹ç™ºåˆ†é‡ã®åˆ¤å®š
        tech_indicators = [
            'claude code', 'chatgpt', 'ai', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°', 'github',
            'python', 'javascript', 'typescript', 'react', 'docker', 'api', 'ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º',
            'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°', 'ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°', 'ãƒ‡ãƒãƒƒã‚°', 'ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°'
        ]
        if any(term in content_lower or term in main_topic for term in tech_indicators):
            return 'tech'
        
        # ãƒ“ã‚¸ãƒã‚¹ãƒ»ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°åˆ†é‡ã®åˆ¤å®š
        business_indicators = [
            'ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'æˆ¦ç•¥', 'ãƒ“ã‚¸ãƒã‚¹', 'sns', 'ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼',
            'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'å£²ä¸Š', 'åç›Š', 'é¡§å®¢', 'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ', 'å–¶æ¥­', 'çµŒå–¶'
        ]
        if any(term in content_lower or term in main_topic for term in business_indicators):
            return 'business'
        
        # æ•™è‚²åˆ†é‡ã®åˆ¤å®š
        education_indicators = [
            'æ•™è‚²', 'å­¦ç¿’', 'è¬›å¸«', 'å¡¾', 'æˆæ¥­', 'ç”Ÿå¾’', 'å­¦ç”Ÿ', 'æŒ‡å°', 'ç ”ä¿®',
            'æ•™æ', 'edtech', 'å­¦ç¿’æ”¯æ´', 'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æˆæ¥­'
        ]
        if any(term in content_lower or term in main_topic for term in education_indicators):
            return 'education'
        
        # åˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆåˆ†é‡ã®åˆ¤å®š
        analysis_indicators = [
            'åˆ†æ', 'è©•ä¾¡', 'è€ƒå¯Ÿ', 'æ¤œè¨¼', 'ãƒ¬ãƒãƒ¼ãƒˆ', 'èª¿æŸ»', 'ç ”ç©¶', 'ãƒ‡ãƒ¼ã‚¿',
            'çµ±è¨ˆ', 'æŒ‡æ¨™', 'kpi', 'æˆæœæ¸¬å®š', 'åŠ¹æœæ¤œè¨¼'
        ]
        if any(term in content_lower or term in main_topic for term in analysis_indicators):
            return 'analysis'
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ»è¨ˆç”»åˆ†é‡ã®åˆ¤å®š
        project_indicators = [
            'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', 'è¨ˆç”»', 'ãƒ—ãƒ©ãƒ³', 'æˆ¦ç•¥', 'ç›®æ¨™', 'ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—',
            'ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³', 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«', 'ã‚¿ã‚¹ã‚¯', 'todo'
        ]
        if any(term in content_lower or term in main_topic for term in project_indicators):
            return 'projects'
        
        # å‚è€ƒè³‡æ–™ãƒ»ãƒ¡ãƒ¢åˆ†é‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        return 'reference'
    
    def _generate_summary(self, semantic: Dict, context: Dict) -> List[str]:
        """è¦ç´„ã®ç”Ÿæˆ"""
        summary_points = []
        
        # ã‚­ãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’è¿½åŠ 
        insights = semantic.get('key_insights', [])
        summary_points.extend(insights[:2])
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿½åŠ 
        actions = semantic.get('action_items', [])
        if actions:
            summary_points.append(f"TODO: {actions[0]}")
        
        # æš—é»™çš„ãªæ„å‘³ã‚’è¿½åŠ 
        implicit = semantic.get('implicit_meanings', [])
        if implicit:
            summary_points.append(implicit[0])
        
        # æœ€ä½3å€‹ã€æœ€å¤§6å€‹ã«ã™ã‚‹
        while len(summary_points) < 3:
            summary_points.append(f"{context.get('intent', 'ãƒ¡ãƒ¢')}ã®è¨˜éŒ²")
        
        return summary_points[:6]
    
    def _combine_key_phrases_to_title(self, key_phrases: List[str]) -> str:
        """ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’çµ„ã¿åˆã‚ã›ã¦ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        if not key_phrases:
            return ""
        
        # æœ€ã‚‚é‡è¦ãª2-3å€‹ã®ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’é¸æŠ
        selected = key_phrases[:3]
        
        # æ¥ç¶šè©ã§çµåˆ
        if len(selected) == 1:
            return selected[0]
        elif len(selected) == 2:
            return f"{selected[0]}ã¨{selected[1]}"
        else:
            return f"{selected[0]}ãƒ»{selected[1]}ä»–"